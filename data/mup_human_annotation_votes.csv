,paper_id,text,abstract,prediction_0,prediction_1,prediction_2,prediction_order,A1_best_system,A1_worst_system,A2_best_system,A2_worst_system,,A1_best_label,A1_worst_label,A2_best_label,A2_worst_label,A1_best,A1_worst,A2_best,A2_worst,A1_best_reason,A1_worst_reason,A2_best_reason,A2_worst_reason
0,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"1 Introduction . Value iteration ( VI ) is one of the most basic and wide-spread algorithms employed for tackling problems in reinforcement learning ( RL ) and optimal control [ 9 , 28 ] formulated as Markov decision processes ( MDPs ) . The VI algorithm simply involves the consecutive applications of the dynamic programming ( DP ) operator T J ( xt ) = min ut { C ( xt , ut ) + γEJ ( xt+1 ) } , where C ( xt , ut ) is the cost of taking the control action ut at the state xt . This fixed point iteration is known to converge to the optimal value function for discount factors γ ∈ ( 0 , 1 ) . However , this algorithm suffers from a high computational cost for large-scale finite state spaces . For problems with a continuous state space , the DP operation becomes an infinite-dimensional optimization problem , rendering the exact implementation of VI impossible in most cases . A common solution is to incorporate function approximation techniques and compute the output of the DP operator for a finite sample ( i.e. , a discretization ) of the underlying continuous state space . This approximation again suffers from a high computational cost for fine discretizations of the state space , particularly in high-dimensional problems . We refer the reader to [ 9 , 26 ] for various approximations of VI . For some problems , however , it is possible to partially address this issue by using duality theory , i.e. , approaching the minimization problem in the conjugate domain . In particular , as we will see 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . in Section 3 , the minimization in the primal domain in DP can be transformed to a simple addition in the dual domain , at the expense of three conjugate transforms . However , proper application of this transformation relies on efficient numerical algorithms for conjugation . Fortunately , such an algorithm , known as linear-time Legendre transform ( LLT ) , has been developed in late 90s [ 23 ] . Other than the classical application of LLT ( and other fast algorithms for conjugate transform ) in solving Hamilton-Jacobi equation [ 1 , 13 , 14 ] , these algorithms are used in image processing [ 24 ] , thermodynamics [ 12 ] , and optimal transport [ 17 ] . The application of conjugate duality for the DP problem is not new and actually goes back to Bellman [ 4 ] . Further applications of this idea for reducing the computational complexity were later explored in [ 15 , 18 ] . However , surprisingly , the application of LLT for solving discrete-time optimal control problems , has been limited . In particular , in [ 11 ] , the authors propose the “ fast value iteration ” algorithm ( without a rigorous analysis of the complexity and error of the proposed algorithm ) for a particular class of infinite-horizon optimal control problems with state-independent stage cost C ( x , u ) = C ( u ) and deterministic linear dynamics xt+1 = Axt + But , where A is a non-negative , monotone , invertible matrix . More recently , in [ 20 ] , we also considered the application of LLT for solving the DP operation in finite-horizon , optimal control of input-affine dynamics xt+1 = fs ( xt ) + But with separable cost C ( x , u ) = Cs ( x ) + Ci ( u ) . In particular , we introduced the “ discrete conjugate DP ” ( d-CDP ) operator , and provided a detailed analysis of its complexity and error . As we will discuss shortly , the current study is an extension of the corresponding d-CDP algorithm that , among other things , considers infinite horizon , discounted cost problems . We note that the algorithms developed in [ 16 , 24 ] for distance transform can also potentially tackle the optimal control problems similar to the ones of interest in the current study . In particular , these algorithms require the stage cost to be reformulated as a convex function of the “ distance ” between the current and next states . While this property might arise naturally , it can generally be restrictive , as it is in the problem class considered in this study . Another line of work that is closely related to ours invloves utilizing max-plus algebra in solving deterministic , continuous-state , continuous-time , optimal control problems ; see , e.g. , [ 2 , 25 ] . These works exploit the compatibility of the DP operation with maxplus operations , and approximate the value function as a max-plus linear combination . Recently , in [ 3 , 5 ] , the authors used this idea to propose an approximate VI algorithm for continuous-state , deterministic MDPs . In this regard , we note that the proposed approach in the current study also involves approximating the value function as a max-plus linear combination , namely , the maximum of affine functions . The key difference is however that by choosing a grid-like ( factorized ) set of slopes for the linear terms ( i.e. , the basis of the max-plus linear combination ) , we take advantage of linear time complexity of LLT in computing the constant terms ( i.e. , the coefficients of the max-plus linear combination ) . Main contribution . In this study , we focus on an approximate implementation of VI involving discretization of the state and input spaces for solving the optimal control problem of discrete-time systems , with continuous state-input space . Building upon our earlier work [ 20 ] , we employ conjugate duality to speed-up VI for problems with separable stage cost ( in state and input ) and input-affine dynamics . We propose the conjugate VI ( ConjVI ) algorithm based on a modified version of the d-CDP operator introduced in [ 20 ] , and extend the existing results in three directions : We consider infinite-horizon , discounted cost problems with stochastic dynamics , while incorporating a numerical scheme for approximation of the conjugate of input cost . The main contributions of this paper are then as follows : ( i ) we provide sufficient conditions for the convergence of ConjVI ( Theorem 3.9 ) ; ( ii ) we show that ConjVI can achieve a linear time complexity of O ( X+U ) in each iteration ( Theorem 3.10 ) , compared to the quadratic time complexity of O ( XU ) of the standard VI , where X and U are the cardinalities of the discrete state and input spaces , respectively ; ( iii ) we analyze the error of ConjVI ( Theorem 3.11 ) , and use that result to provide specific guidelines on the construction of the discrete dual domain ( Section 3.4 ) ; ( iv ) we provide a MATLAB package for the implementation of the proposed ConjVI algorithm . Notations . The standard inner product in Rn and the corresponding induced 2-norm are denoted by ⟨· , ·⟩ and ∥·∥2 , respectively . ∥·∥∞ denotes the infinity norm . We use the superscript d to denote finite ( discrete ) sets ( as in Xd ) and discrete functions ( as in hd : Xd → R ) . We use the superscript g to denote grid-like finite sets ( as in Xg = Πni=1X g i where X g i ⊂ R ) . We also use X g sub to denote the sub-grid of Xg derived by omitting the smallest and the largest elements of Xg in each dimension . The cardinality of the finite set Xd or Xg is denoted by X . We use h̃d : Rn → R = R ∪ { ∞ } to denote a generic extension of a discrete function hd , and hd : Rn → R to denote multilinear interpolation and extrapolation ( LERP ) extension of a discrete function with a grid-like domain . Let X , Y be two arbitrary sets in Rn . co ( X ) is the convex hull of X . We use d ( X , Y ) : = infx∈X , y∈Y ∥x− y∥2 to denote the distance between X and Y . The one-sided Hausdorff distance from X to Y is defined as dH ( X , Y ) : = supx∈X infy∈Y ∥x− y∥2 . Let h : Rn → R be an extended real-valued function with a non-empty effective domain dom ( h ) = X : = { x ∈ Rn : h ( x ) < ∞ } . The range of h is denoted by rng ( h ) : = maxx∈X h ( x ) −minx∈X h ( x ) , and the subdifferential of h at a point x ∈ X is defined as ∂h ( x ) : = { y ∈ Rn : h ( x̃ ) ≥ h ( x ) + ⟨y , x̃− x⟩ , ∀x̃ ∈ X } . We define L ( h ) : = Πni=1 [ L−i ( h ) , L + i ( h ) ] , where L+i ( h ) ( resp . L − i ( h ) ) is the maximum ( resp . minimum ) slope of the function h along the i-th dimension . Note that ∂h ( x ) ⊆ L ( h ) for all x ∈ X . We report the complexities using the standard big-O notations O and Õ , where the latter hides the logarithmic factors . In this study , we are mainly concerned with the dependence of the computational complexities on the size of the finite sets involved ( discretization of the primal and dual domains ) . In particular , we ignore the possible dependence of the computational complexities on the dimension of the variables , unless they appear in the power of the size of those discrete sets . We note that the extended version of this article , including the technical proofs , is available in the supplementary material . 2 VI in primal domain . We are concerned with the infinite-horizon , discounted cost , optimal control problems of the form J⋆ ( x ) =min Ewt [ ∞∑ t=0 γtC ( xt , ut ) ∣∣∣∣x0 = x ] s.t . xt+1 = g ( xt , ut , wt ) , xt ∈ X , ut ∈ U , wt ∼ P ( W ) , ∀t ∈ { 0 , 1 , . . . } , where xt ∈ Rn , ut ∈ Rm , and wt ∈ Rl are the state , input and disturbance variables at time t , respectively ; γ ∈ ( 0 , 1 ) is the discount factor ; C : X × U → R is the stage cost ; g : Rn × Rm × Rl → Rn describes the dynamics ; X ⊂ Rn and U ⊂ Rm describe the state and input constraints , respectively ; and , P ( · ) is the distribution of the disturbance over the support W ⊂ Rl . Assuming the stage cost C is bounded , the optimal value function solves the equation J⋆ = T J⋆ , where T is the DP operator ( C and J are extended to infinity outside their effective domains ) [ 7 , Prop . 1.2.2 ] T J ( x ) : = min u { C ( x , u ) + γ · EwJ ( g ( x , u , w ) ) } , ∀x ∈ X . ( 1 ) Indeed , T is γ-contractive in the infinity-norm [ 7 , Prop . 1.2.4 ] . This property then gives rise to the VI algorithm Jk+1 = T Jk , which converges to J⋆ as k → ∞ , for arbitrary initialization J0 . Moreover , assuming that the composition J ◦ g ( for each w ) and the cost C are jointly convex in the state and input variables , T also preserves convexity [ 8 , Prop . 3.3.1 ] . For numerical implementation of VI , we need to address three issues . First , we need to compute the expectation in ( 1 ) . In order to simplify the exposition and include the computational cost of this operation explicitly , we consider disturbances with finite support in this study : Assumption 2.1 ( Disturbance with finite support ) . The disturbance w has a finite support Wd ⊂ Rl with a given probability mass function ( p.m.f . ) p : Wd → [ 0 , 1 ] . Under the preceding assumption , we have EwJ ( g ( x , u , w ) ) = ∑ w∈Wd p ( w ) · J ( g ( x , u , w ) ) . The second and more important issue is that the optimization problem ( 1 ) is infinite-dimensional for the continuous state space X . This renders the exact implementation of VI impossible , except for a few cases with available closed-form solutions . A common solution to this problem is to deploy a sample-based approach , accompanied by a function approximation scheme . To be precise , for a finite subset Xd of X , at each iteration k = 0 , 1 , . . . , we take the discrete function Jdk : Xd → R as the input , and compute the discrete function Jdk+1 = [ T J̃dk ] d : Xd → R , where J̃dk : X → R is an extension of Jdk . Finally , for each x ∈ Xd , we have to solve the minimization problem in ( 1 ) over the control input . Since this minimization problem is often a difficult , non-convex problem , a common approximation involves enumeration over a discretization Ud ⊂ U . Incorporating these approximations , we end up with the approximate VI algorithm Jdk+1 = T dJdk , characterized by the discrete DP ( d-DP ) operator T dJd ( x ) : = min u∈Ud { C ( x , u ) + γ · ∑ w∈Wd p ( w ) · J̃d ( g ( x , u , w ) ) } , ∀x ∈ Xd . ( 2 ) The convergence of approximate VI described above depends on the properties of the extension operation [ ̃· ] . In particular , if the extension operation is non-expansive ( in the infinity-norm ) , then T d is also γ-contractive . The error of this approximation also depends on the extension operation and its representative power . We refer the interested reader to [ 7 , 10 , 26 ] for detailed discussions on the convergence and error of different approximation schemes for VI . The d-DP operator and the corresponding VI algorithm will be our benchmark for evaluating the performance of the alternative algorithm developed in this study . To this end , we finish this section with some remarks on the time complexity of the d-DP operation . Let the time complexity of a single evaluation of the extension operator [ ̃· ] in ( 2 ) be of O ( E ) . Then , the time complexity of the d-DP operation ( 2 ) is of O ( XUWE ) . In this regard , note that the scheme described above essentially involves approximating a continuous-state/action MDP with a finite-state/action MDP , and then applying the VI algorithm . This , in turn , implies the lower bound Ω ( XU ) for the time complexity ( corresponding to enumeration over u ∈ Ud for each x ∈ Xd ) . This lower bound is also compatible with the best existing time complexities in the literature for VI for finite MDPs ; see , e.g. , [ 3 , 27 ] . However , as we will see in the next section , for a particular class of problems , it is possible to exploit the structure of the underlying continuous system in order to achieve a better time complexity in the corresponding discretized problem .","In this study, we consider the infinite-horizon, discounted cost, optimal control of stochastic nonlinear systems with separable cost and constraints in the state and input variables. Using the linear-time Legendre transform, we propose a novel numerical scheme for implementation of the corresponding value iteration (VI) algorithm in the conjugate domain. Detailed analyses of the convergence, time complexity, and error of the proposed algorithm are provided. In particular, with a discretization of size X and U for the state and input spaces, respectively, the proposed approach reduces the time complexity of each iteration in the VI algorithm from O(XU) to O(X + U), by replacing the minimization operation in the primal domain with a simple addition in the conjugate domain.","The article discusses the problem of optimal control of continuous-state systems with discrete-time dynamics and stochastic cost. The authors propose a new algorithm, called Conjugate Value Iteration (ConjVI), which combines the power of the DP operator with the efficiency of discrete-time methods to solve this problem. ConjVI is based on a modified version of the Discrete Dual Domain (d-CDP) operator, which allows for a more efficient computation of the value function. The authors provide theoretical guarantees on the convergence of ConjVI and analyze its error, showing that it can achieve a linear time complexity of O(X+U) in each iteration, where X and U are the cardinalities of the discrete state and input spaces, respectively.","The paper develops an approximate value iteration method for an infinite-horizon, discounted-cost Markov Decision Processes (MDPs) that satisfy a given set of regulatory assumptions. The idea is to work on a dual space that replaces the DP value function by a reformulation written in terms of biconjugate operations. The authors present an algorithm that applies the conjugate operators iteratively, and present convergence and other structural results associated with the resulting errors. Numerical results compare the proposed approach against a traditional value iteration and a variant that generated dynamic discretization grids.","Value iteration (VI) is an algorithm used for problems in reinforcement learning (RL) and optimal control formulated as Markov decision processes (MDPs). However, VI suffers from a high computational cost for large-scale finite state spaces. A solution to this problem is to incorporate function approximation techniques and compute the output of the DP operator for a finite sample of the underlying continuous state space. The article proposes the conjugate VI (ConjVI) algorithm based on a modified version of the d-CDP operator that extends existing results in three directions for solving optimal control problems of discrete-time systems with a continuous state-input space.","2,0,1",2,1,0,1,,coverage,coverage,"concise, informative",coverage,0,2,1,2,Summary A has better coverage and mentions the algorithm complexity.,Summary C focus too much on background info.,Summary is more concise and is more informative.,Summary C has too much background information and poor content coverage.
1,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"1 INTRODUCTION . The introduction of normalizing layers to neural networks has in no small part contributed to the deep learning revolution in machine learning . The most successful of these techniques in the image classification domain is the batch normalization ( BatchNorm ) layer ( Ioffe & Szegedy , 2015 ) , which works by normalizing the univariate first and second order statistics between layers . Batchnorm has seen near universal adoption in image classification tasks due to its surprisingly multifaceted benefits . Compared to an unnormalized network , its has been widely observed that using batch norm empirically results in : • Stability over a wide range of step sizes • Faster convergence ( particularly with larger step sizes ) • Improved generalization The multiple effects of BatchNorm make it both hard to replace and hard to analyze . In this paper we introduce Equilibrium Normalization ( EquiNorm ) , a normalization that works in weight space and still uses a form of batch statistics unlike previous weight space approaches . EquiNorm results in very rapid convergence , even more so than BatchNorm , however as we will show in our experiments , this also results in a tendency to overfit . When combined with additional regularisation , EquiNorm can significantly outperform BatchNorm , which benefits less from this additional regularisation . 2 RELATED WORK . A number of normalization layers have been proposed that can be considered alternatives to batch normalization . Batch normalization has also been extended as batch renormalization ( Ioffe , 2017 ) to handle smaller batch sizes . Layer/Instance Normalization A simple modification of BatchNorm involves computing the statistics independently for each instance , so that no averaging is done across each mini-batch , instead averaging either across channels ( layer norm ) or separately for each channel ( instance norm ) . Unfortunately these techniques are known to not generalize as well as batch norm for convolutional neural networks ( Sec 6.7 ; Sec 4.1 . Jimmy Lei Ba , 2016 ; Yuxin Wu , 2018 ) . Group Normalization A middle ground between layer and instance normalization can be found by averaging statistics over small groups of channels . This has been shown empirically to be superior to either approach , although there is still a gap in generalization performance ( Yuxin Wu , 2018 ) . Like the approaches above , it avoids a dependence on batch statistics , allowing for potentially much smaller batches to be used without a degradation in generalization performance . Weight Normalization Additional stability can be introduced into NN training by constraining the norm of the weights corresponding to each output channel/neuron to be one . When this is done by an explicit division operation in the forward pass , rather than via an optimization constraint , this is known as weight normalization ( Salimans & Kingma , 2016 ) . An additional learnable scaling factor is also introduced . Unfortunately , to match the generalization performance of BatchNorm on image classification tasks such as CIFAR-10 , this technique needs to be used together with partial ( additive only ) BatchNorm ( Section 5.1 , Salimans & Kingma , 2016 ) . Local Response Normalization A precursor to batch norm , local normalization methods ( Jarrett et al. , 2009 ; Lyu & Simoncelli , 2008 ) played an important part in the seminal AlexNet architecture ( Krizhevsky et al. , 2012 ) , and were widely used before batch norm was introduced . LR normalization has similarities to group norm in that it uses a group of neighboring channels ( with ordering set arbitrary at initialization ) for normalization . Although it aids generalization in a similar manner to BatchNorm , it does not accelerate convergence or allow for larger step sizes to be used ( Sec 4.2.1 , Ioffe & Szegedy , 2015 ) . 3 ASSUMPTIONS . The EquiNorm method functions by modifying the weights of a convolution before it is applied . For justifying the form of our method , we make the following assumptions about this convolution , which we will discuss relaxing after detailing the method : 1 . All inputs to the convolutional layer are positive , such as when the layer is preceded by a ReLU . 2 . The convolution has stride one . 3 . Cyclic padding is used . 4 . All weights are non-zero , and there exists at least one positive and one negative weight per output channel . 4 METHOD . Consider initially for simplicity a convolutional layer with a single input and output channel . Let w : kernelheight× kernelwidth , be the weight kernel for this neuron , and let x : batchsize× height× width , be the input tensor for a single mini-batch . We will compute scalar quantities s and b that modify the weights as follows : w′′ = sw′ = s ( w + b ) . This transformation will be fully differentiated through during the backwards pass ( using automatic differentiation ) so that the gradient of w is correct . As with BatchNorm , we also include an additional affine transformation after the convolution to ensure no expressivity is lost due to the normalization operation . The core idea of equilibrium normalization is to balance the contribution of positive and negative weights to the output of the convolution . To this end , we introduce additional notation to address the positive and negative weights separately . Let superscripts +/− ( i.e . w+/w− ) indicate sums of the positive/negative elements respectively . Also , let v be the sum of the input data to the layer , v = ∑ i , j , k xijk . As we have two constants to determine , we need two constraints that we wish to be satisfied . The first constraint we introduce is common with batch normalization , a constraint on the mean of the output . Since under the cyclic padding assumption , each weight is multiplied by each input element , we can constrain the mean of the output to be zero by requiring that : vs ∑ j , j ( wjk + b ) = 0 , ∴ b = −mean ( w ) . The second constraint controls the magnitude within the total output of the layer , of the positive weight elements : vsw′+ = r , ∴ s = r vw′+ . The constant r is set so that the contribution is of average 1 per output element , which is achieved by setting r to the product of batch-size , output width and output height . Note that due to the mean constraint , this automatically results in the negative weight contribution also being of magnitude r . 4.1 FULL CASE . When multiple input channels are used , each weight no longer multiples each input , rather they each multiply only inputs from a single channel . To compensate for this we need to compute per-channel sums vc ( where c is the channel index ) and change the second constraint as follows : channelsin∑ c vcsw ′+ c = r. The first constraint changes in the same fashion . When multiple output channels are used , we just duplicate this procedure applying it to each channel ’ s weights separately . We thus maintain a s and b value per output channel , and compute as intermediate values a w′+ of matrix shape . For completeness we give the full equations below . All summations are over the full range of the summed indexes . Tensor shapes w : channelsout× channelsin× kernelheight× kernelwidth x : batchsize× channelsin× heightin× widthin w′+ : channelsout× channelsin v : channelsin s , b : channelsout Updates : vc = ∑ i , j , k xicjk , wdc = ∑ j , k wdcjk , r = batchsize× heightout× widthout× stride2 bd = − ∑ c vcwdc ( kernelheight× kernelwidth ) ∑ c vc , w′+dc = ∑ j , k ( wdcjk + bd ) I [ wdcjk + bd > 0 ] , sd = r∑ c vcw ′+ dc , w′′dcjk = sd ( wdcjk + bd ) . At test time , we follow the technique used in BatchNorm of using a running estimate of the data statistics ( vc in our method ) that is computed during training time . SINGLE PASS FORMULATION . The above calculation requires two passes over the weights , first to compute b , then to compute the sum of positive elements after the addition of b . We can do an approximate computation using only one pass by assuming the sign of each element does not change after the addition of b . The s calculation changes as follows : n+dc = ∑ j , k I [ wdcjk > 0 ] , sd = r∑ c vc ( w+dc + bdn + dc ) . We use this variant in all experiments that follow . 5 DISCUSSION . 5.1 CONTROLLING COVARIATE SHIFT . The original justification for batch normalization is its ability to minimize covariate shift , although there is some debate on whether or not this is the main contributing factor to its effectiveness , see Santurkar et al . ( 2018 ) . In this context , covariate shift refers to the change between steps of the statistics of the outputs of a layer . Like batch normalization , the approach we propose also controls the shift in the outputs of a layer between steps , just in a different way . Our approach is motivated by a hypothesis that it is not necessary to control the mean and variance precisely ; other notions of scale and shift may work as well or better . Santurkar et al . ( 2018 ) show that normalizing by other norms , such as L1 , can work well , supporting this hypothesis . The sum of the output from positive and negative weights is a form of L1 control which can be contrasted with the L2 control that Batchnorm uses . This control can be motivated by Young ’ s convolution inequality , which bounds the output of a convolution operation in terms of the norm of the input : ‖x ∗ w‖r ≤ ‖w‖p ‖x‖q , where 1 p + 1 q = 1 r + 1 . Note that p , q , r ≥ 1 is also required , and that this only applies directly when there is a single input and output channel , which we assume in the remainder of this section for simplicity . For EquiNorm , we have assumed that the input is positive , so that our input sum is equivalent to the L1 norm of the input . Additionally , after subtracting off the mean , the weight vector w′ has L1 norm equal to w′+ − w′− = 2w′+ , so we are also normalizing the weights by the L1 norm . In effect , we are applying Young ’ s convolution inequality with p = q = r = 1 . It is also possible to apply the above inequality with p = 2 , q = 1 and r = 2 . I.e . normalize the weights using the L2 norm , giving a bound on the L2 norm of the output in terms of the L1 norm of the input . This is less satisfying as one convolution ’ s output is the input of another convolution ( after passing through scaling & a nonlinearity ) so we would like to use the same norm for both inputs and outputs . The related weight normalization ( WN , Salimans & Kingma , 2016 ) method normalizes weights by their L2 norm , and differs from our method by centering outputs using an additional mean-only output batchnorm . Additionally , since it doesn ’ t normalize by the input norm , the output norm can be correspondingly large . These differences have a significant effect in practice .","We introduce a new normalization technique that exhibits the fast convergence properties of batch normalization using a transformation of layer weights instead of layer outputs. The proposed technique keeps the contribution of positive and negative weights to the layer output in equilibrium. We validate our method on a set of standard benchmarks including CIFAR-10/100, SVHN and ILSVRC 2012 ImageNet.","The article introduces a new normalization technique called Equilibrium Normalization (EquiNorm) that works in weight space and uses a different form of batch statistics compared to batch normalization. EquiNorm has been shown to significantly outperform batch normalization in some experiments, but also suffers from overfitting. To address this, the article proposes combining EquiNorm with additional regularization techniques. The article also discusses the relationship between EquiNorm and other normalization techniques, including layer/instance normalization, group normalization, and weight normalization. The authors argue that EquiNorm controls covariate shift in a different way than batch normalization, which could be beneficial in some cases. Finally, the article highlights the potential advantages of using EquiNorm, including faster convergence and improved generalization performance, but also notes that further research is needed to fully understand its properties and limitations.","The use of normalizing layers in neural networks has contributed significantly to the deep learning revolution in machine learning, particularly in image classification. Batch normalization (BatchNorm) has been the most successful technique in this domain due to its multiple benefits, including stability, faster convergence, and improved generalization. However, it is hard to replace and analyze. Equilibrium Normalization (EquiNorm) is a new normalization that works in weight space and still uses a form of batch statistics. EquiNorm leads to faster convergence than BatchNorm, but it has a tendency to overfit. When combined with additional regularization, EquiNorm can outperform BatchNorm, which benefits less from extra regularization. Other normalization layers have been proposed, including Batch Renormalization, Layer/Instance Normalization, Group Normalization, Weight Normalization, and Local Response Normalization. The EquiNorm method functions by modifying the weights of a convolution before it is applied. The EquiNorm balances the contribution of positive and negative weights to the output of the convolution and controls the shift in the outputs of a layer between steps. Using Young’s convolution inequality, EquiNorm normalizes the weights by the L1 norm and applies Young’s convolution inequality with p=q=r=1. It is also possible to apply the above inequality with p=2, q=1, and r=2.","This manuscript introduces a new layer-wise transform, EquiNorm, to improve upon batch normalization. As with batch normalization and related techniques, the idea is to introduce a simple linear transform at each layer to reduce the dependency of the features to the data. Unlike batch normalization, the procedure does not modify the inputs to the layers but rather the layer weights. For this purpose, a scaling factor and a shift is computed on a mini batch, separating positive and negative weights to compute easily both running estimates of shift and of spread (here in the l1 sense). The method is compared to BatchNorm and GroupNorm on several classic computer vision datasets. Empirically, the method converges faster in the beginning of the optimization, in the sense that in the first few epochs the test accuracy is higher than for BatchNorm. However, this benefit decreases with more epochs and when the results are close to peak performance the difference between methods is a small. In addition, test accuracy can decrease at the end of the optimization, which the authors interpret as a sign of increased overfit, and tackle with clever data augmentation. The paper reads well.","2,1,0",,,,,,,,,,,,,,,,,
2,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"1 Introduction . Topic models , which have the ability to uncover the hidden semantic structure in a text corpus , have been widely applied to text analysis . Generally , a topic model is designed to discover a set of semantically-meaningful latent topics from a collection of documents , each of which captures word co-occurrence patterns commonly observed in a document . While vanilla topic models , such as latent Dirichlet allocation ( LDA ) [ 1 ] and Poisson factor analysis ( PFA ) [ 2 ] , are able to achieve this goal , a series of their hierarchical extensions [ 3–12 ] have been developed in the hope of exploring multi-layer document representations and mining meaningful topic taxonomies . Commonly , these hierarchical topic models aim to learn a hierarchy , in which the latent topics exhibit an increasing level of abstraction when moving towards a deeper hidden layer , as shown in Fig . 1 ( a ) . Consequently , it provides users with an intuitive and interpretable way to better understand textual data . Despite their attractive performance , many existing hierarchical topic models are purely data-driven and incorporate no prior domain knowledge , which may result in some learned topics failing to describe a semantically coherent concept [ 13 ] , especially for those at a deeper hidden layer [ 14 ] . Furthermore , the inflexibility of adding prior knowledge also somewhat limits the applicability of hierarchical topic models , since it is common that a user is interested in a specific topic structure and only focused on information related to it [ 15 ] . To address this issue , we assume a structured prior in the form of a predefined topic hierarchy , where each topic is described by a semantic concept , with the concepts at adjacent layers following the hypernym relations , as illustrated in Fig . 1 ( b ) . Such a ∗Corresponding Author . 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . hierarchy can be easily constructed , either by some generic knowledge graph like WordNet [ 16 ] or according to the user ’ s customization . However , there are two main challenges : one is how to model this semantic hierarchy , and the other is how to combine it with topic models . For the first challenge , a general and powerful approach [ 17 ] is to learn a distance-preserving mapping , which maps semantically similar topics to nearby points in the embedding space , with a symmetric distance measure ( e.g. , Euclidean or cosine distance ) typically being used . However , the embeddings learned by such a scheme can not perfectly reflect the entailment between concepts . To mitigate this issue , Vendrov et al . [ 18 ] exploit the partial order structure of the semantic hierarchy to learn order embeddings that respect this partial order structure . On the other hand , considering that probability distributions are better at capturing uncertainties of concepts than point vectors , Athiwaratkun and Wilson [ 19 ] introduce density order embeddings as an improvement to order embeddings . Density order embeddings allows the entailment relationship to be expressed naturally , with general words such as “ entity ” corresponding to broad distributions that encompass more specific words such as “ animal. ” Through encapsulation of probability densities , it can intuitively reflect the hierarchical structure of semantic concepts , thus offering us an effective way to model the topic hierarchy . As for the second challenge of combining the constructed topic hierarchy with topic models , there are two major difficulties . For one thing , the structured prior requires modeling the relationship between topics across different layers directly , while hierarchical topic models often assume that the topics at different layers are independently drawn in the prior [ 6 ] , which makes the fusion of the two a bit far-fetched . For another , since the constructed topic hierarchy can be very deep for large-scale text corpora , an equally deep topic model with multiple latent layers is needed to incorporate with it . However , most existing deep neural topic models are hard to go very deep due to the problem of posterior collapse [ 14 , 20–25 ] . Fortunately , SawETM [ 14 ] , a recently proposed neural topic model , not only builds the dependency between topics at different layers in a shared embedding space , but also alleviates the issue of posterior collapse to some extent through a novel network module referred to as Sawtooth Connection ( SC ) , and hence overcoming both difficulties . In this paper , a novel knowledge-based hierarchical topic model has been proposed , the core idea of which is to represent each topic as a Gaussian embedding , and then project the topics of all layers into a shared embedding space to facilitate the injection of structured prior knowledge into the model . In particular , we first introduce the Gaussian-distributed embeddings to the SawETM , resulting in a variant of SawETM with a stochastic decoder called Gaussian SawETM . To incorporate prior belief into Gaussian SawETM , we then constrain the topics at adjacent layers to encourage them to capture the concepts satisfying the predefined hypernym relations . With auto-encoding variational inference , the entire model is learned in an end-to-end manner by minimizing the evidence lower bound and a regularization term . Extensive experiments demonstrate our model has competitive performance and better interpretability in comparison to most existing hierarchical topic models . The main contributions of the paper can be summarized as follows : • To capture semantic uncertainties of words and topics , we propose a novel probabilistic generative model with a stochastic decoder , referred to as Sawtooth Factorial Gaussian Topic Embeddings guided gamma belief network ( Gaussian SawETM ) . • To incorporate the structured prior knowledge from the real world , we propose TopicNet , a novel knowledge-based hierarchical topic model based on Gaussian SawETM . • In addition to detailed theoretical analysis , we conduct extensive experiments to verify the effectiveness of the above two models . One of the most appealing properties of TopicNet is its interpretability . We conduct extensive qualitative evaluation on the quality of the topics discovered by TopicNet , including word embedding , topic embedding , and topic hierarchies . 2 Gaussian SawETM . In this section , we elaborate on the construction and inference of Gaussian SawETM , a deep hierarchical topic model that represents words and topics as Gaussian-distributed embeddings [ 26 ] . 2.1 Symmetric similarity : expected likelihood kernel . The distance measure plays a key role in quantifying the similarities between embeddings . A simple choice is taking the inner product between the means of two Gaussians , which , however , does not take the advantage of the semantic uncertainties brought by Gaussian-distributed embeddings . Here we employ the expected likelihood kernel [ 19 , 26 , 27 ] as our similarity measure , which is defined as E ( s ) ( αi , αj ) = ∫ x∈Rn N ( x ; µi , Σi ) N ( x ; µj , Σj ) dx = N ( 0 ; µi − µj , Σi+Σj ) , ( 1 ) whereαi is a Gaussian distribution with mean µi and diagonal covariance matrix Σi . As a symmetric similarity function , this kernel considers the impact of semantic uncertainties brought by covariances . 2.2 Document decoder with sawtooth factorization and Gaussian embeddings . The generative network ( also known as decoder ) is one of the core components of topic models . As discussed in Section 1 , to build dependency between topics at two adjacent layers and learn a deep topic hierarchy , we draw experience from the Sawtooth Connection ( SC ) in SawETM . We develop a stochastic decoder by introducing Gaussian-distributed embeddings to better represent the topics in SawETM . Formally , the generative model with T latent layers can be expressed as x ( 1 ) j ∼ Pois ( Φ ( 1 ) θ ( 1 ) j ) , { θ ( t ) j ∼ Gam ( Φ ( t+1 ) θ ( t+1 ) j , 1/c ( t+1 ) j ) } T−1 t=1 , θ ( T ) j ∼ Gam ( γ , 1/c ( T+1 ) j ) , { Φ ( t ) k = Softmax ( log ( E ( s ) ( α ( t−1 ) , α ( t ) ) k ) ) } T t=1 , { α ( t ) k ∼ N ( x ; µ ( t ) k , Σ ( t ) k ) } T t=0 , ( 2 ) In the above formula , xj ∈ ZV denotes the word count vector of the jth document , which is factorized as the product of the factor loading matrix Φ ( 1 ) ∈ RV×K1+ and gamma distributed factor score θ ( 1 ) j under the Poisson likelihood ; and to obtain a multi-layer document representation , the hidden units θ ( t ) j ∈ R Kt + of the t th layer are further factorized into the product of the factor loading Φ ( t+1 ) ∈ RKt×Kt+1+ and hidden units of the next layer . The top-layer hidden units θ ( T ) j are sampled from a prior distribution . In addition , the kth topic α ( t ) k at layer t follows a Gaussian distribution with mean µ ( t ) k and covariance Σ ( t ) k , where the mean vector describes a semantic concept and the covariance matrix reflects its level of abstraction . Note that in the bottom layer α ( 0 ) represents the distributed embeddings of words . And Φ ( t ) k , used to capture the relationships between topics at two adjacent layers , is calculated based on the topic representations in the shared embedding space , instead of being sampled from a Dirichlet distribution . In particular , in the equation E ( s ) ( · ) refers to the symmetric similarity function defined in Eq . ( 1 ) . 2.3 Document encoder : Weibull upward and downward encoder networks . As presented in Section 2.2 , instead of using Gaussian latent variables like most of neural topic models [ 28 ] , our generative model employs the gamma distributed latent variables that are more suitable for modeling sparse and non-negative document representations . While in sampling based inference , the gamma distribution is commonly used to represent the conditional posterior of these latent variables , the difficulty of reparameterizing a gamma distributed random variable makes it difficult to apply it to an inference network [ 29 , 30 ] . For this reason , we utilize a Weibull upwarddownward variational encoder inspired by the work in Zhang et al . [ 25 , 31 ] . We let q ( θ ( t ) j | − ) = Weibull ( k ( t ) j , λ ( t ) j ) , ( 3 ) where , the parameters k ( t ) j , λ ( t ) j ∈ RKt are deterministically transformed from both the observed document features and the information from the stochastic up-down path θ ( t+1 ) j . In detail , the inference network can be expressed as h ′ ( 0 ) j = ReLU ( W ( 0 ) 1 xj + b ( 0 ) 1 ) ) h ′ ( t ) j = h ′ ( t−1 ) j + ReLU ( W ( t ) 1 h ′ ( t−1 ) j + b ( t ) 1 ) , t = 1 , · · · , T , h ( t ) j = h ′ ( t ) j ⊕Φ ( t+1 ) θ ( t+1 ) j , t = 0 , · · · , T − 1 , h ( T ) j = h ′ ( T ) j , k ( t ) j = Softplus ( W ( t ) 2 h ( t ) j + b ( t ) 2 ) , λ ( t ) j = Softplus ( W ( t ) 3 h ( t ) j + b ( t ) 3 ) , t = 0 , · · · , T , ( 4 ) where { b ( t ) i } 3 , T i=1 , t=1 ∈ RKt , { W ( t ) i } 3 , T i=1 , t=1 ∈ RKt×Kt−1 , and { h ( t ) j } N , T j=1 , t=1 ∈ RKt , ⊕ denotes the concatenation in feature dimension , ReLU ( · ) = max ( 0 , · ) is the nonlinear activation function , and Softplus ( · ) applies ln [ 1+ exp ( · ) ] to each element to ensure positive shape and scale parameters of the Weibull distribution . To reduce the risk of posterior collapse , a skip-connected deterministic upward path [ 14 ] is used to obtain the hidden representations { h ′ ( t ) j } N , T j=1 , t=1 of the input xj .","Existing deep hierarchical topic models are able to extract semantically meaningful topics from a text corpus in an unsupervised manner and automatically organize them into a topic hierarchy. However, it is unclear how to incorporate prior belief such as knowledge graph to guide the learning of the topic hierarchy. To address this issue, we introduce TopicNet as a deep hierarchical topic model that can inject prior structural knowledge as an inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. With an auto-encoding variational inference network, the model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations.","The authors propose a topic model that aim at embedding words and topics into a latent space via Gaussian embeddings. Additional constraints in the training procedure allows to incorporate prior knowledge on the topics, so that they are able to include semantic topic relations.  The model is evaluated on a large set of public datasets and on a number of tasks. It achieves better results with respect to baselines on clustering tasks, and comparable on classification tasks. However, the main advantage is the increase interpretability of topics and topic hierarchy.","In this article, the authors propose a novel knowledge-based hierarchical topic model called TopicNet, which incorporates structured prior knowledge into a deep neural topic model. TopicNet represents words and topics as Gaussian-distributed embeddings, and uses a stochastic decoder to inject prior knowledge into the model. The authors also introduce a novel variational encoder based on Weibull distributions to improve the inference of the model. Extensive experiments demonstrate the effectiveness of TopicNet in capturing semantic uncertainties and improving interpretability compared to existing hierarchical topic models.","The article proposes a novel knowledge-based hierarchical topic model, called TopicNet, that incorporates prior domain knowledge through a predefined topic hierarchy. The model uses Gaussian-distributed embeddings to accurately represent topics and their relationships across different layers. The proposed model is based on Gaussian SawETM, which employs the expected likelihood kernel similarity measure and a stochastic decoder to learn a deep topic hierarchy. Through extensive experiments, the authors demonstrate that TopicNet has competitive performance and better interpretability than most existing hierarchical topic models.","0,2,1",2,0,2,0,,coverage,other,"informative, coverage",informative,1,0,1,0,Summary B covers more relevant details but C is also good.,"Summary A uses more imprecise terms, but not bad.","Summary B is the most informative, striking good balance between details and coverage of aspects.","Summary A is too general, no details at all."
3,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"1 INTRODUCTION . In the past few years , deep neural networks ( DNNs ) ( Goodfellow et al. , 2016 ) have settled as the state-of-the art-techniques in many difficult tasks in a plurality of domains , such as image classification ( Krizhevsky et al. , 2012 ) , speech recognition ( Hinton et al. , 2012 ; Han et al. , 2018 ) , and machine translation ( Bahdanau et al. , 2014 ; van den Oord et al. , 2017 ) . This recent progress has been mainly due to their high accuracy and good generalization ability when dealing with realworld data . Unfortunately , DNNs are also highly confident when tested against unseen samples , even if the samples are vastly different from the ones employed during training ( Hendrycks & Gimpel , 2017 ) . Moreover , several works have shown that such deep networks are easily fooled by minor perturbations to the input ( Goodfellow et al. , 2014 ; Su et al. , 2017 ) . Obtaining a calibrated confidence score from a deep neural network is a problem under continuous investigation ( Hendrycks & Gimpel , 2017 ) and a major thread in artificial intelligence ( AI ) safety ( Amodei et al. , 2016 ) . In fact , knowing when the model is wrong or inaccurate has a direct impact in many production systems , such as self-driving cars , authentication and disease identification ( Akhtar & Mian , 2018 ; Goswami et al. , 2018 ) , to name a few . Guo et al . ( 2017 ) showed that despite producing significantly low classification errors , DNNs confidence scores are not faithful estimates of the true certainty . Their experiments confirmed that depth , width , weight decay , and batch normalization are the main reasons for overconfident scores . Moreover , they demonstrated that a simple and yet powerful method of temperature scaling in the softmax scores is an effective way to improve calibrate a DNNs confidence . While calibrating the classifier ’ s output to represent a faithful likelihood from the training ( in-distribution ) data has effective solutions , the problem of detecting whether or not the samples are generated from a known distribution ( out-of-distribution ) , is still an open problem ( Hendrycks & Gimpel , 2017 ) . One straightforward approach to calibrate the classifier ’ s confidence in order to detect samples whose distribution differs from the training samples distribution is to train a secondary classifier that digests both in-distribution ( ID ) and out-of-distribution ( OOD ) data so that anomalies are scored differently from ID samples , as performed in Hendrycks et al . ( 2018 ) . Re-training a network , however , can be computationally intensive and may even be intractable , since the number of OOD samples is virtually infinite . Other solutions rely on training both classification and generative neural net- works for OOD using a multi-task loss ( Lee et al. , 2018a ) , or using unsupervised fully convolutional networks as done by Sabokrou et al . ( 2016 ) to detect OOD in video samples . All these methods , however , have a major drawback : they require re-training a modified model using a different loss function possibly with additional parameters , which increases the computational burden , and demands access to the entire original ( and probably huge ) training data . In this work , we propose a novel OOD sample detection method that explores low-level statistics from feature layers . The statistics are obtained directly from the batch normalization layers ( Ioffe & Szegedy , 2015 ) , requiring no extra computations during training time , no changes to the network architecture and loss functions , nor preprocessing of the input image . During test time , the proposed method extracts statistics from selected layers and combines them into an OOD-ness score via a linear classifier . Throughout this paper , we observe that the mean and standard deviation of a given channel in a layer differ greatly between ID and OOD samples , which naturally motivates their use as features to be employed by an OOD detector . By selecting the BN layers of a network , we are able to normalize the features according to the learned batch statistics . The effectiveness of the proposed method is validated in two state-of-the-art DNN architectures ( DenseNet and ResNet ) ( Huang et al. , 2017 ; He et al. , 2015 ; 2016 ; Zagoruyko & Komodakis , 2016 ) that are trained for image classification tasks in popular datasets . The proposed approach achieves state-of-the-art performance , surpassing all competitors by a large margin in all tested scenarios , while being much more efficient . Notably , our method only requires one forward pass while Liang et al . ( 2018 ) ; Lee et al . ( 2018b ) ; Vyas et al . ( 2018 ) require two forward and one backward passes . The rest of the paper is organized as follows . Section 2 describes prior work on OOD samples detection . Section 3 introduces the proposed method , whereas Section 4 details all experiments and compares the results with state-of-the-art methods . Finally , we draw our conclusions in Section 5 . 2 PREVIOUS WORK . In this section , we describe recent prior work on OOD detection methods . A summary of all methods described can be seen in Table 1 . Hendrycks & Gimpel ( 2017 ) proposed a baseline method based on the posterior distribution ( i.e . softmax scores ) . They showed that well-trained models tend to produce higher scores for ID samples than for OOD ones . Hence their method comprises of applying a threshold on the softmaxnormalized output of a classifier . If the largest score is below the threshold , then the sample is considered OOD . Liang et al . ( 2018 ) continued the aforementioned line of work and proposed the Out-of-Distribution detector for Neural networks ( ODIN ) , which includes a temperature scaling T ∈ R+∗ to the softmax classifier as in Guo et al . ( 2017 ) . The authors in ODIN argued that a good manipulation of T eases the separation between in- and out-of-distribution samples . Allied to that , they also incorporated small perturbations to the input ( inspired by Goodfellow et al . ( 2014 ) ) whose goal is to increase the softmax score of the predicted class . Liang et al . ( 2018 ) found that this kind of perturbation has a stronger effect on ID samples than on OOD ones , increasing the separation between ID and OOD samples . ODIN outperforms the baseline method ( Hendrycks & Gimpel , 2017 ) by a fair margin ; however , it is three times slower due to the two forward and one backward passes needed to preprocess the input , while ( Hendrycks & Gimpel , 2017 ) only requires one forward pass . Vyas et al . ( 2018 ) describes a novel loss function , called margin entropy loss , over the softmax output that attempts to increase the distance between ID and OOD samples . During training , they partition the training data itself into ID and OOD by assigning samples labeled as certain classes as OOD and use the different partitions to train an ensemble of classifiers that are then used to detect OOD samples during test time . They also use the input pre-processing step proposed in Liang et al . ( 2018 ) , including temperature scaling . Lee et al . ( 2018b ) is the most recent work on OOD detection that we have knowledge of . They show that the posterior distribution defined by a generative classifier ( under Gaussian discriminant analysis ) is equivalent to that of the softmax classifier , and the generative classifier eases the separation between in- and out-of-distribution samples . The confidence score is defined using the Mahalanobis distance between the sample and the closest class-conditional Gaussian distribution . They argue that abnormal samples can be better characterized in the DNN feature space rather than the output space of softmax-based posterior distribution as done in previous work ( e.g. , ODIN ) . Samples are pre-processed similarly as done in ODIN , but the confidence score is increased instead of the softmax one . To further improve the performance , they also consider intermediate generative classifiers for all layers in the network . The final OOD sample detector is computed as an ensemble of confidence scores , chosen by training a logistic regression on validation samples . This method also shows remarkable results for detection of adversarial attacks and for incremental learning . 3 PROPOSED SOLUTION . An OOD detector should incorporate information from the training data in a natural manner , without being directly influenced by the loss function , which is intrinsically related to the task which could be well-defined for ID samples but be meaningless for most OOD samples . Moreover , if the OOD method is more dependent on the training distribution , it should be able to be applied to a wide variety networks , and not be designed specifically for a given architecture . Our method is based on a very simple observation . Input samples with different distributions generate statistically distinct feature spaces in a DNN . In other words , the deep features of an ID sample are distinct from those of an OOD one . Moreover , when using a normalization scheme , such as BN , the features are normalized by the statistics of the ID batch during training , possibly leading to feature statistics that are more similar to the batch statistics , as depicted in Figure 2 . The main problem then becomes how to summarize the feature space distribution for ID samples in a way that best discriminates between ID and OOD samples . In this work we show that using the first and second order statistics within each feature map performs remarkably well for this task . Figure 1 shows the proposed OOD detector , highlighting the feature statistics that are extracted for each channel at each BN layer . These statistics are then combined through a learned linear decision function , providing a measure of OOD-ness . The way we compute the per-channel statistics is formalized in Section 3.1 . Finally , the linear classifier used to combine the statistics from the different layers is described in Section 3.2 . 3.1 LOW-ORDER FEATURE STATISTICS . In the previous section , we motivated that characterizing the feature-space distributions might lead to a robust OOD detector . As a first approach , one could model these distributions using a nonparametric method to estimate the distribution of the features for each channel , which requires the computation of sufficient statistics using the training data or using a parametric method to fit the distribution ( Bishop , 2007 ) , which are both computationally intensive . Here , we propose to use only the mean and standard deviation computed along the spatial dimension for each channel to summarize the per-channel distribution . As it will be shown , these two statistics are sufficient to distinguish between ID and OOD . Moreover , because the mean and standard deviation of each channel are normalized by the running mean and variance computed during training by the BN layers ( Ioffe & Szegedy , 2015 ) , these statistics can be naturally combined within each layer to produce effective features for OOD detection . We describe such a procedure in what follows . Given the l-th BN layer with input tensor X ∈ RC×W×H , the output BN ( l ) c , i , j at channel c is Z ( l ) c , i , j = X ( l ) c , i , j − µ ( l ) c√ [ σ ( l ) c ] 2 + , BN ( l ) c , i , j = Z ( l ) c , i , jγ ( l ) c + β ( l ) c , ( 1 ) where γ ( l ) c > 0 and β ( l ) c are the per channel per layer learned scaling and shifting parameters , > 0 is a small constant value for numerical stability , µ ( l ) c ∈ R and [ σ ( l ) c ] 2 ∈ R+ are the mean and variance estimated through a moving average using the batch statistics during training , and Z ( l ) ∈ RC×W×H is the normalized feature tensor . It is worth noting that the statistics are calculated independently for each channel c at each layer l. In this paper , we conjecture that low-order statistics computed from either X ( l ) c , i , j or Z ( l ) c , i , j can be used to discriminate between ID and OOD samples . Given the unnormalized input X ( l ) c , i , j , we can compute the empirical mean m ( l ) c and standard deviation σ ( l ) c features for each channel c as m ( l ) c = 1 WH ∑ i , j X ( l ) c , i , j , [ s ( l ) c ] 2 = 1 WH ∑ i , j [ X ( l ) c , i , j ] 2 − [ m ( l ) c ] 2 , ( 2 ) and the features for the normalized tensor Z ( l ) c , i , j are defined as m̂ ( l ) c = 1 WH ∑ i , j Z ( l ) c , i , j = m ( l ) c − µc ( l ) σ ( l ) c [ ŝ ( l ) c ] 2 = 1 WH ∑ i , j [ Z ( l ) c , i , j ] 2 − [ m̂ ( l ) c ] 2 = [ ŝ ( l ) c σ ( l ) c ] 2 , ( 3 ) i.e. , the normalized mean feature m̂ ( l ) c represents the difference between the sample mean m ( l ) c and the BN running mean µ ( l ) c weighted by the running standard deviation σ ( l ) c , whereas ŝ ( l ) c is the normalized standard deviation by the BN running standard deviation σ ( l ) c .","The ability to detect when an input sample was not drawn from the training distribution is an important desirable property of deep neural networks. In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples. Specifically, we observe that the mean and standard deviation within feature maps differs greatly between in-distribution and out-of-distribution samples. Based on this observation, we propose a simple and efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model. The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute. Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize. The source code of our method will be made publicly available.","In this article, the authors propose a novel method for detecting out-of-distribution (OOD) samples in deep neural networks (DNNs). The method exploits low-level statistics from feature layers in the network to capture the differences between in-distribution (ID) and OOD samples. By selecting the batch normalization (BN) layers of a network and computing the mean and standard deviation of each channel, the proposed method achieves state-of-the-art performance in detecting OOD samples without requiring any additional computations during training or preprocessing of the input image.","There has been recent interest in using statistics and information summary measures to evaluate what deep nets are trying to do. Following the line of work, the paper suggests to use mean and variance of Z-scores accumulated across all layers/channels as features to distinguish ID and OOD samples. Simple idea but needs some work in its current format. ","The article proposes a novel method for detecting out-of-distribution (OOD) samples in deep neural networks (DNNs) by exploring low-level statistics from feature layers. The statistics are obtained directly from the batch normalization layers used in DNNs and require no extra computations during training time, no changes to the network architecture and loss functions, nor preprocessing of the input image. The proposed method achieves state-of-the-art performance in image classification and surpasses all competitors by a large margin in all tested scenarios, while being much more efficient.","2,0,1",,,,,,,,,,,,,,,,,
4,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"Functional magnetic resonance imaging ( fMRI ) is a notoriously noisy measurement of brain activity because of the large variations between individuals , signals marred by environmental differences during collection , and spatiotemporal averaging required by the measurement resolution . In addition , the data is extremely high dimensional , with the space of the activity typically having much lower intrinsic dimension . In order to understand the connection between stimuli of interest and brain activity , and analyze differences and commonalities between subjects , it becomes important to learn a meaningful embedding of the data that denoises , and reveals its intrinsic structure . Specifically , we assume that while noise varies significantly between individuals , true responses to stimuli will share common , low-dimensional features between subjects which are jointly discoverable . Similar approaches have been exploited previously but they have mainly used linear methods such as PCA and shared response modeling ( SRM ) . In contrast , we propose a neural network called MRMD-AE ( manifold-regularized multiple decoder , autoencoder ) , that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals . We show that our learned common space represents an extensible manifold ( where new points not seen during training can be mapped ) , improves the classification accuracy of stimulus features of unseen timepoints , as well as improves cross-subject translation of fMRI signals . We believe this framework can be used for many downstream applications such as guided brain-computer interface ( BCI ) training in the future . 1 INTRODUCTION . Brain activation dynamics , as measured by fMRI , exist in an extremely high-dimensional collection space and contain high levels of noise . Noise sources include measurement imprecisions and physiological factors such as a subject ’ s blood pressure variability , head motion , and respiration ( Brooks et al. , 2013 ; Mumford & Poldrack , 2007 ) . Moreover , even after accounting for the idiosyncrasies in noise , neural responses evoked by the same stimulus can vary greatly between different subjects . This makes using fMRI to extract meaningful signals or general trends that are task-relevant quite difficult . Experiments involving fMRI are expensive and usually involve pilot studies conducted in a smaller scale with simpler stimuli and on a smaller group of subjects . Thus , it is desirable to have a method that can learn common trends from pilot data and can generalize to a larger group of subjects or more complex tasks . Like for other high dimensional biomedical data , it is broadly acknowledged that neural activity actually lies in a lower-dimensional latent space . Moreover we hypothesize that several features of these latent spaces are shared across subjects given that measurements are usually task-based and are taken when subjects experience the same stimuli . Dimensionality reduction methods have been used to discover such low-dimensional spaces and facilitate understanding the underlying brain activities . Methods such as principal component analysis ( PCA ) and factor analysis have been the common choice ( Smith et al. , 2014 ) . However , these linear methods are sensitive to noise and restrict interactions in the latent space . Nonlinear dimensionality reduction algorithms are shown to better capture the geometry of high-dimensional biological data ( Moon et al. , 2019 ; Van der Maaten & Hinton , 2008 ) and account for dynamics in neural activities ( Gao et al. , 2020 ) . PHATE ( Moon et al. , 2019 ) , a diffusion based manifold learning method , was shown to capture both global and local geometry of complex biological data , which appears to be a good candidate to model fMRI data . Using a nonlinear manifold such as PHATE to reduce the dimensionality of data is appealing as it can extract meaningful signals , denoise the data , and accelerate downstream analysis . However , unlike a method like PCA which learns a projection operator that can be applied to new data , manifold learning methods are fixed to the input data and do not naturally extend to new data from new tasks or from new subjects . To extend an existing manifold to out-of-sample data , landmark interpolation or Nyström extensions are commonly used . However it is usually unsatisfactory as shown by decremented performance ( Appendix Fig . 1 ) . To tackle these shortcomings , we turn to neural networks such as autoencoders that provide nonlinear dimensionality reduction via a learned parametric function that is readily applicable to new data . While our key goal is learning an informative low-dimensional data manifold shared across multiple subjects , this is not something automatically done by autoencoders . First , there is nothing enforcing autoencoder latent embeddings to respect data manifold geometry , and indeed they often just spread out points for ease of decoding . Second , individual variation often dominates embeddings which prevents a common space to be automatically learned . To address these issues , we propose a manifold-regularized multi-decoder autoencoder ( MRMD-AE ) that can process fMRI data from a group of subjects and extract common latent space representations that respect individual data geometry . Key features of the MRMD-AE include : ( a ) A common encoder that projects fMRI data from any subject on to a shared latent space , and subject-specific decoders that learn to reconstruct data for each subject faithfully . ( b ) A manifold-geometry regularization of the latent space based on a precomputed PHATE embedding . ( c ) Penalties for distances between individual patient encodings of common stimuli to ensure that the latent space is not split into individual embeddings . We show results on three types of tasks in two different datasets . First , we show that we are able to extend the manifold embedding to new timepoints of data not used in training . This shows the extensibility of our manifold in contrast to non-neural network based manifold learning methods . Second , we show that our latent space improves the ability to classify or infer stimulus features based on subject fMRI measurements . Surprisingly , the multiple decoder improves classification accuracy over a common decoder , showing that this framework allows the network to separate common from individual variations . Finally , we show that even untrained , translation between subjects has increased accuracy on withheld timepoints . Further , we show highly improved cross subject translation after training for this task . 2 RELATED WORK . Manifold Learning Manifold learning assumes that high-dimensional ambient data X ∈ Rd lies on a low dimensional manifold . Given a set of datapoints measured in the ambient space , assumed to be sampled from a manifoldM , manifold learning optimizes a low dimensional Euclidean space that encodes the intrinsic geometry ofM and the original data . Popular manifold learning methods include diffusion maps ( Coifman & Lafon , 2006 ) , Laplacian Eigenmaps ( Belkin & Niyogi , 2003 ) and PHATE ( Moon et al. , 2019 ) . We note that multiscale manifold learning methods ( Wolf et al. , 2012 ; Brugnone et al. , 2019 ; Kuchroo et al. , 2020 ) can be used to consider multiple manifolds that capture relations and structure in data at different resolutions . Other methods , such as tSNE ( Van der Maaten & Hinton , 2008 ) and UMAP ( McInnes et al. , 2018 ) , relax the manifold assumption to focus on neighborhood preservation in low dimensions . While these are often considered as variations of manifold learning , they are mostly suitable for visualization of clustered data that does not necessarily have global structure , and typically do not preserve overarching trends or relations between data regions ( see , e.g. , Moon et al. , 2019 ; Gigante et al. , 2019 ) . SAUCIE In ( Amodio et al. , 2019 ) , SAUCIE ( Sparse Autoencoder for Clustering , Imputation , and Embedding ) is an autoencoder-based generative model that manipulated internal representations to force the network to produce specific desired transformations in the data . By imposing these structure on the latent representations of data , it can quickly extract information about datasets more massive than alternative approaches can easily handle . Further , this framework establishes that multiple regularizations , in conjunction , can be used to encourage interpretable latent representations that accentuate biologically reliable relations in data coming from multi-patient cohorts , as demonstrated , e.g. , in immune response profiling ( Zhao et al. , 2020 ) . Inspired by the success of this approach in immunology , we apply a similar multi-objective neural network approach here to process multisubject fMRI data and alleviate some of the main challenges associated with this data type . SRM Shared response modeling ( SRM ) is a linear method to account for differences in functional topographies in fMRI data in a low-dimensional space . SRM uses a singular value decomposition approach to learn a set of low-dimensional signals ( shared response ) common to multiple subject ’ s fMRI data during the same experiment . SRM learns subject-specific transformation to align subject data with the shared model using an orthonormally-constrained weight matrix , similar to that of PCA ( Chen et al. , 2015 ) . 3 PRELIMINARIES . Manifold learning and dimensionality reduction with PHATE To capture the complex interaction in the latent space and balanced global and local structures of highly noisy fMRI data , we use Potential of Heat-diffusion for Affinity-based Transition Embedding ( PHATE ) ( Moon et al. , 2019 ) , a manifold learning method based on diffusion geometry ( Coifman & Lafon , 2006 ) . PHATE computes the diffusion operator which is the probability transition matrix P between data point pairs via an α−decay kernel . Then by raising P to the power of t , the PHATE optimal diffusion time scale , it simulates a t−step random walk over the data graph . Diffusion potential distances D is extracted from P t , which accounts for both global and local data geometry . Finally , metric multi-dimensional scaling ( metric MDS ) ( Abdi , 2007 ) , is applied to D to get the PHATE embedding of input data . PHATE has been shown to effectively capture geometry of high-dimensional biological data and was shown to preserve manifold structure in fMRI data in ( Rieck et al. , 2020 ) . We utilize the extensible framework of PHATE and use PHATE embedding of the training data as regularization applied to the manifold embedding layer as shown in Fig . 1 . Note that we use PHATE over other popular dimensionality reduction methods such as tSNE or UMAP because of its established ability to retain manifold and trajectory structure in biomedical data ( Moon et al. , 2019 ; Kuchroo et al. , 2020 ; Chung et al. , 2020 ; Pappalardo et al. , 2020 ) . Geometry-regularized Autoencoders ( GRAE ) The latent space of the vanilla autoencoders are usually difficult to interpret and not necessarily reflect data geometry ( see , e.g. , Mishne et al. , 2019 ; Moor et al. , 2020 ; Duque et al. , 2020 ) . In order to encourage meaningful data geometry to emerge , or be retained , in latent representations , the training process can be enhanced with appropriate regularizations , often derived from manifold learning principles . For example , Jia et al . ( 2015 ) and Yu et al . ( 2013 ) propose regularization terms that penalize inaccurate preservation of neighborhood relationships in the ambient space . Recent work by Duque et al . ( 2020 ) has shown that the geometry extracted by PHATE can be leveraged to formulate Geometry-Regularized Autoencoders ( GRAE ) that faithfully capture the instrinsic data geometry , while demonstrating advantages in extendibility and invertibility compared to previous approaches . In this work we adapt this approach to fMRI data in conjunction with adjusted architecture and additional regularizations that address further challenges encountered in these settings . Cross-subject batch effects in fMRI Relative to other neuroimaging methods , fMRI is noninvasive , minimal risk , and has broad coverage with high spatial resolution . fMRI experiments commonly use naturalistic stimuli such as movies to probe an array of cognitive mechanisms in conditions that closely mimic the human experience of the world . However , the data generated by such experiments are notoriously high-dimensional and complex , as fMRI is rife with noise from sources like the scanner , subject movement , and the measured blood-oxygen-level-dependent ( BOLD ) signal , which is a secondary effect of neural activity . This makes it difficult to aggregate data across sessions , either within or between subjects in a study . Beyond the noise sources of fMRI data , another problem with aggregating data across subjects to uncover something common about brain functioning pertains to the idiosyncratic nature of functional topographies in the brain . In many brain regions , shared stimuli will reliably evoke similar patterns of neural activity across subjects ( Nastase et al. , 2019 ) . However , that signal of interest could be located in slightly different anatomical locations across subjects or otherwise obscured by noise , causing a mismatch between functional and anatomical locations for a signal . A large body of neuroimaging research centers around rectifying this problem with a variety of linear functional alignment methods by learning either high-dimensional ( Haxby et al. , 2011 ; Busch et al. , 2021 ) or reduced-dimensional ( Chen et al. , 2015 ) common spaces . A solution to this issue must consider both the complexity of the data at the individual subject level , where there is biological and environmental noise , and the broader cross-subject representational idiosyncrasies .","Functional magnetic resonance imaging (fMRI) is a notoriously noisy measurement of brain activity because of the large variations between individuals, signals marred by environmental differences during collection, and spatiotemporal averaging required by the measurement resolution. In addition, the data is extremely high dimensional, with the space of the activity typically having much lower intrinsic dimension. In order to understand the connection between stimuli of interest and brain activity, and analyze differences and commonalities between subjects, it becomes important to learn a meaningful embedding of the data that denoises, and reveals its intrinsic structure. Specifically, we assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. Similar approaches have been exploited previously but they have mainly used linear methods such as PCA and shared response modeling (SRM). In contrast, we propose a neural network called MRMD-AE (manifold-regularized multiple decoder, autoencoder), that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. We show that our learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, as well as improves cross-subject translation of fMRI signals. We believe this framework can be used for many downstream applications such as guided brain-computer interface (BCI) training in the future.","The paper proposes a model to learn a low-dimensional representation of fMRI data over multiple subjects of the same experiment. The model is built as an auto-encoder, with an encoder shared across subjects, and a separate decoder per subject. The model is regularized so that the first layer of each decoder gives a representation close to a pre-computed manifold embedding. An optional regularization also constrains the shared representation to be similar across subjects.  The paper then proposes a series of experiments to demonstrate the benefits of the learn representation. The experiments consider the tasks of (a) projecting new test samples to the learned manifold, (b) classify some stimulus features from the embedding (decoding task), and (c) predict brain recordings of a new subject.","Functional magnetic resonance imaging (fMRI) is a notoriously noisy measurement of brain activity due to high variations between individuals, environmental differences during collection, and spatiotemporal averaging required by the measurement resolution. To address these challenges, a new framework called Manifold-Regularized Multi-Decoder Autoencoder (MRMD-AE) is proposed, which can learn a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. The MRMD-AE consists of a common encoder, multiple decoders, and a manifold-geometry regularization based on a precomputed PHATE embedding. The multiple decoders improve classification accuracy over a common decoder, allowing the network to separate common from individual variations. The proposed framework shows improved cross-subject translation of fMRI signals and extensibility to new timepoints not used in training.","Functional magnetic resonance imaging (fMRI) is a challenging area in neuroscience, given the large variations between individuals, signals marred by environmental differences during collection and spatiotemporal averaging required by the measurement resolution. To understand the connection between stimuli and brain activity, and to analyse differences and commonalities between subjects, it becomes important to learn a meaningful embedding of the data that denoises and reveals its intrinsic structure. Researchers have created a neural network called MRMD-AE that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. The learned common space represents an extensible manifold, improves the classification accuracy of stimulus features of unseen timepoints, as well as improves cross-subject translation of fMRI signals. This framework can be used for many downstream applications such as guided brain-computer interface training in the future.","0,2,1",2,1,"0,2",1,,coverage,coverage,coverage,"coverage, relevance (+ detail)",1,2,"0,1",2,Summary B has a better coverage.,Summary C focus too much on background info.,The summaries present good coverage of details.,The summary gives little to no background information and instead elaborates too much on the experimental part.
5,SP:0355b54430b39b52df94014d78289dd6e1e81795,"1 INTRODUCTION . Image restoration has been researched for many years , but in a case-by-case way ( Park et al. , 2003 ; Mairal et al. , 2008 ; Guillemot & Le Meur , 2014 ) . Almost all image restoration algorithms are only designed for certain type of images or degradation . This research paradigm has some obvious disadvantages . It is exhausting to invent new algorithms or train new models for slightly different situations . Even if we can , those specialized solutions are not so elegant , because they are very unlike one another even though the problems they focus on are fundamentally so similar . It is worth noting that any image degradation process can be abstracted as a many-to-one function . More specifically , for any given degradation process , one degraded image could be degraded from many possible original images . From that point of view , we propose a general method for various image restoration problems , such as denoising , deblurring , super-resolution and inpainting . Our algorithm chooses the most probable original image from all those possible original images , and uses it as the restoration of the given degraded image . To be more precise , the general image restoration is formulated as a constrained optimization problem . Its objective is to maximize a posteriori probability of latent variables , and its constraint is that the image generated by these latent variables must be the same as the degraded image . Recent progress of density estimation techniques makes our algorithm possible . In the field of image generation , Generative Adversarial Networks ( GANs ) make a huge success in recent years ( Goodfellow et al. , 2014 ; Radford et al. , 2015 ) . As research continues , images generated by GANs become more and more realistic and clear , and training procedure of GANs become more and more stable ( Salimans et al. , 2016 ; Arjovsky et al. , 2017 ) . Besides being an image generation technique , GANs can also be used for density estimation . The generator part of a GAN is an implicit probability distribution model , and it will converge to a good estimator of the data distribution after training . In this work , we solve the inference problem with the probability density estimated by a GAN . Figure 1 provides an illustration of how our image restoration method works . There are four dashed boxes from left to right in Figure 1 , corresponding to four different phases of image capture and restoration process . Images in the first dashed box are original images , which are clear and undegraded . These images undergo a series of degradation in the second dashed box , and then are captured by our camera . In the image restoration process , we hope to estimate the original images with the degraded images we captured . As we pointed out before , every degraded image could be degraded from many possible original images . To be more precise , there is a particular subset of the original image manifold for any degraded image , and all image samples on the submanifold could be degraded to the given degraded image . Images in the third dashed box are those samples on the submanifold , and they are arranged in ascending order of log-likelihood from left to right . Images marked by yellow boxes are samples with the highest log-likelihood in their group , and they are placed in the last dashed box as restoration outputs . Overall , the contributions of this work are mainly in two aspects : 1 . We propose a general method for various image restoration problems . In the method , we explicitly use density information estimated by a GAN , an implicit model ; and we directly solve the image restoration problem , an inference problem , with a GAN , a generative model . To the best of our knowledge , our work is the first to do those two things . 2 . We propose a new algorithm to solve the optimization problem in our method . The new optimization algorithm is a first-order iterative algorithm for constrained problems , and it works well even for problems with highly nonlinear objective and constraints . These features make it especially suited to neural network related constrained optimization problems . 2 RELATED WORK . The most similar work to ours is proposed in Yeh et al . ( 2017 ) . They propose a semantic image inpainting method , which can generate missing content with a trained GAN . They search in the latent space of the trained GAN for the image which is closest to the corrupted image , and use the discriminator loss of the trained GAN as an indicator of how realistic their restoration is . Their motivation is similar to our work , but unfortunately , there is a major theoretical flaw in their method . Goodfellow et al . ( 2014 ) already prove that the discriminator is unable to identify how realistic an input is after several steps of training , if the GAN has enough capacity . During the training , the information of the data distribution gradually transfer from the discriminator to the generator . Ideally , the generator will have all the information of the data distribution while the discriminator will have none . That is why we use the generator of a trained GAN as an implicit probability density model in our method . Another difference between their work and ours is that they only focus on image inpainting problem , while our method applies to various image restoration problems . The maximum a posteriori ( MAP ) has existed for a long time as a classic estimation method ( Campisi & Egiazarian , 2016 ) . But before GANs , people do not have a probability density model which is good enough to describe the distribution of images . After GANs make a huge success in image generation , researchers start to use them in image restoration tasks to get more realistic results ( Isola et al. , 2017 ; Bousmalis et al. , 2017 ) . Ledig et al . ( 2017 ) and Sønderby et al . ( 2016 ) try to use the MAP estimation on GANs to solve image super-resolution problem . However , they only use the MAP estimation implicitly and indirectly , while our method use it explicitly and directly . We suspect that all methods do implicit MAP estimation on GANs would require redesigning or retraining when the image restoration task changes , and this makes implicit methods not as general as our explicit method . Ulyanov et al . ( 2017 ) is another work which is seemingly similar to ours , but they are actually quite different . Their work uses a randomly-initialized neural network as an image prior to solve various image restoration problems . The prior in their method is elaborate , neural network related but still handcrafted , while in our method the prior is learned from data . So our data-driven prior has better adaptability to specific image distribution . 3 MAXIMUM A POSTERIORI ON A SUBMANIFOLD . 3.1 FORMULATION . Consider a general image degradation model as follows , x̃ = F ( x , Ω ) ( 1 ) where x , x̃ , and Ω represent the original image , the degraded image , and the parameters of the degradation model , respectively . The image degradation function F is a deterministic function . That means , given an original image x and a particular set of parameters Ω , the image degradation model will always produce the same degraded image x̃ . Our goal is to get a reasonable estimate of x with given x̃ and F . In this paper , we use the maximum a posteriori probability ( MAP ) estimate of x as the restoration of x̃ . Compared to MSE-based method , MAP estimate of x is perceptually more convincing . We can perform inference by maximizing the posterior p ( x , Ω|x̃ ) : { x̂ , Ω̂ } =argmax x , Ω p ( x , Ω|x̃ ) = argmax x , Ω p ( x̃|x , Ω ) p ( x|Ω ) p ( Ω ) p ( x̃ ) ( 2 ) where x̂ and Ω̂ represent MAP estimate of x andΩ . Note that p ( x̃ ) is always positive and does not depend on x andΩ , and typically we assume that x andΩ are independent . Therefore , { x̂ , Ω̂ } =argmax x , Ω p ( x̃|x , Ω ) p ( x ) p ( Ω ) ( 3 ) Note that x̃ = F ( x , Ω ) is a deterministic function , i.e. , p ( x̃|x , Ω ) = δ ( x̃− F ( x , Ω ) ) . Therefore , the estimation is equivalent to { x̂ , Ω̂ } = argmax x , Ω p ( x ) p ( Ω ) s.t . ‖x̃− F ( x , Ω ) ‖ = 0 ( 4 ) Here we write p ( x ) more specifically as pr ( x ) , which stand for the probability density of real data distribution . We can estimate pr ( x ) with the generator part of a trained GAN , which is an implicit probability distribution model with distribution pG ( x ) . The trained generator G represents a mapping from latent space of z to data distribution of original image x , i.e. , pr ( x ) = pG ( x ) , and pG ( x ) is a probability density function implicitly defined by x = G ( z ) , where z is typically sampled from some simple distribution , such as the uniform distribution or the normal distribution . Assuming G : Rn → Rm is an injective function , the estimation is equivalent to { ẑ , Ω̂ } = argmax z , Ω pG ( G ( z ) ) p ( Ω ) s.t . ‖x̃− F ( G ( z ) , Ω ) ‖ = 0 ( 5 ) and x̂ = G ( ẑ ) ( 6 ) Generally the dimension of vector space of z is far lower than the dimension of vector space of x . Note that pG ( x ) is nonnegative if and only if x is on the low dimensional manifoldM defined by x = G ( z ) , we can replace the probability density on the original space pG ( G ( z ) ) in Eq . ( 5 ) by the probability density on the manifold pM ( z ) , and end up with the same estimation result ẑ . According to Pennec ( 2004 ) , the probability density on the manifold can be calculated by pM ( z ) = p ( z ) √ detGram ( ∂G∂z1 , . . . , ∂G ∂zn ) ( 7 ) where Gram represents the Gram matrix , and √ detGram ( ∂G∂z1 , . . . , ∂G ∂zn ) is the volume of the parallelotope spanned by the vectors ( ∂G∂z1 , . . . , ∂G ∂zn ) , so the square root of the Gram determinant can serve as a local scale factor . It has an effect similar to the Jacobian determinant , but we can only use the Gram determinant here because G is a function from Rn to Rm , and generally n is much less than m. The Gram matrix can be simply calculated byGram ( ∂G∂z1 , . . . , ∂G ∂zn ) = V TV , where V is anm×n matrix , whose entries are given by V ij = ∂xi∂zj . Therefore , Eq . ( 5 ) is equivalent to { ẑ , Ω̂ } = argmax z , Ω p ( z ) p ( Ω ) √ detV TV s.t . ‖x̃− F ( G ( z ) , Ω ) ‖ = 0 ( 8 ) To solve the estimation problem efficiently , we represent probabilities in Eq . ( 8 ) in logarithmic space , i.e. , { ẑ , Ω̂ } = argmax z , Ω − 1 2 log detV TV + log p ( z ) + log p ( Ω ) s.t . ‖x̃− F ( G ( z ) , Ω ) ‖ = 0 ( 9 ) Matrix V TV is a positive-definite matrix , so we can use Cholesky decomposition to calculate log detV TV efficiently , i.e. , log detV TV = 2 tr ( log ( chol ( V TV ) ) ) ( 10 ) Finally we deduce a set of expressions which can be calculated directly , and their final outcome x̂ is the restored image we want , i.e. , { ẑ , Ω̂ } = argmax z , Ω − tr ( log ( chol ( V TV ) ) ) + log p ( z ) + log p ( Ω ) s.t . ‖x̃− F ( G ( z ) , Ω ) ‖ = 0 ( 11 ) and x̂ = G ( ẑ ) ( 12 ) Note that ( G ( z ) , Ω ) form a low dimensional manifold which is embedded in the space of ( x , Ω ) , and the feasible solutions of Eq . ( 11 ) is on a subset of the manifold , which is defined by ‖x̃ − F ( G ( z ) , Ω ) ‖ = 0 . So our method basically makes a MAP estimate on a submanifold . Figure 2 is a toy example to show how our method works in a very visible way . Suppose there is a grayscale original image x , which has only three pixels . Then it is downsampled to only one pixel during the image capture process , and our task is to estimate xwith the one pixel image we captured . Suppose we have trained a GAN as an implicit model of data distribution of x . More specifically , the generator of the trained GAN represents a mapping from its input noise z to data distribution of x . The left part of Figure 2 describes the two dimensional latent space of z . We use the saturation of orange color to represent probability density level , i.e. , a thicker orange color means a higher probability density . So the uniform orange color in the latent space means that the input noise z is sampled from a uniform distribution . Then the two dimensional vector z is mapped to three dimensional space of image x by the generator of the trained GAN , and the big orange square in the latent space of z is transformed into a twisted torus in the three dimensional data space of x , which is described in the right part of Figure 2 . Some areas in space of z expand during the transformation , while other areas shrink . We can find this out by comparing the red and blue quadrilateral between the latent and data space . Therefore , the probability density on the torus is no longer uniform . The orange colors of the expanded areas become lighter , and the colors of the shrunken areas become thicker . Quantitatively speaking , the square root of the Gram determinant in Eq . ( 7 ) is the local area scale factor of the mapping , and its inverse , of course , is the local density scale factor . The pale yellow plane in the data space represents the constraint in the toy example . All points on the plane would exactly be downsampled to the one pixel image we captured . So the intersection curve of the plane and the torus is the submanifold we are looking for , and that white curve is the feasible set of the toy problem . In this problem , p ( z ) is a constant in the domain , and degradation parameters Ω does not exist at all . According to Eq . ( 8 ) , what we need to do is to maximize the inverse of the square root of the Gram determinant on the submanifold . In other words , the point with the thickest orange color on the intersection curve is the restored image x̂ , the MAP estimate on the submanifold . We can find out that the method is both intuitive and rational for this toy example .","We propose a general method for various image restoration problems, such as denoising, deblurring, super-resolution and inpainting. The problem is formulated as a constrained optimization problem. Its objective is to maximize a posteriori probability of latent variables, and its constraint is that the image generated by these latent variables must be the same as the degraded image. We use a Generative Adversarial Network (GAN) as our density estimation model. Convincing results are obtained on MNIST dataset.","This paper proposed a general method for image restoration based on GAN. In particular, the latent variable z is optimized based on the MAP framework. And the results are obtained by G(z). This method looks reasonable to achieve good results. However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all. ","This article proposes a new method for image restoration, which involves using a trained GAN as an implicit probability density model to estimate the original image from a degraded image. The method formulates the image restoration problem as a constrained optimization problem, and solves it using a new optimization algorithm that is well-suited for neural network-related constrained optimization problems. The proposed method can be applied to various image restoration problems, including denoising, deblurring, super-resolution, and inpainting, and can provide more realistic results than traditional methods.","The article proposes a general method for image restoration of degraded images using density information estimated by a generative adversarial network (GAN). The algorithm chooses the most probable original image from all those possible using probabilistic maximum a posteriori estimation. The method is shown to be effective for various image restoration tasks such as denoising, deblurring, super-resolution, and inpainting.","0,2,1",,,,,,,,,,,,,,,,,
6,SP:c5883e3a59e6575eff044251b38175a6ed024034,"1 INTRODUCTION . In the context of supervised classification , a major factor for consideration is the generalization error of the classifier , i.e. , how good a classifier generalizes to test ( unseen ) data points . The notion of overfitting describes the case when the test error significantly exceeds the training error . Naturally , the objective for building a robust classifier entails the minimization of this generalization gap , to avoid overfitting . To that end , statistical studies on generalization error ( Blumer et al . ( 1989 ) ; Bartlett & Mendelson ( 2003 ) ) find that complexity measures on the classifier function space , F , often directly control the generalization gap of a classifier . Two prominent examples of such measures include the Rademacher Complexity Rm ( F ) ( Bartlett & Mendelson ( 2003 ) ) and the VC dimension ( Blumer et al . ( 1989 ) ) V C ( F ) . Both measures directly estimate the flexibility of a function space , i.e. , how likely is it for F to contain functions that can fit any random labelling over a set of data points . In this paper , we work with Rademacher complexity and propose extensions that provide a new perspective on generalization error . From a statistical perspective , the generalization gap can be understood through convergence bounds on the error function , i.e. , the expected deviation of the error function on the test data compared to the training data . Traditional generalization error bounds ( Bartlett & Mendelson , 2003 ) state that function complexity ( i.e. , Rm ( F ) ) directly corresponds to the generalization gap . Thus , higher Rm ( F ) usually leads to a greater generalization gap and slower convergence . Although the originalRm ( F ) was proposed for binary classification , similar results have been shown for multi-class settings and a larger variety of loss functions ( Xu et al. , 2016 ; Liao et al. , 2018 ) . Note thatRm ( F ) is over the entire function space and thus global in nature . Local forms of Rademacher complexity , which involve restricting the function space and lead to minimum error on the training data samples , have been proposed ( Bartlett et al. , 2005 ; 2002 ) . Apart from function complexity based measures , there is also considerable work which uses an information theoretic perspective ( Xu & Raginsky , 2017 ; Russo & Zou , 2020 ; Bu et al. , 2020 ; Haghifam et al. , 2020 ) in treating the subject . 1.1 WHY THE GROUND TRUTH LABEL GENERATING FUNCTION MATTERS . We define the label generating function for a classification problem as the function which generates the true labels for all possible datapoints . Note that most generalization error bounds , including the traditional ones , primarily are introspective in nature , i.e. , they consider the size and flexibility of the classifier ’ s function space F . The main direction proposed in this work is the investigation of the unknowability of the ground truth label generating functions ( LGF ) , using another function space which we call the generator space . The generalization error bounds in Bartlett & Mendelson ( 2003 ) state that the difference in test and training performance is roughly bounded above by the Rademacher Complexity of the classifier ’ s function space ( i.e. , Rm ( F ) ) . In other words , whatever the training error , the test error will always be likely to be greater by an amountRm ( F ) on average . We note that , in deriving the original bound , a major assumption is that the LGF g is fixed and knowable from the data . We now outline our main argument for taking the generator space into account . The LGF is indeed fixed , i.e. , there can not be two different ground truth label generating functions applicable to the same problem . However , our primary emphasis is on the fact that the true LGF will always be unknown , i.e. , for any finite training data containing data-label pairs ( zi , g ( zi ) ) , we would only truly know the output of the label generating function on the given training data samples . Only when we have infinite training data samples , the values of LGF are known at each z ∈ Rd . In this work , we denote the function space of all possible LGFs , within which the true LGF is contained , as the generator space . Note that the generator space arises due to the unknowability of the LGF . We show in this work that due to the generator space , the true generalization gap is greater than the Rademacher complexity of the classifier , and also depends on the Rademacher complexity of the generator . Note that the size of the generator space , which dictates its complexity , will be dependent on the amount of constraints that the LGF has : if the LGF has no constraints at all , then generator spaces are larger , whereas if the LGF is constrained to be smooth/invariant to many transformations , generator spaces are smaller ( as the set of functions which are smooth and invariant are also much smaller , for instance in vision ) . Let us consider the case where the LGF g has no constraints at all , i.e. , it is sampled from a generator space G which contains all possible functions g : Rd −→ { −1 , 1 } . In this case , the function g is expected to have no structure and behaves like a random function , and thus the expected test accuracy of any classifier will be 50 % ( i.e. , random chance ) . Therefore , even if the classifier function f ∈ F produces a very good fit on the training data and F happens to have a low complexity measure Rm ( F ) , the generalization performance will still be poor , as no knowledge of the LGF values on the unseen datapoints is available from the training samples . This is in contrast to the generalization error bounds based on Rademacher complexity , which would estimate that the classifier should have good generalization performance ( i.e. , low test error ) , as both Rm ( F ) and training error are low . Note that although a typical training dataset extracted from this LGF may be hard to fit using a low-complexity classifier F , there will be examples of training instances with non-zero probability on which a low-complexity classifier can have a good fit . The takeaway from this example is that the structure of the data ( here represented using the complexity of the generator space ) can additionally dictate whether a classifier can generalize . Note that , in this scenario , the expected generalization performance would be better if the LGF had more structure . Figure 1 illustrates the role of both generator and classifier spaces in generalization via the two example scenarios discussed earlier . In both examples , the same low-complexity classifier has a good fit on the training data , thusRm ( F ) is low . In example ( a ) , the LGF has no constraints ; while , in example ( b ) , the LGF has constraints such as local smoothness . In example ( a ) , the classifier clearly exhibits poor generalization performance on the test data . This agrees with our previous argument that if the LGF has no constraints , generalization is essentially impossible . In example ( b ) , the classifier shows much more robust generalization performance on the test data , due to the LGF having significantly more structure . Furthermore , it is intuitively clear that a classifier function space F which has a high overlap with the generator space G ( and therefore its constraints ) should yield good generalization performance . This shows , that in addition to the function spaces F and G individually affecting the generalization gap , the similarities between F and G are also important . Both of these perspectives play leading roles in our construction of generator-and-classifier aware complexity measures and the associated novel generalization error bounds . 1.2 RELATED WORK . To the best of our knowledge , the approach we are proposing to study generalization performance is novel and there is not much directly related work . Here , we describe some examples of works which discuss relevant concepts . The no free lunch theorem proposed in Wolpert & Macready ( 1997 ) indirectly sheds light on the behaviour of the LGFs . However , it does not incorporate ways to reduce variability in the LGFs by considering constraints related to the classification problem . Invariance constraints in learning algorithms were studied in Sokolic et al . ( 2017 ) , where the input space was factored into invariant transformations , similar to what we also do in this work . In doing so , the complexity of the data was indirectly explored , based on the number of invariant transformations present in the input space . However , the generalization bounds were derived with an assumption of perfectly invariant classifier function spaces , which is not applicable for CNNs and their variants ( as shown in Kauderer-Abrams ( 2017 ) ) . In another relevant study ( Jin et al . ( 2019 ) ) , a cover-complexity measure of a single dataset was proposed , and the generalization error of fully connected networks was analyzed with respect to the same . However , invariances in the dataset and the learning algorithm were neglected , i.e. , the similarities between the generator and the classifier spaces were not studied . 1.3 KEY CONTRIBUTIONS . The contributions of this work are as follows : 1 . We propose a novel complexity measure between the classifier function space F and the generator function space G called co-complexity , which we use to derive new , more accurate global estimates of generator-aware generalization error bounds ( Theorem 3 and 4 ) . Co-complexity considers not only the complexity of the generator and classifier function spaces , but also the similarities between them . Doing so allows for the a more exhaustive look into generalization error . 2 . We decompose co-complexity into two different measures of complexity , invariance co-complexity and dissociation co-complexity , which are used to derive new generalization error bounds , including bounds on the expected training error ( Theorem 6 ) . We find that reducing invariance co-complexity while keeping the dissociation co-complexity unchanged , helps reduce the generalization gap ( low variance ) while maintaining low training error ( low bias ) . This emphasizes the importance of having classifiers that share invariance properties with generator spaces , e.g. , rotation-invariant CNNs ( Cohen & Welling ( 2016b ) ) on MNIST-Rot ( Larochelle et al . ( 2007 ) ) . 3 . We present empirical validation of co-complexity measures of CNN and its scale-equivariant and rotation-equivariant counterparts ( SE-CNN in Sosnovik et al . ( 2019 ) , RE-CNN in Cohen & Welling ( 2016b ) ) , which explains their superior generalization ability compared to MLPs . Our proposed error bounds are easily specialized to the case where ground truth label function is fixed , leading to potentially tighter generalization error bounds ( see Appendix A ) . Although our proposed measures are global in nature , local variants can be derived via similar extensions used in Bartlett et al . ( 2005 ) . 2 DEFINITIONS . Assume that we have m number of d-dimensional i.i.d sampled instances S = [ z1 , z2 , .. , zm ] drawn from some distribution P , and another set ofm i.i.d sampled instances S′ = [ z′1 , z ′ 2 , .. , z ′ m ] also drawn from P . Define σ = [ σ1 , σ2 , .. , σm ] where σi are i.i.d random variables following the Rademacher distribution ( Pr ( σi = +1 ) = Pr ( σi = −1 ) = 0.5 ) . F and G are two function spaces from Rd −→ { −1 , 1 } and we assume that they are defined at all points in Rd . In the context of our problem , F will be the classifier ’ s function space , whereas G will be the generator space . Rademacher Complexity ( Bartlett & Mendelson ( 2003 ) ) : First , we provide the definition of Rademacher Complexity : Rm ( F ) = E σ , S [ sup f∈F ( 1 m m∑ i=1 f ( zi ) σi ) ] ( 1 ) It can be seen thatRm ( F ) indicates the noisy-label fitting ability of the classifier ’ s function space F . Correlated Rademacher Complexity : We propose a modified form of the original Rademacher complexity , called Correlated Rademacher Complexity , RCm ( F ) , which is defined as follows : RCm ( F ) = 1 2 × E σ , S , S′ [ sup f∈F ( 1 m m∑ i=1 f ( zi ) f ( z ′ i ) σi ) ] ( 2 ) We have 0 ≤ RCm ( F ) ≤ 1/2 . We also show thatRCm ( F ) ≤ Rm ( F ) ( see Appendix C.1 ) . It has been argued thatRm ( F ) ( and thereforeRCm ( F ) ) can be considered as ” entropy ” measures of the entire function space F ( Anguita et al . ( 2014 ) ) . Note that , likeRm ( F ) , RCm ( F ) is also eventually depends on the noisy label fitting ability of F , but computes it via ability of the function space to assign the same or a different label to two random points zi and z′i ( via f ( zi ) f ( z ′ i ) ) . Thus we denote it as the correlated Rademacher complexity of F . Co-Complexity : Now we propose various complexity measures between two separate function spaces F and G. Similar toRm ( F ) andRCm ( F ) , these measures assess the noisy label fitting abilities of the union of the function spaces F and G. In doing so , these measures compute a joint-entropy like metric over the two function spaces . First , we define the Co-complexity between F and G as follows . Rm ( F , G ) = 1 2 × E σ , S , S′ [ sup f∈F , g∈G ( 1 m m∑ i=1 f ( zi ) f ( z ′ i ) g ( zi ) g ( z ′ i ) σi ) ] ( 3 ) Some of the properties of co-complexity are as follows : P1 Rm ( F , G ) = Rm ( G , F ) , i.e. , co-complexity is symmetric . P2 Rm ( F , G ) ≥ RCm ( F ) and Rm ( F , G ) ≥ RCm ( G ) , i.e. , the co-complexity between F and G is always greater than the individual correlated Rademacher complexities of F and G. P3 Rm ( F , G ) ≤ RCm ( F ) +RCm ( G ) ≤ Rm ( F ) +Rm ( G ) , i.e. , the co-complexity between F and G is upper bounded by the sum of the Rademacher complexities of F and G. P4 Rm ( F , G ) behaves like a joint-entropy measure ofF and G. To see this , let us define Im ( F , G ) = RCm ( F ) +RCm ( G ) − Rm ( F , G ) , called the mutual co-complexity between F and G. We later find that Im ( F , G ) behaves like a mutual-information measure . This , coupled with the fact that RCm ( F ) andRCm ( G ) can be construed as entropy measures of F and G , implies the result . Invariance Co-Complexity : Next , we quantify some of the properties of the ground truth generator space G in terms of its invariance transformations . We define the Invariance Classes of G as follows : IC ( G ) = { τ1 ( . , θ1 ) , τ2 ( . , θ2 ) , ... , τn ( . , θn ) } , ( 4 ) where { τi ( . , θi ) |i = 1 , 2 , · · · , n } are functions from Rd −→ Rd , and θi represents the extent of the transformation τi . Each θi ∈ R is a scalar and takes on a set of admissible values depending on the transformation ( possibly infinite ) . The functions are constrained such that , ∀τ ∈ IC ( G ) , ∀g ∈ G , g ( τ ( z ) , t ) = g ( z ) for all data points z ∈ Rd , and for all admissible values of the transformation parameter t. Also , note that setting θi = 0 leads to the identity function , i.e. , τi ( z , 0 ) = 1 for all z ∈ Rd and all i . Based on the above we define a transformation-indicator function I ( z , zi ) , such that I ( z , zi ) = 1 if z = τ1 ( . , t1 ) ◦ τ2 ( . , t2 ) ◦ .... ◦ τn ( . , tn ) ( zi ) , for a certain t1 , t2 , ... tn and otherwise I ( z , zi ) = 0 . Additionally , with respect to a generator space G , we also define the invariance extended set of a datapoint zi ∈ Rd as follows : IG ( zi ) = { z ∈ Rd | I ( z , zi ) = 1 } ( 5 ) Note that that invariance extended set of z will always have the same ground truth label as z . Next , we define the Invariance Co-complexity between two function spaces F and G as follows : RIm ( F , G ) = 1 2 × E S , S′ [ sup f∈F ( 1− ( 1 m m∑ i=1 f ( zi ) f ( z ′ i ) ) ) ] , where z′i ∼ P ( z ) I ( z , zi ) , ∀i . ( 6 ) Note that RIm ( F , G ) ≤ 1 . Each data point z′i in S′ is contained within the invariance extended set IG ( zi ) of zi , and is sampled according to the un-normalized distribution P ( z ) I ( z , zi ) over z ∈ Rd . The invariance co-complexity between F and G indicates the degree to which F obeys the invariance transformations within G. For instance , a low RIm ( F , G ) would indicate that f ( zi ) f ( z′i ) = 1 ( i.e. , f ( zi ) = f ( z ′ i ) ) for most zi and z ′ i which are related via some invariance transformation in G. Dissociation Co-Complexity : We now define the Dissociation Co-complexity between F and G when the corresponding datapoints in S and S′ are not related by any invariance transformation . That is , for all zi ∈ S and z′i ∈ S′ , z′i 6∈ IG ( zi ) , and is sampled from the distribution P ( z ) ( 1− I ( z , zi ) ) over z ∈ Rd . The dissociation co-complexity can then be defined as : RDm ( F , G ) = 1 2 × E σ , S , S′ [ sup f∈F ( 1 m m∑ i=1 f ( zi ) f ( z ′ i ) σi ) ] , where z′i ∼ P ( z ) ( 1− I ( z , zi ) ) , ∀i . ( 7 ) Note thatRDm ( F , G ) measures the average flexibility of F in its label assignment to any two points zi and z′i which are not related via any invariance transformation in G. Thus , a largerRDm ( F , G ) would indicate that the classifier function space F is able to easily assign separate labels to datapoints which are not related via any invariance transformation in G. We also define a variant of RDm ( F , G ) , in which S contains only one instance , instead of m instances , denoted asRD,1m ( F , G ) . RD,1m ( F , G ) is defined as follows . RD,1m ( F , G ) = 1 2 × E σ , z0∼P , S′ [ sup f∈F ( 1 m m∑ i=1 f ( z0 ) f ( z ′ i ) σi ) ] , where z′i ∼ P ( z ) ( 1− I ( z , z0 ) ) , ∀i . ( 8 ) Note thatRDm ( F , G ) ≤ 0.5 andRD,1m ( F , G ) ≤ 0.5 . Rademacher Smoothness : For our final result in Theorem 6 , we assume that the function space F is Rademacher smooth w.r.t . G. We define Rademacher Smoothness as follows . If F is Rademacher smooth w.r.t . G , then any quantity of the form ES , S′ [ supf∈F ( 1 m ∑m i=1 f ( zi ) f ( z ′ i ) g ( zi ) g ( z ′ i ) σi ) ] or ES , S′ [ supf∈F , g∈G ( 1 m ∑m i=1 f ( zi ) f ( z ′ i ) g ( zi ) g ( z ′ i ) σi ) ] , where the expectation is over datapoint permutations S , S′ , should lie between the two following cases : ( i ) S′ ∈ IG ( S ) ( i.e. , z′i ∼ P ( z ) ( I ( z , zi ) ) for all i ) and ( ii ) S′ /∈ IG ( S ) ( i.e. , z′i ∼ P ( z ) ( 1− I ( z , zi ) ) for all i ) . Then , for some non-zero value of α , 0 ≤ α ≤ 1 , the following holds : E S , S′ [ sup f∈F 1 m ( m∑ i=1 f ( zi ) f ( z ′ i ) g ( zi ) g ( z ′ i ) σi ) ] = ( α ) E S , S′ , S′∈IG ( S ) [ sup f∈F ( 1 m m∑ i=1 f ( zi ) f ( z ′ i ) g ( zi ) g ( z ′ i ) σi ) ] + ( 1− α ) E S , S′ , S′ /∈IG ( S ) [ sup f∈F ( 1 m m∑ i=1 f ( zi ) f ( z ′ i ) g ( zi ) g ( z ′ i ) σi ) ] . ( 9 ) Similarly , the same applies to ES , S′ [ supf∈F , g∈G ( 1 m ∑m i=1 f ( zi ) f ( z ′ i ) g ( zi ) g ( z ′ i ) σi ) ] .","It is well known that the complexity of a classifier’s function space controls its generalization gap, with two important examples being VC-dimension and Rademacher complexity (R-Complexity). We note that these traditional generalization error bounds consider the ground truth label generating function (LGF) to be fixed. However, if we consider a scenario where the LGF has no constraints at all, then the true generalization error can be large, irrespective of training performance, as the values of the LGF on unseen data points can be largely independent of the values on the training data. To account for this, in this work, we consider an extended characterization of the problem, where the ground truth labels are generated by a function within another function space, which we call the generator space. We find that the generalization gap in this scenario depends on the R-Complexity of both the classifier and the generator function spaces. Thus, we find that, even if the R-Complexity of the classifier is low and it has a good training fit, a highly complex generator space could worsen generalization performance, in accordance with the no free lunch theorem. Furthermore, the characterization of a generator space allows us to model constraints, such as invariances (translation and scale in vision) or local smoothness. Subsequently, we propose a joint entropy-like measure of complexity between function spaces (classifier and generator), called co-complexity, which leads to tighter bounds on the generalization error in this setting. Co-complexity captures the similarities between the classifier and generator spaces. It can be decomposed into an invariance co-complexity term, which measures the extent to which the classifier respects the invariant transformations in the generator, and a dissociation co-complexity term, which measures the ability of the classifier to differentiate separate categories in the generator. Our major finding is that reducing the invariance co-complexity of a classifier, while maintaining its dissociation co-complexity, improves the training error and reduces the generalization gap. Furthermore, our results, when specialized to the previous setting where the LGF is fixed, lead to potentially tighter generalization error bounds. Theoretical results are supported by empirical validation on the CNN architecture and its transformation-equivariant extensions. Co-complexity showcases a new side to the generalization abilities of classifiers and can potentially be used to improve their design.","In this paper, we propose a novel approach to studying the generalization gap in machine learning by considering the relationship between the classifier function space and the generator function space. We introduce co-complexity, a measure of the similarity between the classifier and generator function spaces, and derive new generalization error bounds that take into account the complexity of both spaces.","This paper studied a novel perspective on generalization error bounds, by introducing the ""label generating function ""(LGF). Several new complexity measures (correlated Rademacher complexity, co-complexity, invariance co-complexity, dissociation co-complexity, Rademacher smoothness) were proposed. The properties of the measures and generalization error bound with respect to these complexity measures are studied.  ","The article proposes a new framework for studying the generalisation ability of supervised learning models. The focus is on the generator function space, which is the space containing all possible true label generating functions, and its relationship with the classifier space.","2,0,1",,,,,,,,,,,,,,,,,
7,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"1 INTRODUCTION . Graph Convolutional Networks ( GCNs ) are the state of the art in community detection ( Kipf & Welling , 2016 ) . They correspond to Graph Neural Networks ( GNNs ) that propagate graph features through a cascade of linear operator and non-linearities , while exploiting the graph structure through a linear smoothing operator . However , the principles that allow GCNs to obtain good performances remain unclear . It is suggested in Li et al . ( 2018 ) that GCNs are eager to over-smooth their representation , which indicates they average too much neighborhood nodes and dilute classification information . The smoothing is generally interpreted as a low-pass filtering through the graph Laplacian , and finding a way to exploit high-frequencies of the graph Laplacian is an active research question ( Oono & Suzuki , 2019 ) . In contrast to this , our work actually suggests that , in the setting of community detection , graph Laplacian high-frequencies have actually a minor impact on the classification performances of a standard GCN , as opposed to standard Convolutional Neural Networks for vision , which are built thanks to image processing considerations . Graph Signal Processing ( GSP ) is a popular field whose objective is to manipulate signals spectrum whose topology is given by a graph . Typically , this graph has a non-Euclidean structure , however many central theoretical results ( Hammond et al. , 2011 ) are based on an analogy with Euclidean , regular grids . For instance , a spectral component or frequency has to be understood as an eigenvalue of the Laplacian , yet it thus suffers from intrinsic issues such as isotropy ( Oyallon , 2020 ) . The principles of GSP are very appealing because they allow to use the dense literature of harmonic analysis , on graphs . Thus , this literature is at the core of many intuitions and drives many key ingredients of a GCN design , which evokes standard tools of signal processing : convolutions , shift invariance , wavelets , Fourier ( Bronstein et al. , 2017 ) , etc . Here , we certainly observe several limits of this analogy in the context of community detection : for instance , we observe that discarding high-frequencies has a minor impact on a GCN behavior , because the spectrum of the graphs of the datasets that are used is essentially located in the low-frequency domain . This type of ideas is for instance core in spectral clustering algorithms . Spectral clustering is a rather different point of view from deep supervised GCNs which studies node labeling in unsupervised contexts : it generally relies on generative models based on the graph spectrum . The main principle is to consider the eigenvectors corresponding to the smallest nonzero eigenvalues , referred to as Fiedler vectors ( Doshi & Eun , 2020 ) : those directions allow to define clusters , depending on the sign of a feature . Several theoretical guarantees can be obtained in the context of Stochastic Block Model approximation ( Rohe et al. , 2011 ) . Our paper proposes to establish a clear link with this approach : we show that the informative graph features are located in a low-frequency band of the graph Laplacian and do not need extra graph processing tools to be efficiently used in a deep supervised classifier . This paper shows via various ablation experiments that experiments on standard community detection datasets like Cora , Citeseer , Pubmed can be conducted using only few frequencies of their respective graph spectrum without observing any significant performances drop . Other contributions are as follows : ( a ) First we show that most of the necessary information exploited by a GCN for a community detection task can actually be isolated in the very first eigenvectors of a Laplacian . ( b ) We numerically show that the high-frequency eigenvalues are less informative for the supervised community detection task and that a trained GCN is more stable to them . ( c ) We observe that a simple MLP method fed with handcrafted features allows to successfully deal with transdusctive datasets like Cora , Citeseer or Pubmed : to our knowledge , this is the first competitive results obtained with a MLP on those datasets . We now discuss the organization of the paper : first , we discuss the related work in Sec . 2 . We explain our notations as well as our work hypotheses in Sec . 3 . Then , we study low-rank approximations of the graph Laplacian in Sec . 4.1 . Finally , the end of Sec . 4 proposes several experiments to study the impact of high-frequencies on GCNs . A basic code is provided in the supplementary materials , and our code will be released on an online public repository at the time of publication . 2 RELATED WORK . GCNs and Spectral GCNs Introduced in Kipf & Welling ( 2016 ) , GCNs allow to deal with large graph structure in semi-supervised classification contexts . This type of model works at the node level , meaning that it uses locally the adjacency matrix . This approach has inspired a wide range of models , such as linear GCN ( Wu et al. , 2019 ) , Graph Attention Networks ( Veličković et al. , 2017 ) , GraphSAGE ( Hamilton et al. , 2017 ) , etc . In general , this line of work does not consider directly the graph Laplacian . Another line of work corresponds to spectral methods , that employ filters which are designed from the spectrum of a graph Laplacian ( Bruna et al. , 2013 ) . In general , those works make use of polynomials in the Laplacian ( Defferrard et al. , 2016 ) , which are very similar to an anisotropic diffusion ( Klicpera et al. , 2019 ) . All those references share the idea to manipulate bandpass filters that discriminate the different ranges of frequencies . Over-smoothing in GCNs In the context of GCN , Li et al . ( 2018 ) is one of the first papers to notice that cascading low-pass filters can lead to a substantial information loss . The result of our work indicates that the important spectral components for detecting communities are already in the low-frequency domain and that this is not due to an architecture bias . Zhao & Akoglu ( 2019 ) ; Yang et al . ( 2020 ) proposes to introduce regularizations which address the loss of information issues . Cai & Wang ( 2020 ) ; Oono & Suzuki ( 2019 ) study the spectrum of a graph Laplacian under various transform , yet they consider the spectrum globally and in asymptotic settings , with a deep cascade of layers . Huang et al . ( 2020 ) ; Rong et al . ( 2019b ) introduce data augmentations , whose aim is to alleviate over-smoothing in deep networks : we study GCNs without this ad-hoc procedure . Spectral clustering and low rank approximation As the literature about spectral clustering is large , we mainly focus on the subset that connects directly with GCN . Mehta et al . ( 2019 ) proposes to learn an unsupervised auto-encoder in the framework of a Stochastic Block Model . Oono & Suzuki ( 2019 ) introduces the Erdös – Renyi model in the GCN analysis , but only in an asymptotic setting . Loukas & Vandergheynst ( 2018 ) studies the graph topology preservation under the coarsening of the graph , which could be a potential direction for future works . Node embedding A MLP approach can be understood as an embedding at the node level . For instance , Aubry et al . ( 2011 ) applies a spectral embedding combined with a diffusion process for shape analysis , which allows point-wise comparisons . We should also point Deutsch & Soatto ( 2020 ) that uses a node embedding , based on the spectrum of a quite modified graph Laplacian , obtained from on a measure of node centrality . Graph Scattering Networks ( GSN ) This class model explicitly employs band-pass based on the spectrum of a graph Laplacian and it is thus necessary to review it . Gao et al . ( 2019 ) ; Gama et al . ( 2018 ; 2019 ) are a class of neural networks built upon an analogy with a Scattering Transform ( Mallat , 2012 ) . They typically rely on a cascade of wavelets followed by an absolute value : the objective of each wavelet is to separate multi-scale information into dyadic bandpass filters . This method relies heavily on each eigenvector of the Laplacian and the dyadic space is typically constructed from a diffusion process at dyadic intervals . Interferometric Graph Transform on the other hand relies on the concept of demodulation , which is clear in the context of Lie groups but unclear for community detection tasks ( Oyallon , 2020 ) . GCN stability Stability of GCNs has been theoretically studied in Gama et al . ( 2019 ) , which shows that defining a generic notion of deformations is difficult . Surprisingly , it was noted in Oyallon ( 2020 ) that stability is not a key component to good performances . The stability of GCN has also been investigated in Verma & Zhang ( 2019 ) but only considers neural networks with a single layer and relies on the whole spectrum of the learned layer . Keriven et al . ( 2020 ) considers the stability of GCNs , and relies on an implicit Euclidean structure : it is unclear if this holds in our settings . Sun et al . ( 2020 ) is one of the first works to study adversarial examples linked to the node connectivity and introduces a loss to reduce their effects . Zhu et al . ( 2019 ) also addresses the stability issues by embedding GCNs in a continuous representation . None of these work directly related a trained GCN to spectral perturbations . 3 FRAMEWORK . 3.1 METHOD . We first describe our baseline model . Our initial graph data are node features X obtained from a graph with N nodes and an adjacency matrix A with diagonal degree matrix D. We consider GCNs f ( X , A ) as introduced in ( Kipf & Welling , 2016 ) , which correspond to GNNs that propagate features graph input H ( 0 ) , X through a cascade of layers , via the iteration : H ( l+1 ) , σ ( ÃH ( l ) W ( l ) ) , ( 1 ) where Ã = 12 ( I + D −1/2AD−1/2 ) , σ a point-wise non-linearity and W ( l ) a parametrized affine operator . Note that if Ã = IN , then Eq . 1 is simply an MLP , which makes its implementation simple . The 12 factor is a normalization factor to obtain ‖Ã‖ = 1 . In the semi-supervised setting , a final layer f ( X , Ã ) , H ( L ) is fed to a supervised loss ` ( here a softmax ) and { W ( 0 ) , ... , WL−1 } are trained in an end-to-end manner to adjust the label of each node . We note that for undirected graph , Ã is a positive definite matrix with positive weights , which is understood as an averaging operator Li et al . ( 2018 ) , as , ignoring D̃ , we see that for some node features X , we have : [ ( IN +A ) X ] i = Xi + ∑ j→i Ai , jXj . ( 2 ) We remark that multiple choices of averaging operators are possible : as briefly discussed in Appendix A.3 other formulations did not change our numerical conclusions , thus we decided to keep the simplest to be handled mathematically . We are interested in analyzing the properties of spectral approximations of Ã . We consider the decreasing set of eigenvalues Λ = { λk } k≥0 of Ã , such that λk ≥ λk+1 , and we denote by uk the k-th eigenvector corresponding to λk ∈ Λ . We remind that Λ ⊂ [ 0 , 1 ] and that λ0 = 1 can be interpreted as the lowest frequency of the graph Laplacian . Since the adjacency matrix is normalized , one basis of λ0 ’ s eigenspace is constituted by the constant vectors of 1 supported on each connected component . We then write : Ã [ k1 , k2 ] , ∑ k1≤k≤k2 λkuku T k , ( 3 ) such that Ã = Ã [ 0 , N ] . We are interested to study the degradation accuracy if we replace Ã with Ã [ 0 , k ] or Ã [ k , N ] for some 0 < k < N . The next section explains that under standard but oversimplifying assumptions , an approximation of the type Ã [ 0 , k ] is relevant for community detection tasks .","In this work, we study the behavior of standard models for community detection under spectral manipulations. Through various ablation experiments, we evaluate the impact of bandpass filtering on the numerical performances of a GCN: we empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph (e.g., images), high-frequencies are less crucial to community detection. In particular, it is possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies: this is surprising because contrary to GCNs, no cascade of filtering along the graph structure is involved and it indicates that the important spectral components for the supervised community detection task are essentially in the low-frequency domain.","The work presents an interesting analysis of GCN models under spectral manipulations and relates the performance of GCNs through bandpass filtering. The authors demonstrate that GCNs mainly rely more on low-frequencies rather than high-frequencies which is contrary to what is observed in signal processing. For this, the authors use band-pass filters which allow only a portion of the spectrum to be utilized by the GCN model. The major findings are as follows:","The article discusses the use of Graph Convolutional Networks (GCNs) in community detection and the limitations of using the graph Laplacian for such tasks. While GCNs have shown to be effective in processing graph data, their principles for obtaining good performances are still unclear. The article proposes that informative graph features can be isolated in the low-frequency domain of the Laplacian and that the use of high-frequency eigenvalues have a minor impact on the performance of GCNs.","The article discusses the impact of high-frequency components on the performance of graph convolutional neural networks (GCNs) in community detection tasks. The authors show that most of the informative features for community detection can be isolated in the first eigenvectors of the graph Laplacian, and that high-frequency eigenvalues are less informative. They also propose a simple method based on handcrafted features to deal with transductive datasets like Cora, Citeseer, or Pubmed, and demonstrate competitive results without relying on high-frequency components.","0,1,2",,,,,,,,,,,,,,,,,
8,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"1 INTRODUCTION . This paper seeks to tackle the question of how to build machines that leverage prior experience to solve more complex problems than they have previously encountered . How does a learner represent prior experience ? How does a learner apply what it has learned to solve new problems ? Motivated by these questions , this paper aims to formalize the idea of , as well as to develop an understanding of the machinery for , compositional generalization in problems that exhibit compositional structure . The solutions for such problems can be found by composing in sequence a small set of reusable partial solutions , each of which tackles a subproblem of a larger problem . The central contributions of this paper are to frame the shared structure across multiple tasks in terms of a compositional problem graph , propose compositional generalization as an evaluation scheme to test the degree a learner can apply previously learned knowledge to solve new problems , and introduce the compositional recursive learner , a domain-general framework1 for sequentially composing representation transformations that each solve a subproblem of a larger problem . The key to our approach is recasting the problem of generalization as a problem of learning algorithmic procedures over representation transformations . A solution to a ( sub ) problem is a transformation between its input and output representations , and a solution to a larger problem composes 1https : //github.com/mbchang/crl these subsolutions together . Therefore , representing and leveraging prior problem-solving experience amounts to learning a set of reusable primitive transformations and their means of composition that reflect the structural properties of the problem distribution . This paper introduces the compositional recursive learner ( CRL ) , a framework for learning both these transformations and their composition together with sparse supervision , taking a step beyond other approaches that have assumed either pre-specified transformation or composition rules ( Sec . 5 ) . CRL learns a modular recursive program that iteratively re-represents the input representation into more familiar representations it knows how to compute with . In this framework , a transformation between representations is encapsulated into a computational module , and the overall program is the sequential combination of the inputs and outputs of these modules , whose application are decided by a controller . What sort of training scheme would encourage the spontaneous specialization of the modules around the compositional structure of the problem distribution ? First , exposing the learner to a diverse distribution of compositional problems helps it pattern-match across problems to distill out common functionality that it can capture in its modules for future use . Second , enforcing that each module have only a local view of the global problem encourages task-agnostic functionality that prevents the learner from overfitting to the empirical training distribution ; two ways to do this are to constrain the model class of the modules and to hide the task specification from the modules . Third , training the learner with a curriculum encourages the learner to build off old solutions to solve new problems by re-representing the new problem into one it knows how to solve , rather than learning from scratch . How should the learner learn to use these modules to exploit the compositional structure of the problem distribution ? We can frame the decision of which computation to execute as a reinforcement learning problem in the following manner . The application of a sequence of modules can be likened to the execution trace of the program that CRL automatically constructs , where a computation is the application of a module to the output of a previous computation . The automatic construction of the program can be formulated as the solution to a sequential decision-making problem in a meta-level Markov decision process ( MDP ) ( Hay et al. , 2014 ) , where the state space is the learner ’ s internal states of computation and the action space is the set of modules . Framing the construction of a program as a reinforcement learning problem allows us to use techniques in deep reinforcement learning to implement loops and recursion , as well as decide on which part of the current state of computation to apply a module , to re-use sub-solutions to solve a larger problem . Our experiments on solving multilingual arithmetic problems and recognizing spatially transformed MNIST digits ( LeCun et al. , 1998 ) show that the above proposed training scheme prescribes a type of reformulation : re-representing a new problem in terms of other problems by implicitly making an analogy between their solutions . We also show that our meta-reasoning approach for deciding what modules to execute achieves better generalization to more complex problems than monolithic learners that are not explicitly compositional . 2 COMPOSITIONAL GENERALIZATION . Solving a problem simply means representing it so as to make the solution transparent . ( SIMON , 1988 ) Humans navigate foreign cities and understand novel conversations despite only observing a tiny fraction of the true distribution of the world . Perhaps they can extrapolate in this way because the world contains compositional structure , such that solving a novel problem is possible by composing previously learned partial solutions in a novel way to fit the context . With this perspective , we propose the concept of compositional generalization . The key assumption of compositional generalization is that harder problems are composed of easier problems . The problems from the training and test sets share the same primitive subproblems , but differ in the manner and complexity with which these subproblems are combined . Therefore , problems in the test set can be solved by combining solutions learned from the training set in novel ways . Definition . Let a problem P be a pair ( Xin , Xout ) , where Xin and Xout are random variables that respectively correspond to the input and output representations of the problem . Let the distribution of Xin be rin and the distribution of Xout be rout . To solve a particular problem P = p is to transform Xin = xin into Xout = xout . A composite problem pa = pb ◦ pc is that for which it is possible to solve by first solving pc and then solving pb with the output of pc as input . pb and pc are subproblems with respect to pa . The space of compositional problems form a compositional problem graph , whose nodes are the representation distributions r. A problem is described as pair of nodes between which the learner must learn to construct an edge or a path to transform between the two representations . Characteristics . First , there are many ways in which a problem can be solved . For example , translating an English expression to a Spanish one can be solved directly by learning such a transformation , or a learner could make an analogy with other problems by first translating English to French , and then French to Spanish as intermediate subproblems . Second , sometimes a useful ( although not only ) way to solve a problem is indicated by the recursive structure of the problem itself : solving the arithmetic expression 3 + 4× 7 modulo 10 can be decomposed by first solving the subproblem 4×7 = 8 and then 3+8 = 1 . Third , because a problem is just an ( input , output ) pair , standard problems in machine learning fit into this broadly applicable framework . For example , for a supervised classification problem , the input representation can be an image and the output representation a label , and intermediate subproblems can be transforming some intermediate representations to other intermediate representations . Sec . 4 demonstrates CRL on all three of the above examples . Broad Applicability . Problems in supervised , unsupervised , and reinforcement learning can all be viewed under the framework of transformations between representations . What we gain from the compositional problem graph perspective is a methodological way to relate together different problems of various forms and complexity , which is especially useful in a lifelong learning setting : the knowledge required to solve one problem is composed of the knowledge required to solve subproblems seen in the past in the context of different problems . For example , we can view latent variable reinforcement learning architectures such as ( Ha & Schmidhuber , 2018 ; Nair et al. , 2018 ) as simultaneously solving an image reconstruction problem and an action prediction problem , both of which share the same subproblem of transforming a visual observation into a latent representation . Lifelong learning , then , can be formulated as not only modifying the connections between nodes in the compositional problem graph but also continuing to make more connections between nodes , gradually expanding the frontier of nodes explored . Sec . 4 describes how CRL takes advantage of this compositional formulation in a multi-task zero-shot generalization setup to solve new problems by re-using computations learned from solving past problems . Evaluation . To evaluate a learner ’ s capacity for compositional generalization , we introduce two challenges . The first is to generalize to problems with different subproblem combinations from what the learner has seen . The second is to generalize to problems with longer subproblems combinations than the learner has seen . Evaluating a learner ’ s capability for compositional generalization is one way to measure how readily old knowledge can be reused and hence built upon . 3 A LEARNER THAT PROGRAMS ITSELF . This paper departs from the popular representation-centric view of knowledge ( Bengio et al. , 2013 ) and instead adopts a computation-centric view of knowledge : our goal is to encapsulate useful functionality shared across tasks into specialized computational modules – atomic function operators that perform transformations between representations . This section introduces the compositional recursive learner ( CRL ) , a framework for training modules to capture primitive subproblems and for composing together these modules as subproblem solutions to form a path between nodes of the compositional problem graph . 3.1 COMPOSITIONAL RECURSIVE LEARNER . The CRL framework consists of a controller π , a set of modules m ∈M , and an evaluator E. Training CRL on a diverse compositional problem distribution produces a modular recursive program that is trained to transform the input Xin into its output Xout , the corresponding samples of which are drawn from pairs of nodes in the compositional problem graph . In this program , the controller looks at the current state xi of the program and chooses a module m to apply to the state . The evaluator executes the module on that state to produce the next state xi+1 of the program . Xin is the initial state of the program , X̂out is the last , and the intermediate states Xi of the execution trace correspond to the other representations produced and consumed by the modules . The controller can choose to re-use modules across different program executions to solve different problems , making it straightforward to re-use computation learned from solving other problems to solve the current one . The controller can also choose to reuse modules several times within the same program execution , which produces recursive behavior . 3.2 DECIDING WHICH COMPUTATIONS TO EXECUTE . The sequential decision problem that the controller solves can be formalized as a meta-level Markov decision process ( meta-MDP ) ( Hay et al. , 2014 ) , whose state space corresponds to the intermediate states of computation X , whose action space corresponds to the modules M , and whose transition model corresponds to the evaluatorE . The symbiotic relationship among these components is shown in Fig . 2 . In the bounded-horizon version of CRL ( Sec . 4.2 ) , the meta-MDP has a finite horizon whose length is determined by the complexity of the current problem . In the infinite-horizon version of CRL ( Sec . 4.1 ) , the program itself determines when to halt when the controller selects the HALT signal . When the program halts , in both versions the current state of computation is produced as output x̂out , and CRL receives a terminal reward that reflects how x̂out matches the desired output xout . The infinite-horizon CRL also incurs a cost for every computation it executes to encourage it to customize its complexity to the problem . Note the following key characteristics of CRL . First , unlike standard reinforcement learning setups , the state space and action space can vary in dimensionality across and within episodes because CRL trains on problems of different complexity , reducing more complex problems to simpler ones ( Sec . 4.1 ) . Second , because the meta-MDP is internal to CRL , the controller shapes the meta-MDP by choosing which modules get trained and the meta-MDP in turn shapes the controller through its non-stationary state-distribution , action-distribution , and transition function . Thus CRL simultaneously designs and solves reinforcement learning problems “ in its own mind , ” whose dynamics depend just as much on the intrinsic complexity of the problem as well as the current problem-solving capabilities of CRL .","A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning – either training a separate learner per task or training a single learner for all tasks – both have difficulty with such generalization because they do not leverage the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domaingeneral framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.","This paper is about trying to learn a function from typed input-output data so that it can generalize to test data with an input-output type that it hasn't seen during training. It should be able to use ""analogy"" (if we want to translate from French to Spanish but don't know how to do so directly, we should translate from French to English and English to Spanish). It should also be able to generalize better by learning useful ""subfunctions"" that can be composed together by an RL agent. We set up the solution as having a finite number of subfunctions, including ""HALT"" which signifies the end of computation. At each timestep an RL agent chooses a subfunction to apply to the current representation until ""HALT"" is chosen. The main idea is we parameterize these subfunctions and the RL agent as neural networks which are learned based on input -output data. RL agent is also penalized for using many subfunctions. The algorithm is called compositional recursive learner (CRL). Both analogy and meaningful subfunctions should arise purely because of this design.","This paper proposes a new approach to machine learning called the compositional recursive learner (CRL), which aims to build machines that can leverage prior experience to solve more complex problems than they have previously encountered. The key idea is to represent prior experience in terms of a compositional problem graph, which is a graph of nodes representing different representations of a problem, and edges representing the compositional structure of the problem. The CRL framework consists of a controller, a set of modules, and an evaluator, which work together to transform the input representation into the desired output representation. The controller selects a module to apply to the current state of the program, and the evaluator executes the module to produce the next state. The CRL can be used to solve a wide range of problems, including multilingual arithmetic problems and recognizing spatially transformed MNIST digits. The paper demonstrates the effectiveness of CRL on several examples and shows that it can generalize to new problems by reusing computations learned from solving past problems. The CRL framework is distinct from traditional representation-centric approaches to machine learning, which focus on learning a single representation for a given task, and instead adopts a computation-centric view of knowledge, where the goal is to encapsulate useful functionality shared across tasks into specialized computational modules.","This article proposes a framework for building machines that can use prior experience to solve complex problems. The framework is based on the idea of compositional generalization, where problems are solved by composing a small set of reusable partial solutions that tackle subproblems. The key concept of the compositional generalization is the compositional problem graph, which represents the shared structure across multiple tasks. The article introduces the compositional recursive learner (CRL), a framework that learns a modular recursive program that re-represents the input representation into more familiar representations the machine knows how to compute with. The decision of which computation to execute is framed as a reinforcement learning problem, using techniques in deep reinforcement learning to implement loops and recursion. To evaluate a learner's capacity for compositional generalization, two challenges are introduced: generalizing to problems with different subproblem combinations and generalizing to problems with longer subproblems combinations. The framework is shown to achieve better generalization to more complex problems than monolithic learners that are not explicitly compositional.","0,2,1",,,,,,,,,,,,,,,,,
9,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"1 Introduction . Video analysis has become an increasingly important area of research , encompassing multiple relevant problems such as action recognition [ 8 , 13 ] , temporal action localization [ 68 , 7 , 12 , 22 , 59 , 60 , 55 , 67 , 15 ] , video grounding [ 36 , 65 , 64 , 66 , 49 , 16 , 24 ] , and video question answering [ 25 , 31 ] . Among those , temporal action localization ( TAL ) [ 68 , 22 ] is a fundamental task , as natural videos are not temporally trimmed . Given an untrimmed video , TAL aims to identify the start and end points of all action instances and recognize their category labels simultaneously . A typical TAL model is based on deep convolutional neural networks ( CNNs ) composed of two modules : a video encoder and a TAL head . The video encoder is often shared across different TAL methods ( e.g. , G-TAD [ 60 ] , BC-GNN [ 3 ] ) ∗Work done during an internship at Samsung AI Centre . 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . by taking a specific off-the-shelf action classification model ( e.g. , C3D [ 51 ] , I3D [ 8 ] , TSM [ 33 ] ) , with the differences residing only in the TAL head . However , instead of short ( e.g. , 10 seconds ) trimmed video clips as in action recognition , the input videos to a TAL model are characterized by much longer temporal duration ( e.g. , 120 seconds ) . This causes unique computational challenges that remain unsolved , particularly in model optimization . In standard optimization of a TAL model , a two-stage transfer learning pipeline is often involved : 1 . First , the video encoder is optimized on a large source video classification dataset ( e.g. , Kinetics [ 28 ] ) and , optionally , finetunned on the trimmed version of the target dataset under action classification supervision ; 2 . Second , the video encoder is frozen and the TAL head is optimized on the target action localization dataset ( e.g. , ActivityNet [ 22 ] , HACS [ 68 ] ) under TAL task supervision . With this widely-used TAL training pipeline , the video encoder is only optimal for action classification but not for the target TAL task . Specifically , the video encoder is trained so that different short segments within an action sequence are mapped to similar outputs , thus encouraging insensitivity to the temporal boundaries of actions . This is not desirable for a TAL model . We identify this as a task discrepancy problem . Consequently , the final TAL model could suffer from suboptimal performance . Indeed , jointly optimizing all components of a CNN architecture end-to-end with the target task ’ s supervision is a common practice , e.g. , training models for object detection in static images [ 18 , 46 , 38 ] . Unfortunately , this turns out to be non-trivial for TAL . As mentioned above , model training is severely restricted by the large input size of untrimmed videos and subject to the memory constraint of GPUs . This is why the two-stage optimization pipeline as described above becomes the most common and feasible choice in practice for optimizing a TAL model . On the other hand , existing transfer learning methods mostly focus on tackling the data distribution shift problem across different datasets [ 70 , 50 ] , rather than the task shift problem we study here . Regardless , we believe that solving this limitation of the TAL training design bears a great potential for improving model performance . In this work , we present a simple yet effective low-fidelity ( LoFi ) video encoder optimization method particularly designed for better TAL model training . It is designed to adapt the video encoder from action classification to TAL whilst subject to the same hardware budget . This is achieved by introducing a simple strategy characterized by a new intermediate training stage where both the video encoder and the TAL head are optimized end-to-end using a lower temporal and/or spatial resolution ( i.e. , low-fidelity ) in the mini-batch construction . Compared to the standard training method , our proposed strategy does not increase the GPU memory standard ( often a hard constraint for many practitioners ) . Crucially , with our LoFi training the gradients back-propagate to the video encoder from a temporal action localization loss whilst conditioned on the target TAL head , enabling the learning of a video encoder sensitive to the temporal localization objective . We make the following contributions in this work . ( 1 ) We investigate the limitations of the standard optimization method for TAL models , and consider that the task discrepancy problem hinders the performance of existing TAL models . Despite it being a significant ingredient , video encoder optimization is largely ignored by existing TAL methods , left without systematic investigation . ( 2 ) To improve the training of TAL models , we present a novel , simple , and effective low-fidelity ( LoFi ) video encoder optimization method . It is designed specifically to address the task discrepancy problem with the TAL model ’ s video encoder . ( 3 ) Extensive experiments show that the proposed LoFi optimization method yields new state-of-the-art performance when combined with off-the-shelf TAL models ( e.g. , G-TAD [ 60 ] ) . Critically , our method achieves superior efficiency/accuracy trade-off with clear inference cost advantage and good generalizability to varying-capacity video encoders . 2 Related Work . Temporal action localization ( TAL ) models : TAL models can be grouped by architectural design pattern into two categories , one-stage and two-stage architectures . One-stage methods , either predict temporal action boundaries or generate proposals , and classify them within the same network [ 3 , 9 , 21 , 40 , 34 , 58 , 60 , 62 ] . The latter type , two-stage models , generate sets of action proposals ( e.g. , segments ) [ 6 , 12 , 14 , 23 , 39 ] and then an auxiliary head is used for classification of each proposal into an action class [ 35 , 47 , 48 , 63 , 69 ] . In this work , rather than introducing a novel model design , we focus on the training of generic TAL models , with a particular aim to improve the video encoder optimization . This is a relatively less investigated aspect in the TAL literature . Video encoders in TAL : The video encoder is an indispensable part of a TAL model . Main design choices include the base architecture of video encoder and its optimization procedure . With regards to the architecture , the two-stream Temporal Segment Network ( TSN ) [ 54 ] is one of the most common video encoders in existing TAL methods [ 3 , 34 , 35 , 60 ] . Concretely , these works use two TSN networks , one with a ResNet50 [ 20 ] backbone trained on RGB video frames , and the other with a BN-Inception backbone [ 26 ] trained on optical flow . Other alternatives used as a video encoder for TAL include two-stream I3D model [ 8 ] ( see [ 19 , 63 ] ) and Pseudo-3D [ 45 ] ( see [ 40 ] ) . In terms of optimization , a typical paradigm is two-staged : first pre-training the video encoder and then , in a second stage , training the TAL head of the model with the video encoder fixed . This is constrained by the inherent hardware budget derived from having a large per-video input size . In particular , the video encoder is pre-trained using a cross-entropy loss for action recognition on a large-scale video classification dataset such as Kinetics [ 28 , 64 ] . An optional step is to further pre-train it on the foreground segments of the target TAL dataset [ 34 , 60 , 11 , 44 ] . This brings a mismatch between training and inference for the video encoder , which we call a task discrepancy problem . More specifically , although trained to distinguish the content of different action classes , the video encoder is less sensitive to action temporal boundaries and thus less effective for the TAL task . In fact , due to their inherent design , CNNs have limited localization capabilities [ 41 ] , unless they are augmented with specialized localization-specific layers [ 37 ] . Additionally , the action classification task focuses only on the foreground content whilst ignoring per-class background segments , including the transition between foreground and background . In this paper we propose a novel low-fidelity video encoder pre-training method to solve this limitation with existing TAL methods . While current TAL literature mostly relies on pre-training through supervised learning , the rapid advancement of self-supervised learning makes it a promising alternative [ 2 , 4 , 42 , 43 , 56 ] . Some works have focused on finding effective temporal-related pretext tasks , from frame ordering learnt through triplets of frames [ 43 ] , to sorting the frames of a sequence [ 30 ] , distinguishing whether sequences are played forward or backwards [ 56 ] or through playback speed-related pretext tasks [ 4 , 61 , 53 , 27 ] . These methods exploit video-specific characteristics to force the network to focus on some sort of semantic content within video , inducing representations capturing long-term temporal semantic relations , but force invariance to or ignore the relative positioning of the snippets within the action instances . They are thus not suited for pre-training the video encoder of a TAL model . Very recent works [ 1 , 59 ] have exploited some of the aforementioned techniques for better pre-training of action localization models . For example , localization-tailored data augmentation and classification is adopted by [ 59 ] . However , these works introduce a large amount of extra video data and additional stream networks , both of which are expensive in terms of memory and computation . In contrast , our method aims to improve TAL modelling directly without the need for learning from extra training video data and using an expensive second network , nor relying on optical flow obtained at high computational cost . 3 Method . A TAL model takes as input a long untrimmed video with a varying number of frames . For design convenience , it is typical to represent a varying-length video by decomposing it into a fixed-length sequence of L snippets . The definition of a snippet is the same as in action recognition , where first a number of consecutive frames ( e.g. , 64 ) is selected and then sub-sampled with stride r ( e.g. , stride 8 to obtain 8-frame snippets ) . To represent a snippet , one first applies a video encoder to extract frame-level feature vectors and then averages them to obtain the snippet-level feature representation [ 6 , 12 , 17 , 35 ] . The resulting snippet feature sequence is denoted as X ∈ RC×L , where C is the feature dimension of each snippet , and L is the number of snippets . In the training set , each video is associated with its ground truth , consisting of a set of action instance annotations Ψ . In particular , each action instance is represented as a segment , each including the start time , the end time , and the action class label . The objective is to train a TAL model that can accurately localize all the target action instances in a given untrimmed video . To that end , the model predicts a varying number of action instances Φ , each comprised of the predicted temporal boundaries , action class , and confidence score .","Most existing temporal action localization (TAL) methods rely on a transfer learning pipeline, first optimizing a video encoder on a large action classification dataset (i.e., source domain), followed by freezing the encoder and training a TAL head on the action localization dataset (i.e., target domain). This results in a task discrepancy problem for the video encoder – trained for action classification, but used for TAL. Intuitively, joint optimization with both the video encoder and TAL head is an obvious solution to this discrepancy. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity (LoFi) video encoder optimization method. Instead of always using the full training configurations in TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoder and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Crucially, this enables the gradients to flow backwards through the video encoder conditioned on a TAL supervision loss, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream (RGB + optical flow) ResNet50 based alternatives, often by a good margin. Our code is publicly available at https://github.com/saic-fi/lofi_action_localization .","This article discusses the importance of video analysis in various fields, including temporal action localization (TAL), and the challenges posed by the large input size of untrimmed videos. The article proposes a novel low-fidelity (LoFi) video encoder optimization method to address the task discrepancy problem with TAL model's video encoder, as previous TAL methods mostly focused on pre-training the TAL head and ignored video encoder optimization. The proposed LoFi optimization method optimizes both the video encoder and the TAL head end-to-end using a lower temporal and/or spatial resolution in the mini-batch construction, introducing a strategy that does not increase the GPU memory standard while enabling the learning of a video encoder sensitive to the temporal localization objective. Extensive experiments show that the proposed LoFi optimization method yields new state-of-the-art performance when combined with off-the-shelf TAL models.","Temporal action localization (TAL) is a fundamental problem in computer vision, involving the identification of action instances and their temporal boundaries in untrimmed videos. However, existing TAL methods suffer from a task discrepancy problem, where the video encoder is optimized for action classification but not for the TAL task. To address this limitation, we propose a novel low-fidelity video encoder optimization method that adapts the video encoder for TAL while subject to the same hardware budget. Our method does not increase the GPU memory standard and enables the learning of a video encoder sensitive to the temporal localization objective, leading to improved TAL performance.","The paper proposes a simple yet effective method of pretraining the feature encoder in TAL models. The motivation comes from the distribution-shift as well as the task-shift between the classical image-/action-classification pretraining and the TAL training. To make the encoder also adapt to the TAL task, the authors propose to unfreeze the encoder and pretrain under the TAL task. Due to GPU memory limit, the authors use spatially/temporally low fidelity data to pretrain the encoder. Experiments are conducted to verify the authors’ motivation and show the advantage of such a pretraining routine.","1,2,0",,,,,,,,,,,,,,,,,
10,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"1 Introduction . Catastrophic forgetting is a common feature of machine learning algorithms where training on a new task often leads to poor performance on previously learned tasks . This is in contrast to biological agents which are capable of learning many different behaviors over the course of their lives with little to no interference across tasks . The study of continual learning in biological networks may therefore help inspire novel approaches in machine learning , while the development and study of continual learning algorithms in artificial agents can help us better understand how this challenge is overcome in the biological domain . This is particularly true for more challenging continual learning settings where task identity is not provided at test time , and for continual learning in recurrent neural 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . networks ( RNNs ) , which is important due to the practical and biological relevance of RNNs . However , continual learning in these settings has recently proven challenging for many existing algorithms , particularly those that rely on parameter regularization to mitigate forgetting [ 12 , 13 , 47 ] . In this work , we address these shortcomings by developing a continual learning algorithm that not only encourages good performance across tasks at convergence but also regularizes the optimization path itself using trust region optimization . This leads to improved performance compared to existing methods . Previous work has addressed the challenge of continual learning in artificial agents using weight regularization , where parameters important for previous tasks are regularized to stay close to their previous values [ 1 , 17 , 24 , 32 , 37 , 54 ] . This approach can be motivated by findings in the neuroscience literature of increased stability for a subset of synapses after learning [ 49 , 50 ] . More recently , approaches based on projecting gradients into subspaces orthogonal to those that are important for previous tasks have been developed in both feedforward [ 40 , 53 ] and recurrent [ 12 ] neural networks . This is consistent with experimental findings that neural dynamics often occupy orthogonal subspaces across contexts in biological circuits [ 3 , 14 , 20 , 22 ] . While these methods have been found to perform well in many continual learning settings , they also suffer from several shortcomings . In particular , while Bayesian weight regularization provides a natural way to weigh previous and current task information , this approach can fail in practice due to its approximate nature and often requires additional tuning of the importance of the prior beyond what would be expected in a rigorous Bayesian treatment [ 46 ] . In contrast , while projection-based methods have been found empirically to mitigate catastrophic forgetting , it is unclear how the ‘ important subspaces ’ should be selected and how such methods behave when task demands begin to saturate the network capacity . In this work , we develop natural continual learning ( NCL ) , a new method that combines ( i ) Bayesian continual learning using weight regularization with ( ii ) an optimization procedure that relies on a trust region constructed from an approximate posterior distribution over the parameters given previous tasks . This encourages parameter updates predominantly in the null-space of previously acquired tasks while maintaining convergence to a maximum of the Bayesian approximate posterior . We show that NCL outperforms previous continual learning algorithms in both feedforward and recurrent networks . We also show that the projection-based methods introduced by Duncker et al . [ 12 ] and Zeng et al . [ 53 ] can be viewed as approximations to such trust region optimization using the posterior from previous tasks . Finally , we use tools from the neuroscience literature to investigate how the learned networks overcome the challenge of continual learning . Here , we find that the networks learn latent task representations that are stable over time after initial task learning , consistent with results from biological circuits . 2 Method . Notations We use X > , X−1 , Tr ( X ) and vec ( X ) to denote the transpose , inverse , trace , and column-wise vectorization of a matrix X . We use X ⊗ Y to represent the Kronecker product between matrices X ∈ Rn×n and Y ∈ Rm×m such that ( X ⊗ Y ) mi+k , mj+l = XijYkl . We use bold lower-case letters x to denote column vectors . Dk refers to a ‘ dataset ’ corresponding to task k , which in this work generally consists of a set of input-output pairs { x ( i ) k , y ( i ) k } such that ` k ( θ ) : = log p ( Dk|θ ) = ∑ i log pθ ( y ( i ) k |x ( i ) k ) is the task-related performance on task k for a model with parameters θ . Finally , we use D̂k to refer to a dataset generated by inputs from the kth task where { ŷ ( i ) k ∼ pθ ( y|x ( i ) k ) } are drawn from the model distributionM . 2.1 Bayesian continual learning . Problem statement In continual learning , we train a model on a set of K tasks { D1 , . . . , DK } that arrive sequentially , where the data distribution Dk for task k in general differs from D6=k . The aim is to learn a probabilistic model p ( D|θ ) that performs well on all tasks . The challenge in the continual learning setting stems from the sequential nature of learning , and in particular from the common assumption that the learner does not have access to “ past ” tasks ( i.e. , Dj for j < k ) when learning task k. While we enforce this stringent condition in this paper , our approach may be easily combined with memory-based techniques such as coresets or generative replay [ 8 , 10 , 13 , 32 , 34 , 36 , 38 , 42 , 43 , 45 , 48 ] . Bayesian approach The continual learning problem is naturally formalized in a Bayesian framework whereby the posterior after k−1 tasks is used as a prior for task k. More specifically , we choose a prior p ( θ ) on the model parameters and compute the posterior after observing k tasks according to Bayes ’ rule : p ( θ|D1 : k ) ∝ p ( θ ) k∏ k′=1 p ( Dk′ |θ ) ∝ p ( θ|D1 : k−1 ) p ( Dk|θ ) , ( 1 ) where D1 : k is a concatenation of the first k tasks ( D1 , . . . , Dk ) . In theory , it is thus possible to compute the exact posterior p ( θ|D1 : k ) after k tasks , while only observing Dk , by using the posterior p ( θ|D1 : k−1 ) after k − 1 tasks as a prior . However , as is often the case in Bayesian inference , the difficulty here is that the posterior is typically intractable . To address this challenge , it is common to perform approximate online Bayesian inference . That is , the posterior p ( θ|D1 : k−1 ) is approximated by a parametric distribution with parameters φk−1 . The approximate posterior q ( θ ; φk−1 ) is then used as a prior for task k. Online Laplace approximation A common approach is to use the Laplace approximation whereby the posterior p ( θ|D1 : k−1 ) is approximated as a multivariate Gaussian q using local gradient information [ 17 , 24 , 37 ] . This involves ( i ) finding a mode µk of the posterior during task k , and ( ii ) performing a second-order Taylor expansion around µk to construct an approximate Gaussian posterior q ( θ ; φk ) = N ( θ ; µk , Λ−1k ) , where Λk is the precision matrix and φk = ( µk , Λk ) . In this case , gradient-based optimization is used to find the posterior mode on task k ( c.f . Equation 1 ) : µk = arg max θ log p ( θ|Dk , φk−1 ) ( 2 ) = arg max θ log p ( Dk|θ ) + log q ( θ ; φk−1 ) ( 3 ) = arg max θ ` k ( θ ) − 1 2 ( θ − µk−1 ) > Λk−1 ( θ − µk−1 ) ︸ ︷︷ ︸ : = Lk ( θ ) ( 4 ) The precision matrix Λk is given by the Hessian of the negative log posterior at µk : Λk = − ∇2θ log p ( θ|Dk , φk−1 ) ∣∣ θ=µk = H ( Dk , µk ) + Λk−1 , ( 5 ) where H ( Dk , µk ) = − ∇2θ log p ( Dk|θ ) ∣∣ θ=µk is the Hessian of the negative log likelihood of Dk . Continual learning with the online Laplace approximation thus involves two steps for each new task Dk . First , given Dk and the previous posterior q ( θ ; µk−1 , Λ−1k−1 ) ( i.e . the new prior ) , µk is found using gradient-based optimization ( Equation 4 ) . This step can be interpreted as optimizing the likelihood of Dk while penalizing changes in the parameters θ according to their importance for previous tasks , as determined by the prior precision matrix Λk−1 . Second , the new posterior precision matrix Λk is computed according to Equation 5 . Approximating the Hessian In practice , computing Λk presents two major difficulties . First , because q ( θ ; φk ) is a Gaussian distribution , Λk has to be positive semi-definite ( PSD ) , which is not guaranteed for the Hessian H ( Dk , µk ) . Second , if the number of model parameters nθ is large , it may be prohibitive to compute a full ( nθ × nθ ) matrix . To address the first issue , it is common to approximate the Hessian with the Fisher information matrix ( FIM ; 17 , 30 , 37 ) : Fk = Ep ( D̂k|θ ) [ ∇θ log p ( D̂k|θ ) ∇θ log p ( D̂k|θ ) > ] ∣∣∣ θ=µk ≈ H ( Dk , µk ) ( 6 ) The FIM is PSD , which ensures that Λk = ∑k k′=1 Fk′ is also PSD . Computing Fk may still be impractical if there are many model parameters , and it is therefore common to further approximate the FIM using structured approximations with fewer parameters . In particular , a diagonal approximation to Fk recovers Elastic Weight Consolidation ( EWC ; 24 ) , while a Kronecker-factored approximation [ 31 ] recovers the method proposed by Ritter et al . [ 37 ] . We denote this method ‘ KFAC ’ and use it in Section 3 as a comparison for our own Kronecker-factored method . 2.2 Natural continual learning . While the online Laplace approximation has been applied successfully in several continual learning settings [ 24 , 37 ] , it has also been found to perform sub-optimally on a range of problems [ 12 , 46 ] . Additionally , its Bayesian interpretation in theory prescribes a unique way of weighting the contributions of previous and current tasks to the loss . However , to perform well in practice , weight regularization approaches have been found to require ad-hoc re-weighting of the prior term by several orders of magnitude [ 24 , 37 , 46 ] . These shortcomings could be due to an inadequacy of the approximations used to construct the posterior ( Section 2.1 ) . However , we show in Figure 1 that standard gradient descent on the Laplace posterior has important drawbacks even in the exact case . First , we show that exact Bayesian inference on a simple continual regression problem can produce indirect optimization paths along which previous tasks are transiently forgotten as a new task is being learned ( Figure 1A ; green ) . Second , when the loss is non-convex , we show that exact Bayesian inference can still lead to catastrophic forgetting ( Figure 1B ; green ) . An alternative approach that has found recent success in a continual learning setting involves projection based methods which restrict parameter updates to a subspace that does not interfere with previous tasks [ 12 , 53 ] . However , it is not immediately obvious how this projected subspace should be selected in a way that appropriately balances learning on previous and current tasks . Additionally , such projection-based algorithms have fixed points that are minima of the current task , but not necessarily minima of the ( negative ) Bayesian posterior . This can lead to catastrophic forgetting in the limit of long training times ( Figure 1 ; pink ) , unless the learning rate is exactly zero in directions that interfere with previous tasks . To combine the desirable features of both classes of methods , we introduce “ Natural Continual Learning ” ( NCL ) – an extension of the online Laplace approximation that also restricts parameter updates to directions which do not interfere strongly with previous tasks . In a Bayesian setting , we can conveniently express what is meant by such directions in terms of the prior precision matrix Λ . In particular , ‘ flat ’ directions of the prior ( low precision ) correspond to directions that will not significantly affect the performance on previous tasks . Formally , we derive NCL as the solution of a trust region optimization problem . This involves minimizing the posterior loss Lk ( θ ) within a region of radius r centered around θ with a distance metric of the form d ( θ , θ + δ ) = √ δ > Λk−1δ/2 that takes into account the curvature of the prior via its precision matrix Λk−1 : δ = arg min δ Lk ( θ ) +∇θLk ( θ ) > δ subject to 1 2 δ > Λk−1δ ≤ r2 , ( 7 ) where Lk ( θ + δ ) ≈ Lk ( θ ) + ∇θLk ( θ ) > δ is a first-order approximation to the updated Laplace objective . The solution to this subproblem is given by δ ∝ Λ−1k−1∇θ ` k ( θ ) − ( θ − µk−1 ) ( see Appendix A for a derivation ) , which gives rise to the NCL update rule θ ← θ + γ [ Λ−1k−1∇θ ` k ( θ ) − ( θ − µk−1 ) ] ( 8 ) for a learning rate parameter γ ( which is implicitly a function of r in Equation 7 ) . To get some intuition for this learning rule , we note that Λ−1k−1 acts as a preconditioner for the first ( likelihood ) term , which drives learning on the current task while encouraging parameter changes predominantly in directions that do not interfere with previous tasks . Meanwhile , the second term encourages θ to stay close to µk−1 , the optimal parameters for the previous task . As we illustrate in Figure 1 , this combines the desirable features of both Bayesian weight regularization and projection-based methods . In particular , NCL shares the fixed points of the Bayesian posterior while also mitigating intermediate or complete forgetting of previous tasks by preconditioning with the prior covariance . Notably , if the loss landscape is non-convex ( as it generally will be ) , NCL can converge to a different local optimum from standard weight regularization despite having the same fixed points ( Figure 1B ) . Implementation The general NCL framework can be applied with different approximations to the Fisher matrix Fk in Equation 6 ( see Section 2.1 ) . In this work , we use a Kronecker-factored approximation [ 31 , 37 ] . However , even after making a Kronecker-factored approximation to Fk for each task k , it remains difficult to compute the inverse of a sum of k Kronecker products ( c.f . Equation 5 ) . To address this challenge , we derived an efficient algorithm for making a Kroneckerfactored approximation to Λk = Fk + Λk−1 ≈ Ak ⊗Gk when Λk−1 = Ak−1 ⊗Gk−1 and Fk are also Kronecker products . This approximation minimizes the KL-divergence between N ( µk , ( Ak ⊗ Gk ) −1 ) and N ( µk , ( Λk−1 + Fk ) −1 ) ( see Appendix G for details ) . Before training on the first task , we assume a spherical Gaussian prior θ ∼ N ( 0 , p−2w I ) . The scale parameter pw can either be set to a fixed value ( e.g . 1 ) or treated as a hyperparameter , and we optimize pw explicitly for our experiments in feedforward networks . NCL also has a parameter α which is used to stabilize the matrix inversion Λ−1k−1 ≈ ( Ak−1 ⊗Gk−1 + α2I ) −1 ( Appendix E ) . This is equivalent to a hyperparameter used for such matrix inversions in OWM [ 53 ] and DOWM [ 12 ] , and it is important for good performance with these methods . The pw and α are largely redundant for NCL , and we generally prefer to fix α to a small value ( 10−10 ) and optimize the pw only . However , for our experiments in RNNs , we instead fix pw = 1 and perform a hyperparameter optimization over α for a more direct comparison with OWM and DOWM . The NCL algorithm is described in pseudocode in Appendix E together with additional implementation and computational details . Finally , while we have derived NCL with a Laplace approximation in this section for simplicity , it can similarly be applied in the variational continual learning framework of Nguyen et al . [ 32 ] ( Appendix J ) . Our code is available online1 .","Biological agents are known to learn many different tasks over the course of their lives, and to be able to revisit previous tasks and behaviors with little to no loss in performance. In contrast, artificial agents are prone to ‘catastrophic forgetting’ whereby performance on previous tasks deteriorates rapidly as new ones are acquired. This shortcoming has recently been addressed using methods that encourage parameters to stay close to those used for previous tasks. This can be done by (i) using specific parameter regularizers that map out suitable destinations in parameter space, or (ii) guiding the optimization journey by projecting gradients into subspaces that do not interfere with previous tasks. However, these methods often exhibit subpar performance in both feedforward and recurrent neural networks, with recurrent networks being of interest to the study of neural dynamics supporting biological continual learning. In this work, we propose Natural Continual Learning (NCL), a new method that unifies weight regularization and projected gradient descent. NCL uses Bayesian weight regularization to encourage good performance on all tasks at convergence and combines this with gradient projection using the prior precision, which prevents catastrophic forgetting during optimization. Our method outperforms both standard weight regularization techniques and projection based approaches when applied to continual learning problems in feedforward and recurrent networks. Finally, the trained networks evolve task-specific dynamics that are strongly preserved as new tasks are learned, similar to experimental findings in biological circuits.","This paper proposes a novel regularization based continual learning approach, NCL, which is mainly used for recurrent neural network (RNN). Since NCL is based on Bayesian continual learning, NCL utilizes the approximate posterior of previous tasks as a prior for current task, and the Laplace approximation is used to compute the approximate posterior. Through simple toy analyses on convex and non-convex loss landscapes for NCL and other baseline methods, authors show that previous methods have a problem on the trajectories of convergence when learning a new task. Based on the results on simple toy analyses, authors proposed a novel trust region based regularization method that modulate the gradient by considering the curvature of prior distribution via its precision matrix. The proposed technique is highly similar to projection based methods, but its origin is quite different from previous methods. Experiment results show that NCL outperforms other baselines in stimulus-response and MNIST tasks. ","The phenomenon of catastrophic forgetting is a common feature of machine learning algorithms. This is in contrast to biological agents who are capable of learning many different behaviors over the course of their lives with little to no interference across tasks. Therefore, the study of continual learning in biological networks can help inspire novel approaches in machine learning. A continual learning algorithm has been developed that not only encourages good performance across tasks at convergence but also regularizes the optimization path itself using trust region optimization. However, continual learning in these settings has recently proven challenging for many existing algorithms. Therefore, we introduce “Natural Continual Learning” (NCL) as an extension of the online Laplace approximation that also restricts parameter updates to directions that do not interfere strongly with previous tasks.","Continual learning in artificial agents is a challenging task, as the agent must learn new tasks while avoiding forgetting previous ones. Bayesian continual learning is a popular approach that uses weight regularization to mitigate forgetting, but this method can suffer from shortcomings such as over-reliance on prior knowledge. Natural continual learning (NCL) is a new method that combines Bayesian continual learning with trust region optimization, which encourages parameter updates in directions that do not interfere with previous tasks. NCL outperforms previous continual learning algorithms in both feedforward and recurrent networks. The neuroscience literature suggests that the learned networks in NCL overcome the challenge of continual learning by learning latent task representations that are stable over time. NCL provides a natural way to weigh previous and current task information and can be viewed as an approximation to a Bayesian approximate posterior using the posterior from previous tasks.","0,1,2",,,,,,,,,,,,,,,,,
11,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"1 INTRODUCTION . Despite the success of deep neural networks in machine learning , they are often found to be highly overparametrized making them computationally expensive with excessive memory requirements . Pruning such large networks with minimal loss in performance is appealing for real-time applications , especially on resource-limited devices . In addition , compressed neural networks utilize the model capacity efficiently , and this interpretation can be used to derive better generalization bounds for neural networks ( Arora et al . ( 2018 ) ) . In network pruning , given a large reference neural network , the goal is to learn a much smaller subnetwork that mimics the performance of the reference network . The majority of existing methods in the literature attempt to find a subset of weights from the pretrained reference network either based on a saliency criterion ( Mozer & Smolensky ( 1989 ) ; LeCun et al . ( 1990 ) ; Han et al . ( 2015 ) ) or utilizing sparsity enforcing penalties ( Chauvin ( 1989 ) ; Carreira-Perpiñán & Idelbayev ( 2018 ) ) . Unfortunately , since pruning is included as a part of an iterative optimization procedure , all these methods require many expensive prune – retrain cycles and heuristic design choices with additional hyperparameters , making them non-trivial to extend to new architectures and tasks . In this work , we introduce a saliency criterion that identifies connections in the network that are important to the given task in a data-dependent way before training . Specifically , we discover important connections based on their influence on the loss function at a variance scaling initialization , which we call connection sensitivity . Given the desired sparsity level , redundant connections are pruned once prior to training ( i.e. , single-shot ) , and then the sparse pruned network is trained in the standard way . Our approach has several attractive properties : • Simplicity . Since the network is pruned once prior to training , there is no need for pretraining and complex pruning schedules . Our method has no additional hyperparameters and once pruned , training of the sparse network is performed in the standard way . • Versatility . Since our saliency criterion chooses structurally important connections , it is robust to architecture variations . Therefore our method can be applied to various architectures including convolutional , residual and recurrent networks with no modifications . i • Interpretability . Our method determines important connections with a mini-batch of data at single-shot . By varying this mini-batch used for pruning , our method enables us to verify that the retained connections are indeed essential for the given task . We evaluate our method on MNIST , CIFAR-10 , and Tiny-ImageNet classification datasets with widely varying architectures . Despite being the simplest , our method obtains extremely sparse networks with virtually the same accuracy as the existing baselines across all tested architectures . Furthermore , we investigate the relevance of the retained connections as well as the effect of the network initialization and the dataset on the saliency score . 2 RELATED WORK . Classical methods . Essentially , early works in network pruning can be categorized into two groups ( Reed ( 1993 ) ) : 1 ) those that utilize sparsity enforcing penalties ; and 2 ) methods that prune the network based on some saliency criterion . The methods from the former category ( Chauvin ( 1989 ) ; Weigend et al . ( 1991 ) ; Ishikawa ( 1996 ) ) augment the loss function with some sparsity enforcing penalty terms ( e.g. , L0 or L1 norm ) , so that back-propagation effectively penalizes the magnitude of the weights during training . Then weights below a certain threshold may be removed . On the other hand , classical saliency criteria include the sensitivity of the loss with respect to the neurons ( Mozer & Smolensky ( 1989 ) ) or the weights ( Karnin ( 1990 ) ) and Hessian of the loss with respect to the weights ( LeCun et al . ( 1990 ) ; Hassibi et al . ( 1993 ) ) . Since these criteria are heavily dependent on the scale of the weights and are designed to be incorporated within the learning process , these methods are prohibitively slow requiring many iterations of pruning and learning steps . Our approach identifies redundant weights from an architectural point of view and prunes them once at the beginning before training . Modern advances . In recent years , the increased space and time complexities as well as the risk of overfitting in deep neural networks prompted a surge of further investigation in network pruning . While Hessian based approaches employ the diagonal approximation due to its computational simplicity , impressive results ( i.e. , extreme sparsity without loss in accuracy ) are achieved using magnitude of the weights as the criterion ( Han et al . ( 2015 ) ) . This made them the de facto standard method for network pruning and led to various implementations ( Guo et al . ( 2016 ) ; Carreira-Perpiñán & Idelbayev ( 2018 ) ) . The magnitude criterion is also extended to recurrent neural networks ( Narang et al . ( 2017 ) ) , yet with heavily tuned hyperparameter setting . Unlike our approach , the main drawbacks of magnitude based approaches are the reliance on pretraining and the expensive prune – retrain cycles . Furthermore , since pruning and learning steps are intertwined , they often require highly heuristic design choices which make them non-trivial to be extended to new architectures and different tasks . Meanwhile , Bayesian methods are also applied to network pruning ( Ullrich et al . ( 2017 ) ; Molchanov et al . ( 2017a ) ) where the former extends the soft weight sharing in Nowlan & Hinton ( 1992 ) to obtain a sparse and compressed network , and the latter uses variational inference to learn the dropout rate which can then be used to prune the network . Unlike the above methods , our approach is simple and easily adaptable to any given architecture or task without modifying the pruning procedure . Network compression in general . Apart from weight pruning , there are approaches focused on structured simplification such as pruning filters ( Li et al . ( 2017 ) ; Molchanov et al . ( 2017b ) ) , structured sparsity with regularizers ( Wen et al . ( 2016 ) ) , low-rank approximation ( Jaderberg et al . ( 2014 ) ) , matrix and tensor factorization ( Novikov et al . ( 2015 ) ) , and sparsification using expander graphs ( Prabhu et al . ( 2018 ) ) or Erdős-Rényi random graph ( Mocanu et al . ( 2018 ) ) . In addition , there is a large body of work on compressing the representation of weights . A non-exhaustive list includes quantization ( Gong et al . ( 2014 ) ) , reduced precision ( Gupta et al . ( 2015 ) ) and binary weights ( Hubara et al . ( 2016 ) ) . In this work , we focus on weight pruning that is free from structural constraints and amenable to further compression schemes . 3 NEURAL NETWORK PRUNING . The main hypothesis behind the neural network pruning literature is that neural networks are usually overparametrized , and comparable performance can be obtained by a much smaller network ( Reed ( 1993 ) ) while improving generalization ( Arora et al . ( 2018 ) ) . To this end , the objective is to learn ii a sparse network while maintaining the accuracy of the standard reference network . Let us first formulate neural network pruning as an optimization problem . Given a dataset D = { ( xi , yi ) } ni=1 , and a desired sparsity level κ ( i.e. , the number of non-zero weights ) neural network pruning can be written as the following constrained optimization problem : min w L ( w ; D ) = min w 1 n n∑ i=1 ` ( w ; ( xi , yi ) ) , ( 1 ) s.t . w ∈ Rm , ‖w‖0 ≤ κ . Here , ` ( · ) is the standard loss function ( e.g. , cross-entropy loss ) , w is the set of parameters of the neural network , m is the total number of parameters and ‖ · ‖0 is the standard L0 norm . The conventional approach to optimize the above problem is by adding sparsity enforcing penalty terms ( Chauvin ( 1989 ) ; Weigend et al . ( 1991 ) ; Ishikawa ( 1996 ) ) . Recently , Carreira-Perpiñán & Idelbayev ( 2018 ) attempts to minimize the above constrained optimization problem using the stochastic version of projected gradient descent ( where the projection is accomplished by pruning ) . However , these methods often turn out to be inferior to saliency based methods in terms of resulting sparsity and require heavily tuned hyperparameter settings to obtain comparable results . On the other hand , saliency based methods treat the above problem as selectively removing redundant parameters ( or connections ) in the neural network . In order to do so , one has to come up with a good criterion to identify such redundant connections . Popular criteria include magnitude of the weights , i.e. , weights below a certain threshold are redundant ( Han et al . ( 2015 ) ; Guo et al . ( 2016 ) ) and Hessian of the loss with respect to the weights , i.e. , the higher the value of Hessian , the higher the importance of the parameters ( LeCun et al . ( 1990 ) ; Hassibi et al . ( 1993 ) ) , defined as follows : sj = |wj | , for magnitude based w2jHjj 2 or w2j 2H−1jj for Hessian based . ( 2 ) Here , for connection j , sj is the saliency score , wj is the weight , and Hjj is the value of the Hessian matrix , where the Hessian H = ∂2L/∂w2 ∈ Rm×m . Considering Hessian based methods , the Hessian matrix is neither diagonal nor positive definite in general , approximate at best , and intractable to compute for large networks . Despite being popular , both of these criteria depend on the scale of the weights and in turn require pretraining and are very sensitive to the architectural choices . For instance , different normalization layers affect the scale of the weights in a different way , and this would non-trivially affect the saliency score . Furthermore , pruning and the optimization steps are alternated many times throughout training , resulting in highly expensive prune – retrain cycles . Such an exorbitant requirement hinders the use of pruning methods in large-scale applications and raises questions about the credibility of the existing pruning criteria . In this work , we design a criterion which directly measures the connection importance in a datadependent manner . This alleviates the dependency on the weights and enables us to prune the network once at the beginning , and then the training can be performed on the sparse pruned network . Therefore , our method eliminates the need for the expensive prune – retrain cycles , and in theory , it can be an order of magnitude faster than the standard neural network training as it can be implemented using software libraries that support sparse matrix computations . 4 SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY . Given a neural network and a dataset , our goal is to design a method that can selectively prune redundant connections for the given task in a data-dependent way even before training . To this end , we first introduce a criterion to identify important connections and then discuss its benefits .","Pruning large neural networks while maintaining their performance is often desirable due to the reduced space and time complexity. In existing methods, pruning is done within an iterative optimization procedure with either heuristically designed pruning schedules or additional hyperparameters, undermining their utility. In this work, we present a new approach that prunes a given network once at initialization prior to training. To achieve this, we introduce a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. This eliminates the need for both pretraining and the complex pruning schedule while making it robust to architecture variations. After pruning, the sparse network is trained in the standard way. Our method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to various architectures including convolutional, residual and recurrent networks. Unlike existing methods, our approach enables us to demonstrate that the retained connections are indeed relevant to the given task.","The paper focuses on pruning neural networks. They propose to identify the nodes to be pruned even before training the whole network (conventionally, it is done as a separate step after the nn was trained and involves a number of iterations of retraining pruned nn). This initial step that identifies the connections to be pruned works off a mini-batch of data.","The article discusses the problem of overparametrization of deep neural networks and introduces a method for network pruning called single-shot pruning based on connection sensitivity. This approach identifies redundant connections from an architectural point of view and prunes them once at the beginning before training, thereby eliminating the need for pretraining and expensive prune-retrain cycles. The proposed method is simple, versatile, and interpretable, enabling the identification of important connections with a mini-batch of data before pruning.","Despite the success of deep neural networks in machine learning, they are often found to be highly overparameterized, making them computationally expensive with excessive memory requirements. In this work, we introduce a saliency criterion that identifies connections in the network that are important to the given task in a data-dependent way before training. Our method prunes redundant connections once prior to training and then trains the sparse pruned network in the standard way, resulting in extremely sparse networks with virtually the same accuracy as the existing baselines across all tested architectures.","0,1,2",,,,,,,,,,,,,,,,,
12,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"1 INTRODUCTION . An open debate since the inception of vision science concerns why we experience visual illusions . Consider the class of “ contextual ” illusions , where the perceived qualities of an image region , such as its orientation or color , are biased by the qualities of surrounding image regions . A well-studied contextual illusion is the orientation-tilt illusion depicted in Fig . 1a , where perception of the central grating ’ s orientation is influenced by the orientation of the surrounding grating ( O ’ Toole & Wenderoth , 1977 ) . When the two orientations are similar , the central grating appears tilted slightly away from the surround ( Fig . 1a , top ) . When the two orientations are dissimilar , the central grating appears tilted slightly towards the surround ( Fig . 1a , bottom ) . Is the contextual bias of the orientation-tilt illusion a bug of biology or a byproduct of optimized neural computations ? Over the past 50 years , there has been a number of neural circuit mechanisms proposed to explain individual contextual illusions ( reviewed in Mély et al. , 2018 ) . Recently , Mély et al . ( 2018 ) proposed a cortical circuit , constrained by physiology of primate visual cortex ( V1 ) , that offers a unified explanation for contextual illusions across visual domains – from the orientation-tilt illusion to color induction . These illusions arise in the circuit from recurrent interactions between neural populations with receptive fields that tile visual space , leading to contextual ( center/surround ) effects . For the orientation-tilt illusion , neural populations encoding the surrounding grating can either suppress or facilitate the activity of neural populations encoding the central grating , leading to repulsion vs. attraction , respectively . These surround neural populations compete to influence encodings of the central grating : suppression predominates when center/surround are similar , and facilitation predominates when center/surround are dissimilar . The neural circuit of Mély et al . ( 2018 ) explains how contextual illusions might emerge , but it does not explain why . One possibility is that contextual illusions like the orientation-tilt illusion are “ bugs ” : vestiges of evolution or biological constraints on the neural hardware . Another possibility is that contextual illusions are the by-product of efficient neural routines for scene segmentation ( Keemink & van Rossum , 2016 ; Mély et al. , 2018 ) . Here , we provide computational evidence for the latter †These authors contributed equally to this work . ‡Currently at DeepMind , London , UK . possibility and demonstrate that the orientation-tilt illusion reflects neural strategies optimized for object contour detection . Contributions We introduce the γ-Net , a trainable and hierarchical extension of the neural circuit of Mély et al . ( 2018 ) , which explains contextual illusions . ( i ) The γ-Net is more sample efficient than state-of-the-art convolutional architectures on two separate contour detection tasks . ( ii ) Similar to humans but not state-of-the-art contour detection models , the γ-Net exhibits an orientation-tilt illusion after being optimized for contour detection . This illusion emerges from its preference for high-level object-boundary contours over low-level edges , indicating that neural circuits involved in contextual illusions also support sample-efficient solutions to contour detection tasks . 2 RELATED WORK . Modeling the visual system Convolutional neural networks ( CNNs ) are often considered the de facto “ standard model ” of vision . CNNs and their extensions represent the state of the art for most computer vision applications with performance approaching – and sometimes exceeding – human observers on certain visual recognition tasks ( He et al. , 2016 ; Lee et al. , 2017 ; Phillips et al. , 2018 ) . CNNs also provide the best fit to rapid neural responses in the visual cortex ( see Kriegeskorte 2015 ; Yamins & DiCarlo 2016 for reviews ) . Nevertheless , multiple lines of evidence suggest that biological vision is still far more robust and versatile than CNNs ( see Serre , 2019 , for a recent review ) . CNNs suffer from occlusions and clutter ( Fyall et al. , 2017 ; Rosenfeld et al. , 2018 ; Tang et al. , 2018 ) . They are also sample inefficient at learning visual relations ( Kim et al. , 2018 ) and solving simple grouping tasks ( Linsley et al. , 2018c ) . State-of-the-art CNNs require massive datasets to reach their impressive accuracy ( Lake et al. , 2015 ) and their ability to generalize beyond training data is limited ( Geirhos et al. , 2018 ; Recht et al. , 2018 ) . Cortical feedback contributes to the robustness of biological vision ( Hochstein & Ahissar , 2002 ; Wyatte et al. , 2014 ; Kafaligonul et al. , 2015 ) . Feedforward projections in the visual system are almost always matched by feedback projections ( Felleman & Van Essen , 1991 ) , and feedback has been implicated in visual “ routines ” that can not be implemented through purely feedforward vision , such as incremental grouping or filling-in ( O ’ Reilly et al. , 2013 ; Roelfsema , 2006 ) . There is a also a growing body of work demonstrating the potential of recurrent neural networks ( RNNs ) to account for neural recordings ( Fyall et al. , 2017 ; Klink et al. , 2017 ; Siegel et al. , 2015 ; Tang et al. , 2018 ; Nayebi et al. , 2018 ; Kar et al. , 2019 ; Kietzmann et al. , 2019 ) . Feedback for computer vision In contrast to CNNs , which build processing depth through a cascade of filtering and pooling stages with unique weights , RNNs process stimuli with filtering stages that reuse weights over “ timesteps ” of recurrence . On each discrete processing timestep , an RNN updates its hidden state through a nonlinear combination of an input and its the hidden state from its previous timestep . RNNs have been extended from their roots in sequence processing ( e.g. , Mozer 1992 ) to computer vision by computing the activity of RNN units through convolutional kernels . The common interpretation of these convolutional-RNNs , is that the input to each layer functions as a ( fixed ) feedforward drive , which is combined with layer-specific feedback from an evolving hidden state to dynamically adjust layer activity ( Linsley et al. , 2018c ; George et al. , 2017 ; Lotter et al. , 2016 ; Wen et al. , 2018 ; Liao & Poggio , 2016 ; Spoerer et al. , 2017 ; Nayebi et al. , 2018 ; Tang et al. , 2018 ) . In the current work , we are motivated by a similar convolutional-RNN , the horizontal gated recurrent unit ( hGRU , Linsley et al . 2018a ) , which approximates the recurrent neural circuit model of ( Mély et al. , 2018 ) for explaining contextual illusions . 3 RECURRENT NEURAL MODELS . We begin by reviewing the dynamical neural circuit of Mély et al . ( 2018 ) . This model explains contextual illusions by simulating interactions between cortical hypercolumns tiling the visual field ( where hypercolumns describe a set of neurons encoding features for multiple visual domains at a single retinotopic position ) . In the model , hypercolumns are indexed by their 2D coordinate ( x , y ) and feature channels k. Units in hypercolumns encode idealized responses for a visual domain ( e.g. , neural responses from the orientation domain were used to simulate an orientation-tilt illusion ; ( a ) xyk ) interactions between units in a layer ` , which receives feedforward drive from a center pathway encoding feature k ( e.g. , edges oriented at 0◦ or 22.5◦ ) at position ( x , y ) in an image . Blocks depict different layers , and arrowed connections denote top-down feedback . ( c ) A deep network schematic of the circuit diagram in ( b ) , which forms the basis of the γ-Net introduced here . Horizontal and top-down connections are implemented with feedback gated recurrent units ( fGRUs ) . Image encodings pass through these blocks on every timestep , from bottom-up ( left path ) to top-down ( right path ) , and predictions are read out from the fGRU closest to image resolution on the final timestep . This motif can be stacked to create a hierarchical model . Fig . 1b ) . Dynamics of a single unit at xyk obey the following equations ( we bold activity tensors to distinguish them from learned kernels and parameters ) : ηḢ ( S ) xyk + 2H ( S ) xyk = [ ξZxyk − ( αH ( F ) xyk + µ ) C ( S ) xyk ] + # Stage 1 : Recurrent suppression of Z τḢ ( F ) xyk + σ 2H ( F ) xyk = [ νC ( F ) xyk ] + , # Stage 2 : Recurrent facilitation of H ( S ) where C ( S ) xyk = ( W S ∗H ( F ) ) xyk # Compute suppression interactions C ( F ) xyk = ( W F ∗H ( S ) ) xyk . # Compute facilitation interactions Circuit activities consist of a feedforward drive , recurrent suppression , and recurrent facilitation , respectively denoted as Z , H ( S ) , H ( F ) ∈ RX×Y×K ( X is width , Y is height of the tensor , and K is its feature channels ) ∗ . The circuit takes its “ feedforward ” input Z from hypercolumns ( e.g. , orientation encodings from hypercolumn units ) , and introduces recurrent suppressive and facilitatory interactions between units , C ( S ) , C ( F ) ∈ RX×Y×K ( Fig . 1b ) . These interactions are implemented with separate kernels for suppression and facilitation , WS , WF ∈ RE×E×K×K , where E is the spatial extent of connections on a single timestep ( connectivity in this model is constrained by primate physiology ) . ∗Suppression refers to interactions that reduce unit activity , and facilitation refers to interactions that increase activity . These computations can cause illusory repulsion or attraction at the level of neural populations . These interactions are implemented through convolutions , allowing them to serially spread over timesteps of processing to connect units positioned at different spatial locations . The circuit outputs H ( F ) after reaching steady state . The circuit model of Mély et al . ( 2018 ) depends on competition between H ( S ) and H ( F ) to explain the orientation-tilt illusion . Competition is implemented by ( i ) computing suppression vs. facilitation in separate stages , and ( ii ) having non-negative activities , which enforces these functionally distinct processing stages . With these constraints in the circuit model , the strength of recurrent suppression – but not facilitation – multiplicatively increases with the net recurrent output . For the orientation-tilt illusion , suppression predominates when center and surround gratings have similar orientations . This causes encodings of the surround grating to “ repulse ” encodings of the center grating . On the other hand , facilitation predominates ( causing “ attraction ” ) when center and surround gratings have dissimilar orientations because it is additive and not directly scaled by the circuit output . Parameters controlling the circuit ’ s integration , suppression/facilitation , and patterns of horizontal connections between units are tuned by hand . Linear and multiplicative suppression ( i.e. , shunting inhibition ) are controlled by scalars µ and α , feedforward drive is modulated by the scalar ξ , and linear facilitation is controlled by the scalar ν . Circuit time constants are scalars denoted by η , , τ and σ . All activities are non-negative and both stages are linearly rectified ( ReLU ) [ · ] + = max ( · , 0 ) . Feedback gated recurrent units Linsley et al . ( 2018a ) developed a version of this circuit for computer vision applications , called the hGRU . In their formulation they use gradient descent ( rather than hand-tuning like in the original circuit ) to fit its connectivity and parameters to image datasets . The hGRU was designed to learn a difficult synthetic incremental grouping task , and a single layer of the hGRU learned long-range spatial dependencies that CNNs with orders-of-magnitude more weights could not . The hGRU replaced the circuit ’ s time constants with dynamic gates , converted the recurrent state H ( S ) for suppression into an instantaneous activity , and introduced a term for quadratic facilitation . The hGRU also relaxed biological constraints from the original circuit , including an assumption of non-negativity , which enforced competition between recurrent suppression vs. facilitation ( e.g. , guaranteeing that Stage 1 in the circuit model describes suppression of Zxyk ) . We extend the hGRU formulation in two important ways . First , like Mély et al . ( 2018 ) , we introduce non-negativity . This constraint was critical for Mély et al . ( 2018 ) to explain contextual illusions , and as we describe below , was also important for our model . Second , we extend the circuit into a hierarchical model which can learn complex contour detection tasks . Recent neurophysiological work indicates that contextual effects emerge from both horizontal and top-down feedback ( Chettih & Harvey , 2019 ) . Motivated by this , we develop versions of the circuit to simulate horizontal connections between units within a layer , and top-down connections between units in different layers . We call our module the feedback gated recurrent unit ( fGRU ) . We describe the evolution of fGRU recurrent units in H ∈ RX×Y×K , which are influenced by non-negative feedforward encodings Z ∈ RX×Y×K ( e.g. , a convolutional layer ’ s response to a stimulus ) over discrete timesteps · [ t ] : Stage 1 : GS = sigmoid ( US ∗H [ t− 1 ] ) # Compute channel-wise selection CS = WS ∗ ( H [ t− 1 ] GS ) # Compute suppression interactions S = [ Z− [ ( αH [ t− 1 ] + µ ) CS ] + ] + , # Suppression of Z Stage 2 : GF = sigmoid ( UF ∗ S ) # Compute channel-wise recurrent updates CF = WF ∗ S # Compute facilitation interactions H̃ = [ ν ( CF + S ) + ω ( CF ∗ S ) ] + # Facilitation of S H [ t ] = ( 1−GF ) H [ t− 1 ] + GF H̃ . # Update recurrent state Like the original circuit , the fGRU has separate stages for suppression ( S ) and facilitation ( H ) . In the first stage , the feedforward encodings Z are suppressed by non-negative interactions between units in H [ t− 1 ] ( an fGRU hidden state from the previous timestep ) . Suppressive interactions are computed with the kernel WS ∈ RE×E×K×K , where E describes the spatial extent of horizontal connections on a single timestep . This kernel is convolved with a gated version of the persistent hidden state H [ t−1 ] . The gate activity GS is computed by applying a sigmoid nonlinearity to a convolution of the kernel US ∈ R1×1×K×K with H [ t− 1 ] , which transforms its activity into the range [ 0 , 1 ] . Additive and multiplicative forms of suppression are controlled by the parameters µ , α ∈ RK , respectively . In the second stage , additive and multiplicative facilitation is applied to the instantaneous activity S. The kernels WF ∈ RE×E×K×K controls facilitation interactions . Additive and multiplicative forms of facilitation are scaled by the parameters ν , ω ∈ RK , respectively . A gate activity is also computed during this stage to update the persistent recurrent activity H. The gate activity GF is computed by applying a sigmoid to a convolution of the kernel UF ∈ R1×1×K×K with S. This gate updates H [ t ] by interpolating H [ t− 1 ] with the candidate activity H̃ . After every timestep of processing , H [ t ] is taken as the fGRU output activity . As detailed in the following section , the fGRU output hidden state is either passed to the next convolutional layer ( Fig . 1c , fGRU ( ` ) −→ conv ( ` +1 ) ) , or used to compute top-down connections ( Fig . 1c , fGRU ( ` +1 ) −→ fGRU ( ` ) ) . The fGRU has different configurations for learning horizontal connections between units within a layer or top-down connections between layers ( Fig . 1b ) . These two configurations stem from changing the activities used for a fGRU ’ s feedforward encodings and recurrent hidden state . “ Horizontal connections ” between units within a layer are learned by setting the feedforward encodings Z to the activity of a preceding convolutional layer , and setting the hidden state H to a persistent activity initialized as zeros ( Fig . 1c , conv ( ` ) −→ fGRU ( ` ) ) . “ Top-down connections ” between layers are learned by setting fGRU feedforward encodings Z to the persistent hidden state H ( ` ) of a fGRU at layer ` in a hierarchical model , and the hidden state H to the persistent activity H ( ` +1 ) of an fGRU at a layer one level up in the model hierarchy ( Fig . 1c , fGRU ( ` +1 ) −→ fGRU ( ` ) ) . The functional interpretation of the top-down fGRU is that it first suppresses activity in the lower layer using the higher layer ’ s recurrent horizontal activities , and then applies a kernel to the residue for facilitation , which allows for computations like interpolation , sharpening , or “ filling in ” . Note that an fGRU for top-down connections does not have a persistent state ( it mixes high and low-level persistent states ) , but an fGRU for horizontal connections does . γ-Net Our main objective is to test how a model with the capacity for contextual illusions performs on natural image analysis . We do this by incorporating fGRUs into leading feedforward architectures for contour detection tasks , augmenting their feedforward processing with modules for learning feedback from horizontal and top-down connections ( Fig . 1c ) . We refer to the resulting hierarchical models as γ-Nets , because information flows in a loop that resembles a γ : image encodings make a full bottom-up to top-down cycle through the architecture on every timestep , until dense predictions are read-out from the lowest-level recurrent layer of the network ( thus , information flows in at the top of the hierarchy , loops through the network , and flows out from the top of the hierarchy ) . In our experiments we convert leading architectures for two contour detection problems into γ-Nets : A VGG16 for BSDS500 ( He et al. , 2019 ) , and a U-Net for detection of cell membranes in serial electron microscopy images ( Lee et al. , 2017 ) . See Appendix A for an algorithmic description of γ-net .","We introduce a deep recurrent neural network architecture that approximates known visual cortical circuits (Mély et al., 2018). We show that this architecture, which we refer to as the γ-Net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces γ-Net contour detection accuracy by driving it to prefer lowlevel edges over high-level object boundary contours. Overall, our study suggests that the orientation-tilt illusion is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour detection, and that incorporating such circuits in artificial neural networks can improve computer vision.","The article presents a debate on why humans experience visual illusions, specifically contextual illusions, such as the orientation-tilt illusion. The authors propose a cortical circuit, constrained by the physiology of primate visual cortex, that offers a unified explanation for contextual illusions across visual domains. This circuit model involves recurrent interactions between neural populations with receptive fields that tile visual space, leading to contextual effects. The authors introduce the gamma-Net, a trainable and hierarchical extension of the neural circuit of Mely et al. (2018), which explains contextual illusions and is more sample efficient than state-of-the-art convolutional architectures on two separate contour detection tasks. They demonstrate that the orientation-tilt illusion reflects neural strategies optimized for object contour detection, suggesting that contextual illusions are by-products of efficient neural routines for scene segmentation. The study also suggests that convolutional neural networks (CNNs), the de facto standard model of vision, are limited in sample efficiency and their ability to generalize beyond training data, and that recurrent neural networks (RNNs) offer potential advantages in simulating biological vision. The authors also introduce the feedback gated recurrent unit (fGRU), a trainable model for simulation of horizontal and top-down feedback. The fGRU has different configurations for learning horizontal or top-down connections and has separate stages for suppression and facilitation, where the feedforward encodings are suppressed by non-negative interactions between units and are facilitated in the second stage.","The article discusses the orientation-tilt illusion, a perceptual phenomenon where the perceived orientation of an image is influenced by the surrounding image regions. The authors propose a neural circuit model, the γ-Net, to explain the contextual illusions, including the orientation-tilt illusion, based on a hierarchical extension of the neural circuit of Mély et al. (2018). The γ-Net is more sample-efficient than state-of-the-art convolutional architectures and exhibits the orientation-tilt illusion after being optimized for contour detection tasks. The model demonstrates that contextual illusions like the orientation-tilt illusion arise from neural strategies optimized for object contour detection. The authors introduce the feedback gated recurrent unit (fGRU), a hierarchical extension of the horizontal gated recurrent unit (hGRU) that can learn complex contour detection tasks. The fGRU model includes horizontal connections within layers and top-down connections between layers, which simulate the feedback connections in the visual system. The article highlights the importance of non-negativity constraints in the circuit model, which were critical for explaining contextual illusions, and the extension of the circuit into a hierarchical model for learning complex contour detection tasks.","The presented paper introduces a novel neural network architecture to explore the question whether visual illusions are corner cases of the human visual system, or whether they represent limitations of perception. The developed recurrent network architecture aims at being more sample efficient than existing methods. The findings discussed in the paper suggest that visual illusions are a byproduct of neural circuits that help to increase the robustness of the human visual system, which in turn suggests that neural networks for processing visual data could benefit from integrating circuits in similar ways. While existing work predominantly aims at explaining whether visual illusions are features or artifacts of the visual system, this work focuses on finding a computational solution to support the hypothesis that visual illusions are features. In particular, the contributions of this work are: (1) novel neural network architecture, called \gamma-networks, which is derived from the work of [Meley et al. 2018] and (2) that the proposed architecture is more sample efficient than SOTA convolutional architectures on contour detection tasks. ","1,2,0",,,,,,,,,,,,,,,,,
13,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"1 Introduction . Recently , deep reinforcement learning ( DRL ) methods have demonstrated encouraging performance on a variety of domains such as outperforming humans in complex games [ 1–4 ] or controlling robots [ 5–11 ] . Despite the recent progress in the field , acquiring complex skills through trial and error still remains challenging and these neural network policies often have difficulty generalizing to novel scenarios . Moreover , such policies are not interpretable to humans and therefore are difficult to debug when these challenges arise . To address these issues , a growing body of work aims to learn programmatic policies that are structured in more interpretable and generalizable representations such as decision trees [ 12 ] , statemachines [ 13 ] , and programs described by domain-specific programming languages [ 14 , 15 ] . Yet , the programmatic representations employed in these works are often limited in expressiveness due to constraints on the policy spaces . For example , decision tree policies are incapable of naïvely generating repetitive behaviors , state machine policies used in [ 13 ] are computationally complex to ∗Contributed equally . †Work partially done as a visiting scholar at USC . ‡AI Advisor at NAVER AI Lab . 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . scale to policies representing diverse behaviors , and the programs of [ 14 , 15 ] are constrained to a set of predefined program templates . On the other hand , program synthesis works that aim to represent desired behaviors using flexible domain-specific programs often require extra supervision such as input/output pairs [ 16–20 ] or expert demonstrations [ 21 , 22 ] , which can be difficult to obtain . In this paper , we present a framework to instead synthesize human-readable programs in an expressive representation , solely from rewards , to solve tasks described by Markov Decision Processes ( MDPs ) . Specifically , we represent a policy using a program composed of control flows ( e.g . if/else and loops ) and an agent ’ s perceptions and actions . Our programs can flexibly compose behaviors through perception-conditioned loops and nested conditional statements . However , composing individual program tokens ( e.g . if , while , move ( ) ) in a trial-and-error fashion to synthesize programs that can solve given MDPs can be extremely difficult and inefficient . To address this problem , we propose to first learn a latent program embedding space where nearby latent programs correspond to similar behaviors and allows for smooth interpolation , together with a program decoder that can decode a latent program to a program consisting of a sequence of program tokens . Then , when a task is given , this embedding space allows us to iteratively search over candidate latent programs to find a program that induces desired behavior to maximize the reward . Specifically , this embedding space is learned through reconstruction of randomly generated programs and the behaviors they induce in the environment in an unsupervised manner . Once learned , the embedding space can be reused to solve different tasks without retraining . To evaluate the proposed framework , we consider the Karel domain [ 23 ] , featuring an agent navigating through a gridworld and interacting with objects to solve tasks such as stacking and navigation . The experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms program synthesis and deep RL baselines . In addition , we justify the necessity of the proposed two-stage learning scheme as well as conduct an extensive analysis comparing various approaches for learning the latent program embedding spaces . Finally , we perform experiments which highlight that the programs produced by our proposed framework can both generalize to larger state spaces and unseen state configurations as well as be interpreted and edited by humans to improve their task performance . 2 Related Work . Neural program induction and synthesis . Program induction methods [ 20 , 24–36 ] aim to implicitly induce the underlying programs to mimic the behaviors demonstrated in given task specifications such as input/output pairs or expert demonstrations . On the other hand , program synthesis methods [ 16– 19 , 21 , 37–58 ] explicitly synthesize the underlying programs and execute the programs to perform the tasks from task specifications such input/output pairs , demonstrations , language instructions . In contrast , we aim to learn to synthesize programs solely from reward described by an MDP without other task specifications . Similarly to us , a two-stage synthesis method is proposed in [ 46 ] . Yet , the task is to match truth tables for given test programs rather than solve MDPs . Their first stage requires the entire ground-truth table for each program synthesized during training , which is infeasible to apply to our problem setup ( i.e . synthesizing imperative programs for solving MDPs ) . Learning programmatic policies . Prior works have also addressed the problem of learning programmatic policies [ 59–61 ] . Bastani et al . [ 12 ] learns a decision tree as a programmatic policy for pong and cartpole environments by imitating an oracle neural policy . However , decision trees are incapable of representing repeating behaviors on their own . Silver et al . [ 49 ] addresses this by including a loop-style token for their decision tree policy , though it is still not as expressive as synthesized loops . Inala et al . [ 13 ] learns programmatic policies as finite state machines by imitating a teacher policy , although finite state machine complexity can scale quadratically with the number of states , making them difficult to scale to more complex behaviors . Another line of work instead synthesizes programs structured in Domain Specific Languages ( DSLs ) , allowing humans to design tokens ( e.g . conditions and operations ) and control flows ( e.g . while loops , if statements , reusable functions ) to induce desired behaviors and can produce human interpretable programs . Verma et al . [ 14 , 15 ] distill neural network policies into programmatic policies . Yet , the initial programs are constrained to a set of predefined program templates . This significantly limits the scope of synthesizable programs and requires designing such templates for each task . In contrast , our method can synthesize diverse programs , without templates , which can flexibly represent the complex behaviors required to solve various tasks . 3 Problem Formulation We are interested in learning to synthesize a program structured in a given DSL that can be executed to solve a given task described by an MDP , purely from reward . In this section , we formally define our definition of a program and DSL , tasks described by MDPs , and the problem formulation . Program and Domain Specific Language . The programs , or programmatic policies , considered in this work are defined based on a DSL as shown in Figure 1 . The DSL consists of control flows and an agent ’ s perceptions and actions . A perception indi- cates circumstances in the environment ( e.g . frontIsClear ( ) ) that can be perceived by an agent , while an action defines a certain behavior that can be performed by an agent ( e.g . move ( ) , turnLeft ( ) ) . Control flow includes if/else statements , loops , and boolean/logical operators to compose more sophisticated conditions . A policy considered in this work is described by a program ρ which is executed to produce a sequence of actions given perceptions from the environment . MDP . We consider finite-horizon discounted MDPs with initial state distribution µ ( so ) and discount factor γ . For a fixed sequence { ( s0 , a0 ) , ... , ( st , at ) } of states and actions obtained from a rollout of a given policy , the performance of the policy is evaluated based on a discounted return ∑T t=0 γ trt , where T is the horizon of the episode and rt = R ( st , at ) the reward function . Objective . Our objective is maxρ Ea∼EXEC ( ρ ) , s0∼µ [ ∑T t=0 γ trt ] , where EXEC returns the actions induced by executing a program policy ρ in the environment . Note that one can view this objective as a special case of the standard RL objective , where the policy is represented as a program which follows the grammar of the DSL and the policy rollout is obtained by executing the program . 4 Approach . Our goal is to develop a framework that can synthesize a program ( i.e . a programmatic policy ) structured in a given DSL that can be executed to solve a task of interest . This requires the ability to synthesize a program that is not only valid for execution ( e.g . grammatically correct ) but also describes desired behaviors for solving the task from only the reward . Yet , learning to synthesize such a program from scratch for every new task can be difficult and inefficient . To this end , we propose our Learning Embeddings for lAtent Program Synthesis framework , dubbed LEAPS , as illustrated in Figure 2 . LEAPS first learns a latent program embedding space that continuously parameterizes diverse behaviors and a program decoder that decodes a latent program to a program consisting of a sequence of program tokens . Then , when a task is given , we iteratively search over this embedding space and decode each candidate latent program using the decoder to find a program that maximizes the reward . This two-stage learning scheme not only enables learning to synthesize programs to acquire desired behaviors described by MDPs solely from reward , but also allows reusing the learned embedding space to solve different tasks without retraining . In the rest of this section , we describe how we construct the model and our learning objectives for the latent program embedding space in Section 4.1 . Then , we present how a program that describes desired behaviors for a given task can be found through a search algorithm in Section 4.2 . 4.1 Learning a Program Embedding Space . To learn a latent program embedding space , we propose to train a variational autoencoder ( VAE ) [ 62 ] that consists of a program encoder qφ which encodes a program ρ to a latent program z and a program decoder pθ which reconstructs the program from the latent . Specifically , the VAE is trained through reconstruction of randomly generated programs and the behaviors they induce in the environment in an unsupervised manner . Architectural details are listed in Section L.6 . Since we aim to iteratively search over the learned embedding space to achieve certain behaviors when a task is given , we want this embedding space to allow for smooth behavior interpolation ( i.e . programs that exhibit similar behaviors are encoded closer in the embedding space ) . To this end , we propose to train the model by optimizing the following three objectives . 4.1.1 Program Reconstruction . To learn a program embedding space , we train a program encoder qφ and a program decoder pθ to reconstruct programs composed of sequences of program tokens . Given an input program ρ consisting of a sequence of program tokens , the encoder processes the input program one token at a time and produces a latent program embedding z . Then , the decoder outputs program tokens one by one from the latent program embedding z to synthesize a reconstructed program ρ̂ . Both the encoder and the decoder are recurrent neural networks and are trained to optimize the β-VAE [ 63 ] loss : LPθ , φ ( ρ ) = −Ez∼qφ ( z|ρ ) [ log pθ ( ρ|z ) ] + βDKL ( qφ ( z|ρ ) ‖pθ ( z ) ) . ( 1 )","Recently, deep reinforcement learning (DRL) methods have achieved impressive performance on tasks in a variety of domains. However, neural network policies produced with DRL methods are not human-interpretable and often have difficulty generalizing to novel scenarios. To address these issues, prior works explore learning programmatic policies that are more interpretable and structured for generalization. Yet, these works either employ limited policy representations (e.g. decision trees, state machines, or predefined program templates) or require stronger supervision (e.g. input/output state pairs or expert demonstrations). We present a framework that instead learns to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies. We also justify the necessity of the proposed two-stage learning scheme as well as analyze various methods for learning the program embedding. Website at https://clvrai.com/leaps.",The paper approaches reinforcement learning from a program synthesis perspective and proposes a new method - LEAPS - that is capable of learning programmatic policies just from weak reward signals. This is achieved in two steps: i) learning a smooth latent manifold where similar programs are encoded close to each other; ii) searching the latent space efficiently via the Cross Entropy Method. LEAPS is applied to several tasks in the Karel domain where it successfully can learn programmatic policies that are even transferable to larger state spaces for zero-shot learning.,"The article discusses the challenges faced by deep reinforcement learning methods in acquiring complex skills through trial and error and their difficulty in generalizing to new scenarios. To address these issues, the article proposes a framework for synthesizing human-readable programs in an expressive representation solely from rewards to solve tasks described by Markov Decision Processes (MDPs). The proposed framework involves learning a latent program embedding space through randomly generated programs and behaviors they induce in the environment in an unsupervised manner. The experimental results show that the proposed framework not only learns to synthesize task-solving programs but also outperforms program synthesis and deep RL baselines.","The article presents a framework for learning to synthesize programmatic policies for solving Markov Decision Processes (MDPs) from reward signals alone. The proposed framework, called Learning Embeddings for lAtent Program Synthesis (LEAPS), consists of two stages: learning a latent program embedding space and searching for a program that maximizes the reward in the embedding space. The embedding space is learned through a variational autoencoder (VAE) trained on randomly generated programs and their corresponding behaviors in the environment. The framework is able to learn to synthesize programs that can solve complex tasks, such as stacking blocks, without requiring any task-specific supervision or demonstrations.","0,1,2",1,"0,2",2,0,,coverage,coverage,coverage,factuality,1,"0,2",2,0,"All summaries present the paper well, but only summary B mentions that the method outperforms other DRL baselines.",Summaries A and C does not mention comparison to other baselines.,Good balance of details and coverage,summary presentes some factuality errors.
14,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"Given an unnormalized target distribution we want to obtain approximate samples from it and a tight lower bound on its ( log ) normalization constant logZ . Annealed Importance Sampling ( AIS ) with Hamiltonian MCMC is a powerful method that can be used to do this . Its main drawback is that it uses non-differentiable transition kernels , which makes tuning its many parameters hard . We propose a framework to use an AIS-like procedure with Uncorrected Hamiltonian MCMC , called Uncorrected Hamiltonian Annealing . Our method leads to tight and differentiable lower bounds on logZ . We show empirically that our method yields better performances than other competing approaches , and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements . 1 Introduction . Variational Inference ( VI ) [ 4 , 41 , 45 ] is a method to do approximate inference on a target distribution p ( z ) = p̄ ( z ) /Z that is only known up to the normalization constant Z . The basic insights are , first , that the evidence lower bound ( ELBO ) Eq ( z ) [ log p̄ ( z ) − log q ( z ) ] lower-bounds logZ and , second , that maximizing the ELBO is equivalent to minimizing the KL-divergence from q to p. The simplest VI method chooses a parameterized family for q and optimizes its parameters to maximize the ELBO . A recent direction involves combining VI with Markov chain Monte Carlo ( MCMC ) [ 34 , 43 ] . These methods can be seen as an instance of the auxiliary VI framework [ 2 ] – they create an augmented variational distribution that represents all intermediate random variables generated during the MCMC procedure . An augmented target distribution that attempts to capture the inverse MCMC dynamics is optimized jointly with this variational distribution . However , it has been observed that capturing inverse dynamics is challenging [ 43 , §5.4 ] ( further discussion in Section 4 ) . Annealed Importance Sampling ( AIS ) [ 22 , 27 ] is a powerful technique used to build augmented distributions without the need of learning inverse dynamics . While it was originally proposed to estimate expectations using importance sampling , it can be easily used to build lower bounds on normalization constants of intractable densities [ 18 , 44 ] . AIS creates a sequence of densities that bridge from a tractable initial approximation q to the target p̄ . Then , the augmented variational distribution is given by a sequence of MCMC kernels targeting each bridging density , while the augmented target uses the reversals of those kernels . It turns out that the ratio of these augmented distributions can be computed using only evaluations of the bridging densities . Combining Hamiltonian MCMC kernels with AIS has been observed to produce strong lower bounds [ 35 , 44 ] . However , these bounds are sensitive to numerous parameters , such as the initial distribution , bridging schedule , and parameters of the MCMC kernels . It would be desirable to optimize these parameters 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . to tighten the bound . Unfortunately , the presence of Metropolis-Hastings acceptance steps means that the the final estimator is non-differentiable , and thus reparameterization gradients can not be used . In this work , we propose Uncorrected Hamiltonian Annealing ( UHA ) , a differentiable alternative to Hamiltonian AIS . We define an augmented variational distribution using Hamiltonian MCMC kernels , but dropping the accept-reject steps . This is motivated by the fact that Hamiltonian dynamics sometimes have high acceptance rates . Since these uncorrected MCMC kernels do not exactly hold the bridging densities invariant , an augmented target distribution can not be defined in terms of reversals . Instead , we define our augmented target by deriving an algorithm for the exact reversal of the original ( corrected ) MCMC kernel and dropping the accept-reject step . Surprisingly , this yields a very simple expression for the resulting lower bound . We use reparameterization gradients to tune various parameters involved in the lower bound produced by UHA , including the initial approximation q , parameters of the uncorrected MCMC kernel , and the bridging densities . Experimentally , tuning all these leads to large gains . For example , in several inference tasks we observe that tuning UHA with K = 64 bridging densities gives better results than traditional Hamiltonian AIS with K = 512 . Finally , we use UHA to train VAEs [ 24 , 31 ] . In this case we observe that using UHA leads to higher ELBOs . In addition , we observe that increasing the number of bridging densities with UHA consistently leads to better results , and that for a large enough number of bridging densities the variational gap ( difference between ELBO and true log-likelihood ) becomes small , and models with higher log-likelihood are obtained . 2 Preliminaries . Variational inference and augmentation . Suppose that p ( z ) = 1Z p̄ ( z ) is some target density , where p̄ is unnormalized and Z = ∫ p̄ ( z ) dz is the corresponding normalizer , and let ELBO ( q ( z ) , p̄ ( z ) ) = E q ( z ) log p̄ ( z ) q ( z ) ( 1 ) be the `` ELBO operator '' . Variational inference ( VI ) is based on the fact that for any q ( z ) we have [ 4 ] logZ = ELBO ( q ( z ) , p̄ ( z ) ) + KL ( q ( z ) ‖p ( z ) ) . ( 2 ) In VI , the parameters of q are tuned to maximize the `` evidence lower bound '' ( ELBO ) . Since the KL-divergence is non-negative , this is always a lower bound on logZ . Also , maximizing the ELBO is equivalent to minimizing the KL-divergence from q to p. To get tighter bounds and better approximations recent work has made use of augmented distributions [ 2 , 21 ] . Let z1 : M = ( z1 , · · · , zM ) and suppose that p̄ ( z1 : M ) = p̄ ( zM ) p ( z1 : M−1|zM ) augments the original target density while preserving its normalization constant . Then , for any q ( z1 : M ) we have logZ = ELBO ( q ( z1 : M ) , p̄ ( z1 : M ) ) + KL ( q ( z1 : M ) ‖p ( z1 : M ) ) . ( 3 ) The first term is called the `` augmented '' ELBO and again lower bounds logZ . By the chain rule of KL-divergence [ 12 ] , the KL-divergence from q to p over z1 : M upper-bounds the KL-divergence over zM . This justifies using the marginal of q over zM to approximate the original target distribution . Annealed Importance Sampling . A successful approach for creating augmented distributions is Annealed Importance Sampling ( AIS ) [ 27 ] . It creates an augmented proposal distribution q by applying a sequence of transition densities Tm ( zm+1|zm ) , and an augmented target by defining transition densities Um ( zm|zm+1 ) . This gives the augmented densities q ( z1 : M ) = q ( z1 ) M−1∏ m=1 Tm ( zm+1|zm ) and p̄ ( z1 : M ) = p̄ ( zM ) M−1∏ m=1 Um ( zm|zm+1 ) . ( 4 ) Naively , the ratio of these densities is p̄ ( z1 : M ) q ( z1 : M ) = p̄ ( zM ) q ( z1 ) M−1∏ m=1 Um ( zm|zm+1 ) Tm ( zm+1|zm ) . ( 5 ) To define the transitions Tm and Um , AIS creates a sequence of unnormalized densities π̄1 , . . . , π̄M−1 that “ bridge ” from a starting distribution q to the target p̄ , meaning that π̄1 is close to q and π̄M−1 is close to p̄ . Then , for each intermediate distribution , Tm ( zm+1|zm ) is chosen to be a Markov kernel that holds πm invariant , and Um to be the reversal of Tm with respect to πm , defined as Um ( zm|zm+1 ) = T ( zm+1|zm ) πm ( zm ) πm ( zm+1 ) . ( 6 ) This choice produces a simplification so that eq . 5 becomes p̄ ( z1 : M ) q ( z1 : M ) = p̄ ( zM ) q ( z1 ) M−1∏ m=1 π̄m ( zm ) π̄m ( zm+1 ) . ( 7 ) This can be easily evaluated without needing to evaluate the transition densities . The ratio from eq . 7 can be used to get an expression for the lower bound ELBO ( q ( z1 : M ) , p̄ ( z1 : M ) ) . Research has shown that the AIS augmentation may lead to extremely tight lower bounds [ 18 , 17 , 35 , 44 ] . Hamiltonian Dynamics . Many MCMC methods used to sample from p ( z ) are based on Hamiltonian dynamics [ 3 , 8 , 29 , 42 ] . The idea is to create an augmented distribution p ( z , ρ ) = p ( z ) S ( ρ ) , where S ( ρ ) is a distribution over a momentum variable ρ ( e.g . a Multivariate Gaussian ) . Then , one can define numerical integration schemes where z and ρ evolve while nearly holding p ( z , ρ ) constant . When corrected by a Metropolis-Hastings acceptance step , this can be made to exactly hold p ( z , ρ ) invariant . This is alternated with a scheme that resamples the momentum ρ while holding S ( ρ ) invariant . When Hamiltonian dynamics work well , z can quickly move around , suppressing random-walk behavior . There are a variety of different Hamiltonian MCMC methods , corresponding to different integration schemes , momentum distributions , and ways of resampling the momentum . For instance , HMC and Langevin dynamics use the leapfrog integrator , a Gaussian for the momentum variables and a full resampling of the momentum variables at each step [ 29 , 42 ] . On the other hand , if the momentum variables are only partially resampled , the under-damped variants of HMC and Langevin dynamics are recovered [ 29 ] . It was observed that partial resampling may lead to improved perfomance [ 9 ] . It is easy to integrate Hamiltonian dynamics into AIS . First , define an augmented target p̄ ( z , ρ ) = p̄ ( z ) S ( ρ ) and an augmented starting distribution q ( z , ρ ) = q ( z ) S ( ρ ) . Then , create a series of augmented densities π̄1 ( z , ρ ) , . . . , π̄M−1 ( z , ρ ) bridging the two as π̄m ( z , ρ ) = π̄m ( z ) S ( ρ ) . Finally , define the forward transition Tm ( zm+1 , ρm+1|zm , ρm ) to be an iteration of a Hamiltonian MCMC method that leaves πm ( z , ρ ) invariant . We will describe a single transition Tm as a sequence of three steps : ( 1 ) resample the momentum ; ( 2 ) simulate Hamiltonian dynamics and apply an accept-reject step ; and ( 3 ) negate the momentum . The precise process that defines the transition is shown in Alg . 1 . Note that this algorithm is quite general , and compatible with HMC , Langevin dynamics and their underdamped variants ( by selecting an appropriate integrator and resampling method ) . Algorithm 1 Corrected Tm ( zm+1 , ρm+1|zm , ρm ) 1 . Sample ρ′m from some s ( ρ ′ m|ρm ) that leaves S ( ρ ) invariant . Set z′m ← zm . 2 . Simulate Hamiltonian dynamics as ( z′′m , ρ′′m ) ← Tm ( z′m , ρ′m ) . Calculate an acceptance probability α = min ( 1 , π̄m ( z′′m , ρ′′m ) /π̄m ( z′m , ρ′m ) ) . With probability α , set ( z′′′m , ρ′′′m ) ← ( z′′m , ρ′′m ) . Otherwise , set ( z′′′m , ρ′′′m ) ← ( z′m , ρ′m ) . 3 . Reverse the momentum as ( zm+1 , ρm+1 ) ← ( z′′′m , −ρ′′′m ) . return ( zm+1 , ρm+1 ) Representing Tm this way makes it easy to show it holds the density πm ( z , ρ ) invariant . The overall strategy is to show that each of the steps 1-3 holds πm invariant , and so does the composition of them [ 29 , §3.2 ] . For steps 1 and 3 this is trivial , provided that S ( ρ ) = S ( −ρ ) . For step 2 , we require that the simulation Tm has unit Jacobian and satisfies T −1m = Tm . Then , Tm can be interpreted as a symmetric Metropolis-Hastings proposal , meaning the Metroplis-Hastings acceptance probability α is as given . A typical choice for Tm that satisfies these requirements is the leapfrog integrator with a momentum reversal at the end . ( This reversal then gets `` un-reversed '' in step 3 for accepted moves . ) Since Tm holds πm invariant , we can define Um as the reversal of Tm wrt πm . Then , eq . 7 becomes p̄ ( z1 : M , ρ1 : M ) q ( z1 : M , ρ1 : M ) = p̄ ( zM , ρM ) q ( z1 , ρ1 ) M−1∏ m=1 π̄m ( zm , ρm ) π̄m ( zm+1 , ρm+1 ) . ( 8 ) Using this ratio we get an expression for the lower bound ELBO ( q ( z1 : M , ρ1 : M ) , p̄ ( z1 : M , ρ1 : M ) ) obtained with Hamiltonian AIS . While this method has been observed to yield strong lower bounds on logZ [ 35 , 44 ] ( see also Section 5.2 ) , its performance depends on many parameters : initial distribution q ( z ) , momentum distribution S , momentum resampling scheme , simulator Tm , and bridging densities . We would like to tune these parameters by maximizing the ELBO using reparameterization-based estimators . However , due to the accept-reject step required by the Hamiltonian MCMC transition , the resulting bound is not differentiable , and thus reparameterization gradients are not available .","Given an unnormalized target distribution we want to obtain approximate samples from it and a tight lower bound on its (log) normalization constant logZ. Annealed Importance Sampling (AIS) with Hamiltonian MCMC is a powerful method that can be used to do this. Its main drawback is that it uses non-differentiable transition kernels, which makes tuning its many parameters hard. We propose a framework to use an AIS-like procedure with Uncorrected Hamiltonian MCMC, called Uncorrected Hamiltonian Annealing. Our method leads to tight and differentiable lower bounds on logZ. We show empirically that our method yields better performances than other competing approaches, and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements.","The article proposes a new method called ""Uncorrected Hamiltonian Annealing"" (UHA), which is a differentiable alternative to Hamiltonian Annealed Importance Sampling (AIS). UHA creates an augmented variational distribution using Hamiltonian MCMC kernels but without the Metropolis-Hastings acceptance step, which makes it easier to tune its many parameters, such as the initial distribution, bridging schedule, and MCMC kernel parameters. The proposed method yields better performance than other competing approaches and leads to tight and differentiable lower bounds on the log-normalization constant. The authors also show that using UHA to train VAEs leads to higher ELBOs and that increasing the number of bridging densities with UHA consistently leads to better results.","The paper propose to use uncorrected HMC kernels (i.e. HMC kernels without accept-reject steps) in AIS to make the estimate of log normalisation constant differentiable w.r.t. to various parameters (e.g. step-size for numerical integration). The author(s) name(s) the method UHA. The promise is that one can then using reparameterisation gradient to tune these parameters. Through experiments on inference tasks, the benefit of tuning parameters in UHA is demonstrated against other methods that use HMC in AIS. For experiments on VAE training, UHA with tuned parameters has shown better test ELBO than IW.","In this article, we propose a new method called Uncorrected Hamiltonian Annealing (UHA) to estimate the normalization constant of a target distribution p(z) without using non-differentiable transition kernels. UHA uses an augmented variational distribution created by applying Hamiltonian MCMC kernels to bridging densities, which are intermediate distributions between the initial distribution and the target. The resulting lower bound is differentiable, allowing for efficient optimization using reparameterization gradients. Experimental results show that UHA outperforms traditional Hamiltonian AIS in terms of lower bound tightness and computational efficiency.","1,0,2",,,,,,,,,,,,,,,,,
15,SP:4c2928f6772664d63c02c29f913b476e1c932983,"1 INTRODUCTION . Multi-Task Learning ( MTL ) ( Caruana , 1997 ; Zhang & Yang , 2021 ) , which aims to improve the generalization performance of multiple learning tasks by sharing knowledge among those tasks , has attracted much attention in recent years . Compared with single-task learning , it not only improves the performance but also reduces the training and inference time . Though MTL has demonstrated its usefulness in many applications , MTL can not guarantee to improve the performance of all the tasks when compared with single-task learning . As empirically observed in ( Lee et al. , 2016 ; Guo et al. , 2020 ; Sun et al. , 2020 ; Standley et al. , 2020 ) , when learning on multiple tasks together , each of some existing MTL models can achieve better performance on some of the tasks than a single-task model but perform worse on the other tasks . Such phenomenon is defined as the negative sharing phenomenon here , which is similar to the ‘ negative transfer ’ phenomenon ( Wang et al. , 2019 ) in transfer learning ( Yang et al. , 2020 ) but with some difference in the ways of knowledge transfer and sharing in those two learning paradigms as discussed later . One reason for the occurrence of negative sharing is that there are unrelated tasks among tasks in investigation , making jointly learning these tasks impair the performance of some tasks . To the best of our knowledge , there is little work to study the negative sharing problem for MTL . In this paper , we firstly give a formal definition for negative sharing occurred in MTL . Then we formally define an ideal and also basic situation of MTL , safe multi-task learning , where an MTL model performs no worse than its single-task counterpart on each task . Hence , safe multi-task learning means that there is no negative sharing occurred . According to the definition of MTL ( Caruana , 1997 ; Zhang & Yang , 2021 ) , we can see that every MTL model should achieve safe multi-task learning . Otherwise , single-task learning is more preferred than MTL , since an unsafe MTL model may bring the risk of worsening the generalization performance of some or even all the tasks . Moreover , we formally define η-partially safe multi-task learning as a loose version of safe multi-task learning to allow the MTL model to perform worse than its single-task counterpart on tasks with a proportion no larger than η . To achieve safe multi-task learning , we propose a Safe Multi-Task Learning ( SMTL ) model . Specifically , given m learning tasks , the SMTL model consists of a public encoder shared by all the tasks , m private encoders for the m tasks , m gates for the m tasks , and m private decoders for the m tasks . Hence each task has a private encoder , a gate , and a private decoder . The gate of each task is responsible of learning how to linearly combine the public encoder and the corresponding private encoder for the downstream private decoder . To reduce the storage cost during the inference stage , we propose a lite version of SMTL via the Gumbel-softmax trick ( Jang et al. , 2017 ; Maddison et al. , 2017 ) to enforce each gate to choose either the public encoder or private encoder . Moreover , to study the impact of different locations of the gates , we propose variants of the SMTL model , which place the gates after the decoders . Furthermore , we analyze the SMTL model from the perspectives of generalization bound and optimization . Experiments on several MTL benchmark datasets demonstrate the effectiveness of the proposed methods . The main contributions of this paper are summarized as follows . • We provide formal definitions for MTL , including negative sharing , safe multi-task learning , and η-partially safe multi-task learning . • To achieve safe multi-task learning , we propose a simple and effective SMTL model . Built on the SMTL model , we propose its variants . • We conduct extensive experiments to demonstrate the superiority of the proposed methods over state-of-the-art methods . 2 RELATED WORK . MTL has been extensively studied in recent years ( Evgeniou & Pontil , 2004 ; Zhang & Yeung , 2010 ; Kumar & Daume III , 2012 ; Zhang et al. , 2021 ; Guo et al. , 2021 ) . How to design a good network architecture for MTL is an important issue . The most popular model is the multi-head hard sharing architecture ( Caruana , 1997 ; Zhang et al. , 2014 ; Long & Wang , 2015 ; Liu et al. , 2015 ; Ruder et al. , 2019 ) , which shares the first several layers among all the tasks and allow the subsequent layers to be specific to different tasks . Then , to better handle task relationships , different MTL architectures have been proposed . For example , Misra et al . ( 2016 ) propose a cross-stitch network to learn to linearly combine hidden representations of different tasks . Liu et al . ( 2019 ) propose a Multi-Task Attention Network ( MTAN ) , which consists of a shared network and an attention module for each task so that both shared and task-specific feature representations can be learned via the attention mechanism . Gao et al . ( 2019 ) propose a Neural Discriminative Dimensionality Reduction ( NDDR ) layer to enable automatic feature fusing at every layer from different tasks . Sun et al . ( 2020 ) propose an Adaptive Sharing ( AdaShare ) method to learn the sharing pattern through a task-specific policy that selectively chooses which layers to be executed for each task . Guo et al . ( 2020 ) propose an algorithm to learn where to share or branch within a network for MTL . Cui et al . ( 2021 ) propose an Adaptive Feature Aggregation ( AFA ) layer , where a dynamic aggregation mechanism is designed to allow each task to adaptively determine the degree of the knowledge sharing between tasks . All the existing works do not study how to achieve safe multi-task learning , which is the focus of this paper . 3 SAFE MULTI-TASK LEARNING . In this section , we first formally define some terminologies for MTL . Then we introduce the proposed SMTL method . Moreover , we propose some variants of the SMTL model . Finally , we provide some analyses for SMTL . 3.1 DEFINITIONS . Definition 1 ( Negative Sharing ) . For an MTL model which is trained on multiple learning tasks jointly , if its generalization performance on some tasks is inferior to the generalization performance of the corresponding single-task model which is trained on each task separately , then negative sharing occurs . Remark 1 . Negative sharing occurs when some tasks are totally or partially irrelevant to other tasks . In this case , manually enforcing all the tasks to have some forms of sharing will impair the performance of some or even all the tasks . In Definition 1 , the MTL model and the singletask model usually have similar architectures as totally different architectures may bring additional confounding factors . Moreover , negative sharing is similar to negative transfer ( Wang et al. , 2019 ) in transfer learning ( Yang et al. , 2020 ) . However , knowledge transfer in transfer learning is directed as it is from a source domain to a target domain , while knowledge sharing in MTL is among all the tasks , making it undirected . From this perspective , negative sharing is different from negative transfer . Definition 2 ( Safe Multi-Task Learning ) . When no negative sharing occurs for an MTL model on a dataset , this MTL model is said to achieve safe multi-task learning on this dataset . The ideal situation for an MTL model is to achieve safe multi-task learning . However , not all the MTL methods can achieve safe multi-task learning and hence we define η-partially safe multi-task learning , which can be viewed as a loose version of safe multi-task learning . Definition 3 ( η-Partially Safe Multi-Task Learning ) . Given multiple learning tasks in a dataset , ηpartially safe multi-task learning ( 0 ≤ η ≤ 100 ) indicates that on about η percentage of tasks , the generalization performance of an MTL model is no worse than that of its single-task counterpart . When η equals 100 , η-partially safe multi-task learning becomes safe multi-task learning . When η is equal to 0 , the MTL model performs worse than its single-task counterpart in all the tasks . 3.2 SMTL . With m learning tasks { Ti } mi=1 , our goal is to design a model that can achieve safe multi-task learning . To achieve this goal , we propose the SMTL model , which is introduced in the following . Without loss of generality , we consider the case that different tasks share the input data or equivalently each data point has an output for each task . As shown in the left figure of Figure 1 , the SMTL model can be divided into four parts : a public encoder fS shared by all the tasks , m private encoders { ft } mt=1 for m tasks , m gates { gt } mt=1 for m tasks , and m private decoders { ht } mt=1 for m tasks . For task t , its model consists of the public encoder fS , the private encoder ft , the gate gt , and the private decoder ht , where fS and ft are combined by gt . Specifically , given a data point x , the gate gt in task t receives two inputs : fS ( x ) and ft ( x ) , and outputs gt ( fS ( x ) , ft ( x ) ) , which is fed into ht to obtain the final prediction ht ( gt ( fS ( x ) , ft ( x ) ) ) , which is used to define a loss for x . Here the gate gt is to determine the contributions of fS and ft . Ideally , when task t is unrelated to other tasks , gt should choose ft only . On another extreme where all the tasks have the same data distribution , all the tasks should use the same model and hence gt should choose fS only . On cases between the two extremes , gt can combine fS and ft in proportion . To achieve the aforementioned effects , we use a simple convex combination function for gt as gt ( fS ( x ) , ft ( x ) ) = αtfS ( x ) + ( 1− αt ) ft ( x ) , ( 1 ) where αt ∈ [ 0 , 1 ] defines the weight of fS ( x ) and is a learnable parameter . When αt equals 0 , only the private encoder ft will be used , which corresponds to the unrelated case . When αt is equal to 1 , only the public encoder fS will be used , which is corresponding to the case that all the tasks follow the same distribution . When αt is between 0 and 1 , fS and ft are combined with proportions αt and 1− αt , respectively , where αt can be adaptively learned to minimize the training loss on task t. Thus , the entire objective function of SMTL is formulated as min Θ∈C 1 mn n∑ i=1 m∑ t=1 Lt ( yit , ht ( gt ( fS ( xi ) , ft ( xi ) ) ) ) , ( 2 ) where xi denotes the ith data point , yit denotes the label of xi in task t , n denotes the total number of data points in the training dataset , Θ includes all the parameters in fS , { ft } mt=1 , { gt } mt=1 , and { ht } mt=1 , C = { Θ|0 ≤ αt ≤ 1 , ∀t } denotes the feasible set for Θ , and Lt denotes the loss function for task t ( e.g. , the pixel-wise cross-entropy loss for semantic segmentation , L1 loss for depth estimation , and element-wise dot product loss for surface normal prediction ) . To see why the proposed SMTL model could achieve safe multi-task learning , we compare SMTL and the corresponding Single-Task Learning ( STL ) model which consists of a private encoder ft and a private decoder ht . It is easy to see that SMTL can reduce to the STL model for some or even all the tasks by making the gates of those tasks choose the corresponding private encoders ( i.e. , setting αt ’ s of those tasks to 0 ) . So if a task is unrelated to other tasks , the SMTL model can use the gating mechanism to separate this outlier task from other tasks , which may help achieve safe multi-task learning . Moreover , we can show that the training loss of the SMTL model is no larger than the average of that of the STL model on each task . To see that , it is easy to show that the STL model for task t can be represented as ht ( g0t ( ∅ , ft ( x ) ) ) , where g0t denotes the gate of task t with αt as 0 and ∅ denotes a null network . As g0t is a feasible gate for SMTL , after sufficient training , we probably have min Θ∈C 1 mn n∑ i=1 m∑ t=1 Lt ( yit , ht ( gt ( fS ( xi ) , ft ( xi ) ) ) ) ≤ min Θ′ 1 mn n∑ i=1 m∑ t=1 Lt ( yit , ht ( g0t ( ∅ , ft ( xi ) ) ) ) , where Θ′ includes parameters in { ft } mt=1 and { ht } mt=1 in STL models for all the tasks . This inequality shows the benefit of the proposed SMTL model . Even though the training loss is not a tight estimation of the generalization loss , we think that it is an indicator to reflect the generalization performance of the two models . In Section 4.4 , we study to use a bi-level formulation to learn the gates as hyperparameters on a validation set and its performance is comparable with the SMTL model based on problem ( 2 ) , which proves the usefulness of the indication of the training loss . Similarly , we can show that the training loss of the SMTL model is no larger than that of the DMTL model ( a.k.a . the multi-head hard sharing network ) which consists of a shared encoder by all the tasks and private decoders for m tasks . To see this , the DMTL model for task t can be represented as ht ( g1t ( fS ( x ) , ∅ ) ) , where g1t denotes the gate of task t with αt as 1 . As g1t is a feasible gate for SMTL , it is easy to get that the training loss of the SMTL model after sufficient training is lower than that of the DTML model , which could be one reason for the phenomenon that SMTL outperforms DMTL in our experiments .","In recent years, Multi-Task Learning (MTL) attracts much attention due to its good performance in many applications. However, many existing MTL models cannot guarantee that its performance is no worse than its single-task counterpart on each task. Though this phenomenon has been empirically observed by some works, little work aims to handle the resulting problem, which is formally defined as negative sharing in this paper. To achieve safe multi-task learning where no negative sharing occurs, we propose a Safe Multi-Task Learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder, where the gate is to learn how to combine the private encoder and public encoder for the downstream private decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed to allow the gate to choose either the public encoder or the corresponding private encoder. Moreover, we propose a variant of SMTL to place all the gates after decoders of all the tasks. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods.","In this paper, we propose a new method for multi-task learning called Safe Multi-Task Learning (SMTL) to avoid negative sharing, which occurs when a model performs worse on one task than a single-task model trained on that task. SMTL uses a gating mechanism to share knowledge among tasks while avoiding negative sharing, and is shown to outperform existing multi-task learning methods in experiments.","The article introduces the concept of Negative Sharing in Multi Task Learning (MTL) and proposes a Safe Multi-Task Learning (SMTL) model to achieve safe multi-task learning, which aims to improve the generalization performance of multiple learning tasks by sharing knowledge among those tasks. Several variants of the proposed SMTL model are also proposed, and extensive experiments are conducted to demonstrate the superiority of the proposed methods over state-of-the-art methods.","The paper propose a simple method for safe multi-task learning where there is no negative transfer or ""negative sharing"" among tasks. It jointly trains shared encoder, task-specific (private) encoders, gate, and decoder. The gate computes importance for each output of shared and task specific encoder and combine the outputs with simple convex combination. ","2,1,0",,,,,,,,,,,,,,,,,
16,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"1 INTRODUCTION . Learning disentangled representations is important for a variety of tasks , including adversarial robustness , generalization to novel tasks , and interpretability ( Stutz et al. , 2019 ; Alemi et al. , 2017 ; Ridgeway , 2016 ; Bengio et al. , 2013 ) . Recently , deep generative models have shown marked improvement in disentanglement across an increasing number of datasets and a variety of training objectives ( Chen et al. , 2016 ; Lin et al. , 2020 ; Higgins et al. , 2017 ; Kim and Mnih , 2018 ; Chen et al. , 2018b ; Burgess et al. , 2018 ; Karras et al. , 2019 ) . Nevertheless , quantifying the extent of this disentanglement has remained challenging and inconsistent . As a result , evaluation has often resorted to qualitative inspection for comparisons between models . Existing evaluation metrics are rigid : while some rely on training additional ad-hoc models that depend on the generative model , such as a classifier , regressor , or an encoder ( Eastwood and Williams , 2018 ; Kim and Mnih , 2018 ; Higgins et al. , 2017 ; Chen et al. , 2018b ; Glorot et al. , 2011 ; Grathwohl and Wilson , 2016 ; Karaletsos et al. , 2015 ; Duan et al. , 2020 ) , others are tuned for a particular dataset ( Karras et al. , 2019 ) . These both pose problems to the evaluation metric ’ s reliability , its relevance to different models and tasks , and consequently , its applicable scope . Specifically , reliance on training and tuning external models presents a tendency to be sensitive to additional hyperparameters and introduces partiality for models with particular training objectives , e.g . variational methods ( Chen et al. , 2018b ; Kim and Mnih , 2018 ; Higgins et al. , 2017 ; Burgess et al. , 2018 ) or adversarial methods with an encoder head on the discriminator ( Chen et al. , 2016 ; Lin et al. , 2020 ) . In fact , this reliance may provide an explanation for the frequent fluctuation in model rankings when new evaluation metrics are introduced ( Kim and Mnih , 2018 ; Lin et al. , 2020 ; Chen et al. , 2016 ) . Meanwhile , dataset-specific preprocessing , such as automatically removing background portions from generated portrait images ( Karras et al. , 2019 ) , generally limits the scope of the evaluation metric ’ s applicability because it depends on the preprocessing procedure and may otherwise be unreliable . To address this , we introduce an unsupervised disentanglement evaluation metric that can be applied across different model architectures and datasets without training an ad-hoc model for evaluation or introducing a dataset-specific preprocessing step . We achieve this by using topology , the mathematical discipline which differentiates between shapes based on gross features such as holes , loops , etc. , alongside density analysis of samples . The combination of these two ideas are the basis for functional persistence , which is one of the areas of application of persistent homology ( Cayton , 2005 ; Narayanan and Mitter , 2010 ; Goodfellow et al. , 2016 ) . In discussing topology , we walk a fine line between perfect mathematical rigor on the one hand and concreteness for a more general audience on the other . We hope we have found the right level for the machine learning community . Our method investigates the topology of these low-density regions ( holes ) by estimating homology , a topological invariant that characterizes the distribution of holes on a manifold . We first condition the manifold on each latent dimension and subsequently measure the persistent homology of these conditional submanifolds . By comparing persistent homology , we examine the degree to which conditional submanifolds continuously deform into each other . This provides a notion of topological similarity that is higher across submanifolds conditioned on disentangled dimensions than those conditioned on entangled ones . From this , we construct our evaluation metric using the aggregate topological similarity across data submanifolds conditioned on every latent dimension in the generative model . In this paper , we make several key contributions : • We present an unsupervised metric for evaluating disentanglement that only requires the generative model ( decoder ) and is dataset-agnostic . In order to achieve this , we propose measuring the topology of the learned data manifold with respect to its latent dimensions . Our approach measures the topological dissimilarity measure across latent dimensions , and permits the clustering of submanifolds based on topological similarity . • We also introduce a supervised variant that compares the generated topology to a real reference . • For both variants , we develop a topological similarity criterion based on Wasserstein distance , which defines a metric on barcode space in persistent homology ( Carlsson , 2019 ) . • Empirically , we perform an extensive set of experiments to demonstrate the applicability of our method across 10 models and three datasets using both the supervised and unsupervised variants . We find that our results are consistent with several existing methods . 2 BACKGROUND . Our method draws inspiration from the Manifold Hypothesis ( Cayton , 2005 ; Narayanan and Mitter , 2010 ; Goodfellow et al. , 2016 ) , which posits that there exists a low-dimensional manifoldMdata on which real data lie and pdata ( x ) is supported , and that generative models g : Z → X learn an approximation of that manifold Mmodel . As a result , the true data manifold Mdata contains high-density regions , separated by large expanses of low-density regions.Mmodel approximates the topology ofMdata , and superlevel sets of density withinMdata , through the learning process . A k-manifold is a space X , for example a subset of Rn for some n , which locally looks like an open set in Rk ( formally , for every point x ∈ X , there is a subset can be reparametrized to an open disc in Rk ) . A coordinate chart for the manifold X is an open subset U of Rk together with a continuous parametrization g : U → X of a subset of X . An atlas for X is a collection of coordinate charts that cover X . For example , any open hemisphere in a sphere is a coordinate chart , and the collection of all open hemispheres form an atlas . We say two manifolds are homeomorphic if there is a continuous map from X to Y that has a continuous inverse . Intuitively , two manifolds are homeomorphic if one can be viewed as a continuous reparametrization of the other . If we have a continuous map f from a manifold X to Rn , and are given two nearby points ~x and ~y in Rn , it is often useful to compare the subsets f−1 ( ~x ) and f−1 ( ~y ) , which are manifolds ( where the Jacobian matrix of f is maximal rank ) . They are frequently homeomorphic , and we will be using topological invariants that can distinguish between two non-homeomorphic manifolds . Among the easiest topological invariants to numerically estimate is homology ( Hatcher , 2005 ) , which characterizes the number of k-dimensional holes in a topological space such as a manifold . Intuitively , these holes correspond to low-density regions on the manifold . The field of persistent homology offers several methods for estimating the homology of a topological space from data samples ( Carlsson , 2019 ) . Recent work on Relative Living Times ( RLTs ) ( Khrulkov and Oseledets , 2018 ) has applied persistent homology to generative model data manifolds , also enabling direct comparison of generated data manifolds to real ones . For low-dimensional data ( and images ) , points correspond to their vector representation ( flattened matrices of pixels ) ; for high-dimensional data , particularly images , points correspond to vectorized embeddings from a pretrained VGG16 ( Simonyan and Zisserman , 2015 ) . To obtain RLTs , we first construct a family of simplicial complexes–graph-like structures–from data samples , each starting with a set of vertices representing the data points and no edges ( Figure 2 ) . These are witness complexes that characterize the topology of a set of points , a common method for statistically estimating topological invariants in persistent homology ( Carlsson , 2019 ; Lim et al. , 2020 ) . These simplicial complexes approximate the persistent homology of the data manifold by identifying k-dimensional holes present in the simplices at varying levels of proximity . Proximity is defined with a dissimilarity metric ( Euclidean distance ) between points . It is used to build a simplicial complex in which a collection of points spans a simplex if all points have proximity measure less than some threshold , as shown in Figure 2d . Varying the threshold gives persistent homology . As proximity increases , simplices are added , creating varying numbers of k-dimensional holes , which gives rise to persistence barcodes ( Carlsson , 2019 ; Zomorodian and Carlsson , 2005 ; Ghrist , 2008 ) . RLTs are vectorizations of these persistence barcodes , specifically the discrete distributions over the duration of each k-dimensional hole as it appears and disappears , or their lifetime relative to other holes . This is merely one method to ( partially ) vectorize persistence barcodes efficiently , and we leave it to future work to explore alternate methods ( Adcock et al. , 2013 ; Bubenik , 2015 ) . To measure the topological similarity between data samples representing two generative model manifolds , Khrulkov and Oseledets ( 2018 ) then take the Euclidean mean of several RLTs to produce a discrete probability distribution , called a Mean Relative Living Time ; they propose employing the Euclidean distance between two Mean Relative Living Times as the measure of topological similarity between two sets of data samples , known as the Geometry Score . Additional background is in Appendix B . 3 MANIFOLD INTERPRETATION OF DISENTANGLEMENT . We use prevailing definitions of disentanglement where a disentangled model has a factorized latent space corresponding bijectively to factors of variation ( Shu et al. , 2019 ; Higgins et al. , 2018 ; Duan et al. , 2020 ) . We frame our approach primarily on the definition in Shu et al . ( 2019 ) , with a more formal connection between their approach and our method in Appendix A . From the manifold perspective , disentanglement is an extrinsic property that is dependent on the generative model ’ s atlas . Consider a disentangled generative model g with manifoldM that assumes topology τ . We can define another generative model g′ with the same underlying manifoldM and τ , but it is entangled and has a different atlas . In fact , we can define several alternate disentangled and entangled atlases , provided there are multiple valid factorizations of the space . As a result , we need a method that can detect whether an atlas is disentangled . In this paper , we sliceM into submanifolds Usi=v ⊂M that are conditioned on a factor si at value v. These conditional submanifolds may have different homology from their supermanifoldM . If we observe samples from one factor , e.g . Xs1=v ∼ Us1=v at varying values of v , we find that all samples Xs1=v appear identical , except with respect to that single factor of variation si set to a different value of v. For a generative model , the correspondence between latent dimensions zj and factors si is not known upfront . As a result , we perform this procedure by conditioning on each latent dimension zj . Conditional submanifold topology . For two submanifolds to have the same topology , there needs to be a continuous and invertible mapping between them . First , assume that there exists an invertible mapping , or encoder e : X → Z , and a generative model g : Z → X , where both functions are continuous . Then , for a given z and x = g ( z ) , we can recover z by the composition z = e ( g ( z ) ) . We can also construct a simple linear mapping l : z→ z ’ , which adapts a factor ’ s value , such that z ’ = l ( e ( g ( z ) ) ) remains continuously deformable . This holds across factors , where the manifold is topologically symmetric with respect to different factors , i.e . its conditional submanifolds are homeomorphic . As an example , consider a disentangled generative model g ( z0 , z1 , z2 ) that traces a tri-axial ellipsoid x 2 z20 + y 2 z21 + z 2 z22 = 1 . If we condition the model on varying values of each factor , the resulting submanifolds are ellipses and have the same topology . Most complex manifolds have submanifolds that have non-homeomorphic factors of variation . For example , consider a generative model g ( z0 , z1 ) that traces a cylindrical shell with angle z0 , height z1 , and for simplicity , no thickness . The submanifolds conditioned on angle z0 form lines ( no holes ) , while the submanifolds conditioned on height z1 form circles ( a 1D hole ) . However , the topology remains the same for a given factor . A visualization of this principle on a cone is shown in Figure 3 . Taken together , this means that submanifolds within a factor ( intra-factor ) are homeomorphic , while submanifolds between factors ( extra-factor ) can be either homeomorphic or non-homeomorphic . Topological asymmetry . Because topologically asymmetric submanifolds are non-homeomorphic , using a single e that continuously deforms across submanifolds no longer holds under disentanglement . To address this , assume that for each factor j , there exists a continuous invertible encoder ej : X → Zj that exclusively encodes information on j from a generated sample . In the cylindrical shell example , this means continuously deforming across submanifolds conditioned on varying values of z0 using e0 ( deforming between lines ) and likewise for z1 using an e1 ( deforming between circles ) . Note that this formulation prevents continuous deformations between lines and circles . More generally , we can not continuously deform across submanifolds conditioned by arbitrary factors and expect the topology to be preserved . This procedure now amounts to performing latent traversals along an axis and observing the topology of the resulting submanifolds . In a disentangled model , the j-conditional Algorithm 1 : Procedure for producing W. RLTs on generated images : For generator g with latent prior P and N samples , Dz latent dimensions , and nd samples per latent dimension , returns a W. RLT for each dimension . RLT is the RLT procedure from Khrulkov and Oseledets ( 2018 ) . for i in 1 : N do Sample z ( i ) ∼ P { note : z ∈ RN×Dz } end for for latent dimension d in 1 : Dz do for k in 1 : nd do z′ ← copy ( z ) Set z′d ← k ∼ Pd Compute ez ← embedding ( g ( z′d ) ) { e.g . using VGG16 } Compute rlt [ d , k ] ← RLT ( ez , γ = 1/128 , L0 = 64 , n = 100 ) end for Compute WB [ d ] ←W.Barycenter ( rlt [ d , k ] ) end for return WB submanifolds exhibit the same topology by continuous composition of z = ej ( g ( z ) ) , using a linear mapping that only adapts factor j across the traversal , i.e . z′ = lj ( ej ( g ( z ) ) ) . In an entangled model by contrast , more than one factor—such as both the angle and height in the cylindrical shell example—exhibit variation along a dimension zj . Put another way , the topology on submanifolds conditioned on zj changes when multiple factors contribute to variation along this dimension . Concretely , following the cylindrical shell example , a dimension that encodes height and , after a certain height threshold , also begins to adapt the angle will result in a topology that changes to include a 2D hole . Consequently , submanifolds conditioned on the same latent dimension zj have the same topology in a disentangled model , yet different topology in an entangled one . Because we can not assume that the data manifold of a generative model is completely symmetric , we only consider submanifolds to be homeomorphic along the same factor in a disentangled model . By contrast , since these submanifolds are not homeomorphic in an entangled model , we can measure the similarity across submanifolds to evaluate a model ’ s disentanglement . Using this notion of intra-factor topological similarity , we may sufficiently measure disentanglement in most cases , but it does not shield us from the scenario where a generative model learns a single trivial factor along all dimensions , i.e . a factorization of one . If we assume that there exists assymmetries in the data manifold , then ensuring that the manifold exhibits topological dissimilarity between certain factors would disarm that case . We operationalize this by identifying homeomorphic clusters of factors , whereby each cluster has a distinct topology to ensure there is not a factorization of one . Within clusters , we measure topological similarity , but between clusters , we calculate topological dissimilarity . Consequently , topological similarity and dissimilarity form the basis of our evaluation metric . A more principled treatment of how manifold topology measures disentanglement is in Appendix A .","Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make our code publicly available at https://github.com/stanfordmlgroup/disentanglement.","The article proposes a new evaluation metric for disentanglement in generative models, which is based on the concept of persistent homology. The metric measures the topological similarity between submanifolds conditioned on different latent dimensions in the generative model, and can be used to evaluate the disentanglement of different models and datasets. The proposed metric provides a more reliable and robust way of evaluating disentanglement compared to existing methods, and can be applied across different model architectures and datasets without requiring additional training or hyperparameter tuning.","The paper proposes a novel metric for evaluating disentanglement by taking a manifold-topological perspective on the representations learnt. The key insight is that for a disentangled representation, when we fix a certain factor of variation at different values the topology of the conditional sub-manifolds should be similar. Using this insight the paper proposes a metric for disentangling which does not require annotations of the factors of variation and is more general than previous such tests.","Researchers have proposed an unsupervised method for evaluating the level of disentanglement in deep generative models. This evaluation metric uses topology and density analysis of samples to measure the degree of topological similarity across submanifolds conditioned on disentangled latent dimensions versus entangled ones. The approach shows consistent results across 10 models and three datasets, without relying on training additional models or introducing a dataset-specific preprocessing step.","2,0,1",1,0,"2,0",1,,coverage,concise,coverage,coverage,2,1,"0,1",2,Summary C presents more details on the experiments.,Summary B is just slightly worse because it is more terse.,good coverage and details,not much detail about results
17,SP:4e79b326bbda5d1509e88869dde9886764366d41,"1 INTRODUCTION . Localization of a multimedia work , such as a video game or a motion picture , consists in making changes and adaptations to the original work so that international distributors can market the resulting product in a target country . For this purpose , companies need to translate their works into a target language . Various options are available to them to give substance to their translation . Subtitling is the simplest and cheapest option , but not the most practical one for spectators : a large audience is more comfortable with hearing speech , in general in their mother tongue , rather than reading subtitles while hearing speech in another language . Dubbing , which is more expensive and takes longer to set up , better retains the audience ’ s immersion by replacing the original voices with dubbing comedian voices . It involves a voice selection process , which means choosing among several candidate voices in a target language . This selection is referred to as voice casting . With the emergence of streaming platforms ( e.g . Netflix , Disney+ , Amazon Prime Video ) and the rapid development of the video game industry , the number of works to be localized is increasing dramatically . As a result , more and more actors are available on the voice market . Vocal Casting is an artisanal process that requires a lot of precision and is carried out by human operators . As it is , it can not be applied to a very large number of actors . Operators very often rely on their memory , which is limited to a small number of actors with whom they have worked and in whom they have confidence . However , this operation can degrade the immersion of the audience since the most oftused voices become easily recognizable . They then become references and create cognitive biases in the public ’ s perception of the character . Automatic actor search and recommendation tools , based on voice processing , would help operators find new actors who would then enrich the vocal diversity of the works to the delight of the audience . In this article , we wish to address some of the complexity of voice casting and we start with the voices of professional actors playing characters for the gaming industry . The articles Gresse et al . ( 2020 ; 2019 ; 2017 ) ; Malik & Nguyen ( 2021 ) ; Obin & Roebel ( 2016 ) ; Obin et al . ( 2014 ) ; Quillot et al . ( 2021b ; a ) cited in this paper are the only ones dealing with the speech-processing-based automation of Voice Casting in the literature to our knowledge . We rely on Gresse et al . ( 2020 ) , a recent work which defines the p-vector , a neural-network-based representation of the voice dedicated to the character aspects in acted voices . The application context in Gresse et al . ( 2020 ) ( like in related previous works Gresse et al . ( 2017 ; 2019 ) ) is voice casting for audio dubbing . The final objective of this work is to assess how close an actor – acting in a target language – is to the voice of a given character , speaking in a source language . This can be referred to as character-based voice similarity . In contrast to Obin et al . ( 2014 ) ; Obin & Roebel ( 2016 ) , which proposed a voice similarity system for the acted voice based on data labeled with speech classes ( age , gender , emotions ... ) by an expert , the p-vector approach does not use any expert labels . This representation is learnt from audio associated with the played character label , without any additional information . Also , the authors of Quillot et al . ( 2021a ) recently posed the following hypothesis : when a character is played , the actor adds to the voice some information that characterizes the character and is independent of the speaker identity . They then introduced SICI ( Speaker-Independent Character Information ) and showed that there is speaker-independent character information in the acted voice and that it is present in p-vectors pretrained on a speaker task Quillot et al . ( 2021b ) . Although these approaches deal with final works ( video games ) , another one consists in working directly on the decision data from Artistic Directors . Recently , researchers from Warner Bros. evaluated their models on this kind of data . Unfortunately , these data are sensitive and their acquisition is not trivial since it requires to work on the critical voice casting process . This is why we decided to position ourselves in a task quite different from Malik & Nguyen ( 2021 ) , since we do not use Artistic Director decisions . Voice similarity systems alone are not enough for Voice Casting operators . They do not trust the computed recommendations and need a more complete interface to interact with the system . Many works propose explainability approaches Došilović et al . ( 2018 ) but these are too complicated to be understood by a user who is not expert in computer science , and more particularly in machine learning . We strongly believe that the introduction of explanatory labels will help the operators understand the decisions by giving , for example , the list of speech characteristics shared by two recordings/actors . Such labels could also be used to express queries in voice search systems . Unfortunately , voice casting is an underdefined task Bonastre ( 2019 ) . The operators themselves can not formally define their decision criteria . There is also no standard taxonomy in cinema to describe voices , making it impossible to create an annotation protocol . The question is then : how can we extract features from voices when we have no a priori knowledge of them ? Different representation learning approaches exist in the literature , but none of them corresponds to our expectations . 1 ) Feature extraction surveyed in Bengio et al . ( 2014 ) consists in representing an observation by a vector of reals but not in the form of binary vectors that we call characteristics in this paper . 2 ) Supervised methods like Hu et al . ( 2017 ) require labels that we do not have access to in order to extract characteristics . 3 ) Unsupervised methods like Peri et al . ( 2020 ) propose generic representations from which we could not extract characteristics dedicated to the domain of acted voice and character identity . Hence , to address the lack of method to extract characteristics without ground truth , this article proposes a totally new approach entitled Label Refining . It stats with training a representation extractor using non-expert initial labels . A clustering algorithm is then employed with the purpose to emphasizing characteristics shared by groups of observations which we term refined labels . A last step consists in extracting a new representation from refined labels and comparing it with the initial representation by evaluating it on a downstream task . Unlike generic representations that use no label , this method has the advantage of guiding the extracted representations by using the non-expert initial labels . To validate this method , we apply it to dubbing and more specifically to the extraction of characteristics from actors ’ voices . We therefore p-vector features from characters played in a video game . We apply a clustering algorithm , k-means , in the p-vector space to extract refined labels . In order to evaluate the quality of the refined labels , we train a neural network to extract refined p-vectors . We then subject them to a voice character similarity task to compare them with the initial p-vectors and thus measure the loss or gain of dedicated character information . In this experiment , we test the following hypothesis : during the clustering step , using the recordings of the training corpus only highlights the initial labels , the characters . During this step , a second corpus ( close to the domain ) is needed to highlight the vocal features extracted by the p-vectors . Since distance plays an essen- tial role in the k-means algorithm , we also explore the use of cosine and Mahalanobis distances to compare with Euclidean one . Section 2 presents Label Refining . We define the term “ refined label ” and present the method . The experimental protocol to apply Label Refining is described on Section 3 . In Section 4 , we present experiments and discuss about the obtained results . For reproducibility reasons , scripts and models are available on GitHub1 . Finally , we conclude by presenting some takeaways and possible directions for future work in Section 5 . 2 LABEL REFINING . Before experimenting , we first introduce the Label Refining method . It consists in extracting refined labels from initial ones . In this section , we give a definition of refined labels and we detail the method of extraction and evaluation . 2.1 THE DEFINITION OF REFINED LABELS . In this work , we assume that labels are decomposable into various lower-level sublabels . In our case , we wish to use the knowledge provided by the played character labels associated to the recordings with the aim to infer the characteristics such that the emotion played ( e.g . agressive , happy ) , a voice color ( e.g . white , red ) or a timbre ( e.g . husky , enlightened ) . Equation 1 formalizes the definition of what a refined label is . Let R the set of all the refined labels and a ( l ) i ∈ { r1 , r2 ... r|R| } the ith refined label associated to the initial label l. Let us also specify that the number n of refined labels is different depending on l. Thus , from the knowledge given by initial labels in L we wish to find the refined labels a ( l ) i of each l. l a ( l ) 1 a ( l ) 2 ... a ( l ) n ( 1 ) Let us note the functions f : e 7→ l and g : e 7→ r where o is an observation ( e.g . vocal recording ) . The equation 2 gives fictives examples . It shows the expected images of the functions f and g according to two initial labels l1 and l2 . In our model of the problem , we assume that a refined label r may appear in observations associated to different initial labels . Here , this is the case with r1 which appears both in an observation associated to l1 and an observation associated to l2 . l1 { f : o ( l1 ) 1 7→ l1 f : o ( l1 ) 2 7→ l1 , l1 { g : o ( l1 ) 1 7→ r1 g : o ( l1 ) 2 7→ r2 , l2 { f : o ( l2 ) 1 7→ l2 f : o ( l2 ) 2 7→ l2 , l2 { g : o ( l2 ) 1 7→ r1 g : o ( l2 ) 2 7→ r3 ( 2 ) 2.2 OUR PROPOSITION TO REFINE LABELS . Figure 1 presents the refined label extraction method that we propose in three steps . 1 consists in training a system to extract a representation ( i.e . vector , hash Roussev ( 2011 ) ) of the observation o ∈ O using the initial labels l ∈ L ( e.g . label of the character played in a recording ) . 2 consists in using a clustering algorithm in order to associate new labels r ∈ R to the observations O . 3 consists finally in training an new system to extract refined labels by using labels r. In order to evaluate the relevance of labels r , we use two methods . The first method , taking place at step 2 , consists in applying different clustering quality measures such as v-measure Rosenberg & Hirschberg ( 2007 ) or the purity Ajmera et al . ( 2002 ) . Applied to the step 1 and step 3 , the second method consists in extracting a representation of the observations O and evaluating it on 1To keep the identity of the authors secret for the reviewers , the GitHub URL will be added if the paper is accepted . a downstream task . The downstream task can be diverse : speaker recognition , speech recognition or , in our case , character voice similarity . Thanks to the second method , it is possible to highlight the gain or loss of information by comparing the results obtained at steps 1 and 3 . In this work , we postulate that the refined labels are pertinent if they do not lead to a loss of information when extracting refined representation . Depending on the clustering algorithm , it is likely to have to try different values of k. In the following experiments , we use k-means and test different values of k. In this context , we apply the method for all the k values and compare the obtained results . We postulate that Label Refining is validated if the number of refined labels k differs from the number of characters and if the character information is at least preserved in the refined representation .","A characteristic is a distinctive trait shared by a group of observations which may be used to identify them. In the context of voice casting for audiovisual productions, characteristic extraction has an important role since it can help explaining the decisions of a voice recommendation system, or give modalities to the user with the aim to express a voice search request. Unfortunately, the lack of standard taxonomy to describe comedian voices prevents the implementation of an annotation protocol. To address this problem, we propose a new semi-supervised learning method entitled Label Refining that consists in extracting refined labels (e.g. vocal characteristics) from known initial labels (e.g. character played in a recording). Our proposed method first suggests using a representation extractor based on the initial labels, then computing refined labels using a clustering algorithm to finally train a refined representation extractor. The method is validated by applying Label Refining on recordings from the MassEffect 3 video game. Experiments show that, using a subsidiary corpus, it is possible to bring out interesting voice characteristics without any a priori knowledge.","The article discusses the problem of localizing multimedia works, such as video games and motion pictures, for international distribution by making changes and adaptations to the original work. The authors propose a new approach called Label Refining, which involves training a representation extractor using non-expert initial labels and then using a clustering algorithm to extract refined labels that highlight the characteristics shared by groups of observations. The method is evaluated on a downstream task, such as voice character similarity, to compare the results obtained with the initial labels and assess the gain or loss of information in the refined labels.","The paper presents a ""label-refining"" technique, which helps users pick voices to provide a better user experience in dubbed video games. The idea is that a voice talent's voice in the target language should match the character's voice characteristics in the source language. The method seems to work by attaching labels to data-driven clusters, and refining these using a second corpus, on which the labels' value for discrimination is measured.","The article discusses the process of voice casting for multimedia works like video games and motion pictures in an international context. The authors propose a novel approach called Label Refining to extract characteristics from actors' voices without ground truth. They apply this method to extract refined labels in a video game corpus using p-vector features from the characters played, with the aim of highlighting the vocal features extracted by the p-vectors.","2,0,1",2,0,1,"2,0",,coverage,other,coverage,informative,0,1,2,"0,1",Summary A has better coverage.,Summary B restricts the applications to video games.,good coverage and balance of details,too general
18,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"1 INTRODUCTION . Planning algorithms typically leverage the environment dynamics to solve decision-making problems ( Sutton & Barto , 2018 ) . To plan in unknown environments , the agent must learn a dynamics model to predict future ( belief ) states and rewards by conditioning on an action . This dynamics model enables the implementation of intensive search for optimal actions , which can potentially increase both sample efficiency and cumulative rewards , compared to model-free methods ( Hafner et al. , 2019 ; Wang et al. , 2019 ; Kaiser et al. , 2020 ) . Despite the promising performance , learning a dynamics model that can accurately generalize at test time is still challenging , especially when handling a high-dimensional state space for low-level features , e.g. , pixels and text . To facilitate dynamics learning in complex environments , Diuk et al . ( 2008 ) proposed an ObjectOriented Markov Decision Process ( OO-MDP ) that factorizes world states into object states . They showed that the agent can find optimal policies with a better sample efficiency by modeling the dynamics at the object level . Some following works ( Finn et al. , 2016 ; Goel et al. , 2018 ; Zhu et al. , 2018 ; 2020 ) extended OO-MDPs to image-based games . These methods typically assume full observability over game states and a fixed input size , which facilitates the use of object masks to decompose an image into different objects . On the other hand , in Text-Based Games ( TBGs ) , the text observation , whose length runs from a few words to an entire paragraph , is a partial description of the current game state , and thus each observation provides information about a limited number of objects ( in fact , only an average of 4.51 % of the candidate objects are mentioned in an observation from Textworld ( Côté et al. , 2018 ) ) . The dynamics model must remember the history of observed objects to predict accurately their states . Moreover , an observation typically contains lots of noisy patterns that record object-irrelevant information ( e.g. , the non-bolded text from ot in Figure 1 ) . While previous works ( Ammanabrolu & Riedl , 2019 ; Ammanabrolu & Hausknecht , 2020 ) designed rule-based heuristics to extract useful information , we expect that dynamically capturing object information from noisy observations helps to generalize to more environments . In this work , we design an Object-Oriented Text Dynamics ( OOTD ) model that integrates : 1 ) Graph Representation for Objects . OOTD predicts a memory graph from the input states . The memory graph , whose nodes correspond to the objects to be modelled , captures the information about these objects from the beginning of a game . By applying this graph as an information bottleneck for dynamics prediction , OOTD filters the object-irrelevant information in the text inputs , which forms a tighter information plane and facilitates generalization to unseen games ( Tishby & Zaslavsky , 2015 ) . 2 ) Independent Transition Layers . Intuitively , an input action influences the states of a limited number of objects , for example , when the agent performs `` take the carrot from the counter '' , only the states of `` carrot '' and `` counter '' should be updated . To capture which objects are influenced and keep others invariant to this update , OOTD learns an action-aware representation for each object with bi-directional attention and predicts the belief states of objects with independently parameterized transition layers . This independent mechanism enhances the robustness for dynamics prediction ( Goyal et al. , 2019 ) ( also see the ablation study in Section 4.2 ) . During planning , we predict belief states without knowing rewards and observations . To learn a stochastic dynamics model , we propose object-supervised and self-supervised Evidence Lower Bound ( ELBo ) objectives for training our OOTD model . We evaluate how well the OOTD model supports planning by implementing planning algorithms ( e.g. , Dyna-Q ( Kuvayev & Sutton , 1996 ) and Monte Carlo Tree Search ( MCTS ) ( Kocsis & Szepesvári , 2006 ) ) based on learned dynamics . Empirical results show that these planning algorithms significantly outperform other baselines in terms of game performance and sample efficiency . To support the design of our OOTD model , we include an ablation study that visualizes the object states and quantifies the dynamics prediction performance . Contributions : 1 ) We introduce our approach to implementing model-based planning algorithms to solve decision-making problems in the text-domain . 2 ) We propose the OOTD model that learns to predict beliefs about object dynamics . 2 OBJECT LEVEL PLANNING IN TEXT DOMAINS . In this work , the object-level dynamics are learned and evaluated in Text-Based Games ( TBGs ) , for which we extend OO-MDPs ( Diuk et al. , 2008 ) to capture partial observability . We define a graph representation to remember object information and introduce the corresponding planning algorithms . 2.1 TEXT-BASED GAMES . Text-Based Games ( TBGs ) are complex , interactive simulations where an agent explores and plays a game by processing text observations and responding with text commands . To study the generalization ability of this policy , many recent works ( Yin & May , 2019 ; Hausknecht et al. , 2019 ; Ecoffet et al. , 2019 ; Adolphs & Hofmann , 2020 ; Adhikari et al. , 2020 ) proposed to learn a policy by training with a set of games and evaluating with games from a hold-out test set . The distributions of rewards over state-action pairs are different in training and testing games because TBGs assign rewards by conditioning on specific goals ( that are predefined , but unknown ) . In this setting , TBGs have different intermediate goals in each game , but they commonly share the same ultimate goals , and the underlying causal dependencies leading to this ultimate goal are consistent . For example , to solve the ‘ First Text-World Problems ’ ( Côté et al. , 2018 ) , an agent should always gather and process cooking ingredients ( e.g. , `` fry the potato '' ) according to a recipe that it discovers in the game , although the names and locations of the ingredients are different across different games . This property enables the generalization of knowledge learned from training games to solve testing games . 2.2 OBJECT-ORIENTED PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES . Since an observation reveals only partial information about the game state , we formulate TBGs as partially observable Markov decision processes ( POMDPs ) . In this work , we extend the POMDP framework to represent object-level dynamics ( Diuk et al. , 2008 ) . The Object-Oriented ( OO ) POMDP is a tuple 〈S , O , Z , Φ , G , A , R , T , γ〉 , where : • S andO define the spaces of low-level states and observations from the TGB environments while Z and Φ are the spaces of object-level states and observations . In a TBG , o ∈ O is a text message from the game environment ( e.g. , `` Please fry the potato . `` ) , whereas φ ∈ Φ records the specific objects and their relations in this message ( e.g. , a triplet like `` potato-need-fry '' ) ) . To model the object dynamics , the agent must extract φ from o by distilling objects ’ information from text sentences . In this work , we model object states z ∈ Z ( instead of s ∈ S ) and learn latent representations for z . • G and A are the spaces for goals and actions . We study choice-based games ( Yin & May , 2019 ) , where candidate actions At ∈ A are available at time step t. We include a goal variable g to mark different tasks in each game . Following the Universal MDP ( UMDP ) ( Schaul et al. , 2015 ) , the agent initializes a goal at the beginning of a game and updates it when the task is finished . • R and T define the spaces of reward and transition models . γ is a discount factor . We assume the real dynamics models ( p∗T and p ∗ r ) are unknown , so we learn the object-oriented transition model and the reward model , i.e. , ∀k ∈ { 1 , . . . , K } , pkT ( zk , t|zt−1 , at−1 ) ∈ T and pr ( rt|zt−1 , gt ) ∈ R. In our OO-POMDP , transitions pT , observations φ , and states z are modelled for each object , whereas actions a and rewards r are defined for the entire environment . This generalizes to popular RL environments that accept an action and return a reward at every time step . Unlike ( Wandzel et al. , 2019 ) that represents object states with symbolic attributes , we develop a latent object representation that can generalize to the complex environment with high-dimensional inputs ( e.g. , text ) . 2.3 GRAPH REPRESENTATION FOR OBJECTS . We utilize an object-relation-object triplet φ to represent object information . At time step t , φt = [ φt,1 , ... , φt , M ] ( M is the number of observed triplets at t ) . Given a total of K candidate objects and C candidate relations , these triplets can be mapped to a knowledge graph Ωt ∈ { 0 , 1 } C×K×K . Each entry ( c , i , j ) ∈ { 0 , 1 } indicates whether there is a relationship r between the ith and jth objects . This knowledge graph forms a natural representation of object relations since object information in most observations ot corresponds to either entity attributes ( e.g. , `` potato-is-fried '' ) or to relational information about entities ( e.g. , `` potato-on-counter '' and `` bedroom-north_of-kitchen '' ) , which can be conveniently represented by triplets and the corresponding graph . Memory Graph . We store the observations up to time step t in a memory graph ht = Ω0 ⊕ Ω1 ⊕ · · · ⊕ Ωt that captures object information observed by the agent since the beginning of a game . Similar to previous works Adhikari et al . ( 2020 ) ; Ammanabrolu & Riedl ( 2019 ) ; Ammanabrolu & Hausknecht ( 2020 ) , we summarize the object-level history with a latent memory graph ht . To update the memory from t − 1 to t , we learn a graph updater ⊕ , and ht = ht−1 ⊕ Ωt . During updating , ⊕ needs to resolves some semantically contradictory triplets , for example `` player-at-kitchen '' and `` player-at-bedroom '' ( because the player can not simultaneously be in two different locations ) . Our OOTD model is trained to automatically emulate such an operator . Our transition model ( Section 3.1 ) is trained to generate ht from zt , allowing the latent object states to capture object relations . 2.4 MODEL-BASED PLANNING IN TEXT-BASED GAMES . Table 1 : Object dynamics for planning , where the transition models and observation encoders are independently parameterized for a total of K objects . Transition models [ pkT ( zk , t+1|at , zt ) ] K k=1 Observation encoders [ qkE ( zk , t+1|ot+1 , at , zt ) ] K k=1 Reward model pr ( rt|zt , gt ) Graph & Obs . decoder pΩ ( ht|zt ) , po ( ot|zt ) We introduce model-based planning in TBGs . Based on the OO-POMDP ( Section 2.2 ) , at each time step t , we define latent states zt = [ z1 , t , . . . , zK , t ] for a total of K objects , text observations ot , action commands at , and scalar rewards rt , that follow the stochastic dynamics in Table 1 . The details of these dynamics models are introduced in Section 3 . Based on these dynamics , we implement planning algorithms to select an action from candidate commands At for maximizing the expected sum of rewards Eπ ( ∑T t=0 γ trt ) , as shown in Figure 1 . In this work , we study choice-based games ( Yin & May , 2019 ) , where the candidate commands ( or actions ) At are available and the planner determines the action at ∈ At to be performed . The planning algorithms include Dyna-Q , Monte-Carlo Tree Search ( MCTS ) and their combinations : Dyna-Q ( Kuvayev & Sutton , 1996 ) incorporates dynamics models and Q-learning . Dyna-Q interactively 1 ) updates the dynamics model with observed transitions and 2 ) trains the Q network to minimize the Temporal Difference ( TD ) loss ( Equation 1 ) based on both observed transitions from the environment and predicted transitions from the dynamics models . Compared to the model-free Deep Q-Network ( DQN ) , Dyna-Q is more sample efficient : by expanding the replay buffer with dynamics models , Dyna-Q converges faster with the same number of interactions from the environment . LTD = E [ ( rt + γ max a∈At Q ( zt , a , gt ) −Q ( zt−1 , at−1 , gt−1 ) ) 2 ] ( 1 ) MCTS ( Kocsis & Szepesvári , 2006 ) is a heuristic search algorithm that builds and updates a search tree based on environment dynamics . By performing Monte-Carlo simulations organized in a tree structure , MCTS does an efficient search in environments with large action spaces ( Silver et al. , 2018 ) . MCTS iteratively runs multiple playouts . At the ith playout , we implement 1 ) Selection : Traverse the tree from the root node to a leaf node ( corresponding to object state zτ ) by selecting the action command ai , t to maximize the Upper Confidence Bound ( UCB ) ( Couëtoux et al. , 2011 ) : ai , t = arg max a∈At [ Qi ( zt , a , g ) + cpuct √ log ( i− 1 ) ζi−1 ( zt , a , g ) + 1 ] ( 2 ) where cpuct controls the scale of exploration , and ζi records the visit count at the ith play . 2 ) Evaluation : Evaluate the selected leaf zτ with the reward model pr ( rτ |zτ , gt ) . 3 ) Expansion : Expand the leaf node by adding child nodes . 4 ) Back Up : Update the action-values : Qi = ( Qi−1ζi−1 + rτ ) / ( ζi−1 + 1 ) and increment the visit count : ζi = ζi−1 + 1 on all the traversed edges . Dyna-Q + MCTS initializes Q values in MCTS by utilizing the Dyna-Q network . This combination enables MCTS to perform the tree search with more efficiency and efficacy ( Silver et al. , 2016 ) .","The advancement of dynamics models enables model-based planning in complex environments. Dynamics models mostly study image-based games with fully observable states. Generalizing these models to Text-Based Games (TBGs), which often include partially observable states with noisy text observations, is challenging. In this work, we propose an Object-Oriented Text Dynamics (OOTD) model that enables planning algorithms to solve decision-making problems in text domains. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information. To improve the robustness of dynamics, our OOTD model identifies the objects influenced by input actions and predicts beliefs of object states with independently parameterized transition layers. We develop variational objectives under the object-supervised and self-supervised settings to model the stochasticity of predicted dynamics. Empirical results show that our OOTD-based planner significantly outperforms model-free baselines in terms of sample efficiency and running scores.",This paper studies object-oriented text dynamics (OOTD) model and planning in text-based games. OOTD learns internal representation of object dynamics and uses transition layers to predict the belief of object states. Empirical results shows a performance boost compared to model-free baselines. Ablation studies are performed to explain the importance of OOTD components.,"Object-Oriented Text Dynamics (OOTD) is a model that integrates graph representation and independently parameterized transition layers to capture object-level dynamics in text-based games. The model is trained to predict belief states of objects and is used in planning algorithms, such as Dyna-Q and Monte Carlo Tree Search, to solve decision-making problems in the text domain.","The article introduces a new model-based planning algorithm for text-based games, called Object-Oriented Text Dynamics (OOTD) that learns to predict beliefs about object dynamics in these environments by integrating graph representation for objects and independently parameterized transition layers. The OOTD model significantly outperforms other baselines in terms of game performance and sample efficiency, making it a promising candidate for optimizing planning in the text-domain.","0,2,1",0,2,1,0,,coverage,coverage,"coverage, informative",informative,0,1,2,0,Summary A has good coverage and does not contain factual errors.,Summary B covers less relevant details and does not mention comparison do model-free baselines.,good coverage and balance of details,too general
19,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"1 INTRODUCTION . Deep convolutional networks ( Krizhevsky et al. , 2017 ; He et al. , 2016 ) achieve high accuracy but at significant computational cost . Their computational burden hinders deployment , especially for time-critical or low-resource use cases that for instance require interactivity or inference on a mobile device . This efficiency problem is tackled by special-purpose libraries ( Chetlur et al. , 2014 ) , compression by network pruning ( Han et al. , 2015 ; Li et al. , 2016 ; Liu et al. , 2019 ) and quantization ( Rastegari et al. , 2016 ; Jacob et al. , 2018 ) , and adjusting architecture by distillation ( Hinton et al. , 2015 ; Romero et al. , 2014 ) . These solutions accelerate network computation but the entire network must still be computed ; however , a prediction may be needed sooner . Time constraints vary , but the inference time of a standard deep network does not . Anytime inference mitigates this issue by bringing flexibility to model computation . An anytime algorithm ( Dean & Boddy ) gradually improves its results as more computation time is given . It can by interrupted at any point during its computation to return a result as system or user requirements demand . In this way the time to the first output is reduced while the quality of the last output is preserved . An anytime model makes a progression of predictions between the first and last . This pro- gression continues if time remains , or halts if it is either already satisfactory or out of time . For example , consider a user on a mobile device : an approximate result could be returned earlier if there is urgency , or the user could monitor the sequence of predictions as time goes by and stop the model once it is good enough . Note that anytime inference differs from adaptive or dynamic inference ( Veit & Belongie , 2018 ; Wu et al. , 2018 ; Wang et al. , 2018a ) where the model decides how much to compute instead of an external decision . Prior research has explored anytime inference by feature selection ( Karayev et al. , 2014 ) or ensembling models through boosting ( Grubb & Bagnell , 2012 ) . For end-to-end models , research has focused on classification for anytime inference or adaptive inference . In particular , the multi-scale dense network ( Huang et al. , 2017 ) is an architecture for resource efficient classification . The attraction of anytime inference is not limited to classification however , and the additional computation required for pixel-level recognition tasks makes it even more desirable . For instance , an autonomous driving system may demand swifter reaction time for safety in the presence of pedestrians , and so an anytime semantic segmentor might sooner recognize their presence . In addition to urgency , an anytime segmentor could help efficiency , by not further processing already confident predictions of street pixels and therefore save power . In this work , we develop the first single-model anytime approach for pixel-level visual recognition tasks . We adopt an early exiting framework , where multiple predictors branch off from the intermediate stages of the model . The exits are trained end-to-end ( both the original exit and intermediate exits ) , and during inference each provides a prediction in turn . To compensate for differences in depth and spatial dimensions across stages , we redesign the predictors for earlier exits . For each exit we choose an encoder-decoder architecture that combines pooling , convolution , and interpolation to enlarge receptive fields and smooth spatial noise . Exits might suffice for anytime image classification , but pixel-level tasks have spatial structure . Simple regions may need less processing while complex regions need more . Standard inference applies an equal amount of computation at every pixel without taking advantage of spatial structure . Spatially adaptive networks ( Figurnov et al. , 2017 ; Xie et al. , 2020 ) improve efficiency by skipping computation in places , but unlike anytime models they only make one final prediction . We propose a spatially anytime network that makes predictions for different pixels at different points in time . Our spatially adaptive inference scheme decides whether or not to continue computation at each exit and position . We mask the output of each exit by thresholding the confidence of its predictions : the remaining computation for sufficiently confident pixels is then reduced ( Fig . 2 ) . For each masked pixel , its prediction will be persisted in the following exits , as it is already sufficiently confident . In following layers , the features for the masked pixel will be interpolated , rather than convolved , and therefore reduce computation . The confidence measure can depend on the task , e.g. , in segmentation it could be the entropy of class predictions . This confidence adaptivity can substantially reduce the total computation while maintaining accuracy . We experiment with two pixel-level visual recognition tasks : Cityscapes semantic segmentation and MPII human pose estimation . Our networks are based on the recent HRNet architecture ( Wang et al. , 2020 ) . Redesigning the exits and including confidence adaptivity significantly improves across accuracy-efficiency operating points . Our full model not only makes anytime predictions , but its final predictions achieve the same level of accuracy as the base model with 40-60 % less total computation . For analysis , we visualize predictions and confidence adaptivity across exits , and ablate design choices for the exits and masking . To summarize our contributions : ( 1 ) we propose the first general-purpose anytime inference approach for pixel-level visual recognition ; ( 2 ) our redesigned exit architecture and confidenceadaptive masking boost the accuracy-efficiency tradeoff of pixel-wise anytime inference ; ( 3 ) we experiment with semantic segmentation and human pose estimation to show efficacy across tasks ; and ( 4 ) we analyze our design choices by visualization and ablation . 2 RELATED WORK . Anytime Inference . Anytime algorithms ( Zilberstein , 1996 ; Dean & Boddy ) can be interrupted at any point during computation to return a result , whose quality improves gradually with more computation time . In machine learning , anytime inference has been achieved by boosting ( Grubb & Bagnell , 2012 ) , reinforcement learning ( Karayev et al. , 2014 ) , and random forests ( Fröhlich et al. , 2012 ) . Anytime deep networks have been brought to bear on image recognition , but not pixel-level recognition . Branching architectures have been a common strategy ( Amthor et al. , 2016 ; Teerapittayanon et al. , 2016 ) along with other techniques such as adaptive loss balancing ( Hu et al. , 2019 ) . While there is work on the tasks of person re-identification ( Wang et al. , 2019 ) and stereo depth ( Wang et al. , 2018b ) , these techniques are task specific , while our method applies to multiple pixellevel tasks , as we show with semantic segmentation and pose estimation . Liu et al . ( Liu & He , 2016 ) learn a hierarchy of models for anytime segmentation , but its multiple models complicate training and testing , and require more memory . Our work instead augment the base model architecture for simplicity and efficiency . The PointRend method ( Kirillov et al. , 2020 ) outputs a initial dense prediction first and then refine it adaptively , but the predictions are all made at full depth . Its majority of the computation is spent before the first output and thus can not be practically “ anytime ” . Our method is the first to selectively update anytime predictions across space and layers . Adaptive Computation . An adaptive model adjusts its computation to each specific instance during inference . For deep networks , this is often done by adjusting which layers to execute , that is , choosing which layers to run or skip . This can be done by a supervised controller ( Veit & Belongie , 2018 ; Liu & Deng , 2017 ) , a routing policy optimized by reinforcement learning ( Wang et al. , 2018a ; Wu et al. , 2018 ; Lin et al. , 2017a ) , or other training strategies ( McGill & Perona , 2017 ) . Rather than choosing layers , spatial adaptivity chooses where to adjust the amount of computation across different spatial positions in the input . For example , the model could infer spatial masks for feature maps and skip computation on masked areas ( Shomron et al. , 2019 ; Dong et al. , 2017 ; Lin et al. , 2017b ; Ren et al. , 2018 ; Cao et al. , 2019 ) . Figurnov et al . ( 2017 ) maintains a halting score at each pixel and once it reaches a threshold the model will stop inference at those positions for spatially coarse tasks like classification or bounding box detection . Xie et al . ( 2020 ) stochastically sample positions for computation from an end-to-end learned sampling distribution . Li et al . ( 2017 ) convert a deep network into a difficulty-aware cascade , where earlier steps handle easier regions and later steps tackle harder regions . These spatially adaptive models reduce computation , but are not anytime : they do not make a series of predictions and can not be interrupted . 3 APPROACH . Anytime Setting and the Early Exiting Framework . In an anytime inference setting , the user can stop the inference process based on the input or a current event . Thus the computation budget for each instance x could be time or input dependent . We use B ( x , t ) to denote the computation budget assigned for instance x at time t , where the time variable t models events that can change the budget . B ( x , t ) could be independent of x , i.e. , the budget only depends on the time t , for example if a model on a server is asked to make predictions with less budget during high-traffic hours ; B ( x , t ) can also be independent of t , meaning the budget is only decided by input x , regardless of external events . The output of the anytime model depends on the budget given , and we denote it as f ( x , B ( x , t ) ) . Assuming L is the task loss and y is the ground truth , the per-instance loss is L ( f ( x , B ( x , t ) ) , y ) . This leads to the expected training loss to be E ( x , y ) ∼ ( X , Y ) , t∼T [ L ( f ( x , B ( x , t ) ) , y ) ] , where ( X , Y ) is the input-output joint distribution and T is the distribution modeling the time or event variable . Next , we introduce the early exiting framework which have been used in prior works ( Huang et al. , 2017 ; Teerapittayanon et al. , 2016 ) for anytime prediction . Standard convolutional networks only have one prediction “ head ” at its final stage . The network takes the input x , forwards it through intermediate layers , and finally output the prediction at its head . The concrete form of the head depends on the task . For pixel-level recognition , the head is usually one or multiple convolutions that output spatial maps representing pixel-wise predictions . To obtain an anytime model , we attach multiple heads to the network , branching from its intermediate features ( Fig . 2 ) . We call these additional heads early exits , since they allow the network to give early predictions and stop the inference at the current layer . Suppose we add k early exits at intermediate layers with layer indices l1 . . . , lk . We denote the intermediate features at these layers Fl1 ( x ) . . . , Flk ( x ) , and the functions represented by the early exits E1 . . . , Ek . Note that Eis may be of the same form but they do not share weights . The early prediction maps can be denoted as ŷi = Ei ( Fli ( x ) ) , i = 1 . . . k. Together with the original final prediction ŷk+1 , the total loss is : Ltotal = k+1∑ i=1 wiL ( ŷi , y ) ( 1 ) where wi is the weight coefficient at exit i . The original network , together with the added exits , will be trained end-to-end to optimize this total loss function . In experiments , we set all weights equal to 1 . This corresponds to the minimization of the expected loss in Sec . 3 when the exiting probabilities at all exits are equal . We find this to be a simple yet effective scheme . For anytime inference , as the network propagates features through its layers , if the computation budget is reached or the user asks the model to stop , it will output the latest ŷi that is already computed . Similar early exiting strategies have been used in resource-efficient image classification ( Teerapittayanon et al. , 2016 ; Huang et al. , 2017 ) , but pixel-level recognition requires further steps detailed in the following subsections . Head Redesign . Typical convolutional networks have a hierarchical structure that begins with shallow , fine , and more local features and ends with deep , coarse , and more global features . These deeper features represent more image content by their larger receptive fields . For pixel-level tasks , upsampling is done within the network to restore lost resolution during downsampling , and ensure precise spatial correspondence between the input and the output . This upsampling can be accomplished in few ( Long et al. , 2015 ) or many ( Zhao et al. , 2017 ) layers , but no matter the architecture , the network learns its most local features in its earliest layers . This presents a challenge for the earliest exits , since these features are limited in depth and receptive field . Making direct predictions at these exits with the typical 1×1 convolution head produces spatially noisy and inaccurate results . To compensate for these lacking early features , we redesign the prediction heads for the exits Ei . Each Ei first downsamples its input features Fli ( x ) , through a series of pooling and 1×1 convolution layers . Each pooling operation halves the spatial resolution , increasing its output ’ s receptive fields . The following convolution provides the opportunity to learn new coarser-level features , specifically for that exit ’ s prediction . After several ( denoting this number as D ) “ pool-conv ” layers , we upsample the features back to the original output resolution , with an equal number ( D ) of bilinear interpolation and 1× 1 convolution layers . The output of this “ interpolate-conv ” sequence will be the prediction ŷi at this exit . This is important for ensuring the spatial accuracy of the prediction for pixel-level tasks . Our redesigned exits are essentially small “ encoder-decoder ” modules ( Fig . 2 ) , where the encoder downsamples the features , the decoder upsamples them back . The downsampling ratio at each exit is determined by D , the number of consecutive “ pool-conv ” layers . Intuitively , features at earlier layers are more fine-level , and the exit branching from them can potentially benefit from more downsampling . In experiments , we use encoder with D = N − i downsampling operations at exit i , where N is the total number of exits , including the original last exit . Empirically we find this strategy works well , and alternative strategies are compared in Sec . 5 . Finally , in all early exits , the first convolution will transform the number of channels to a fixed number for all exits . By setting the channel width relatively small , we can still save computation while adding layers with this redesigned encoder-decoder head structure . Confidence Adaptivity . For pixel-level recognition tasks , any early prediction ŷi is a spatial map consisting of pixel-wise predictions at each position . While most convolution networks spend equal amount of computation at each input position , it is likely that recognition at some regions are easier than others , where the network can make predictions with a high confidence even at earlier exits . For instance , the inner part of a large sky segment may be easy to recognize , whereas the boundary between the bicycle and the person riding it may need more careful delineation . Once an early prediction is made , we can inspect the “ confidence ” at each position . As an example , for semantic segmentation , the maximum probability over all classes can serve as a confidence measure . If the confidence has passed a pre-defined threshold at certain positions ( green squares on predictions in Fig . 2 ) , we may decide these predictions are likely to be correct , and not continue the computation of further layers at this position . Suppose the pixels of the early prediction ŷi are indexed by p , we form a mask Mi : Mi ( p ) = { 0 , if Confidence ( ŷi ( p ) ) ≥ Threshold 1 , otherwise ( 2 ) For any convolution layer between exit i ( Ei ) and the next exit i + 1 ( Ei+1 ) , we could choose whether to perform or skip computation at position p based on the mask ( Fig . 2 ) . Assuming C is a convolution layer with input fin , then by applying the mask , the output fout at position p becomes : fout ( p ) = { C ( fin ) ( p ) , if Mi ( p ) = 1 , 0 , if Mi ( p ) = 0 . ( 3 ) If C ’ s output and the mask Mi do not share the same spatial size , we interpolate ŷi in Eqn . 2 to the size of C ’ s output , so that the mask Mi is compatible with C in Eqn . 3 . The output fout could be sparse , with many positions being 0 . This could potentially harm further convolutional computation . To compensate for this , we spatially interpolate these positions from their neighbors across all channels , using a similar approach as in ( Xie et al. , 2020 ) . Denoting the interpolation operation as I , the final output feature f∗out is f∗out ( p ) = { fout ( p ) , if Mi ( p ) = 1 , I ( fout ) ( p ) , if Mi ( p ) = 0 . ( 4 ) Here , the value of I ( fout ) ( p ) is a weighted average of all the neighboring pixels centered at p within a radius r : I ( fout ) ( p ) = ∑ s∈Ω ( p ) W ( p , s ) fout ( s ) ∑ s∈Ω ( p ) W ( p , s ) ( 5 ) where s indexes p ’ s neighboring pixels and Ω ( p ) = { s|∥s− p∥∞ ≤ r , s ̸= p } , the neighborhood of p. We set radius r = 7 in our experiments . W ( p , s ) is the weight assigned to point s for interpolating at p , for which we use the RBF kernel , a distance-based exponential decaying weighting scheme : W ( p , s ) = exp ( −λ2∥p− s∥22 ) ( 6 ) with λ being a trainable parameter . This indicates that the closer s is to p , the larger its assigned weight will be . Note that masked-out features ( Mi ( p ) = 0 ) still participate in the interpolation process as inputs with values of 0 . Replacing filtering by interpolation at these already confident spatial locations ( Mi ( p ) = 0 ) could potentially save a substantial amount of computation . The mask Mi will be used for all convolutions between exit i and i+ 1 , including the convolutions inside exit i+ 1 . Once the forward pass arrives at the next exit , to make the prediction ŷi+1 , the last prediction at positions where Mi ( p ) = 0 will be carried over , having already been deemed confident enough at the last exit and having been skipped during further computations . This means : ŷi+1 ( p ) = { Ei+1 ( Fli+1 ( x ) ) , if Mi ( p ) = 1 , ŷi ( p ) , if Mi ( p ) = 0 . ( 7 ) The network then calculates a new mask Mi+1 based on ŷi+1 , and uses it to skip computation going forward . The process continues until we reach the final exit . In summary , we incorporate spatial confidence adaptivity into the early exiting network , by not filtering at spatial locations that are already sufficiently confident in the latest prediction . At these positions interpolation is used instead , at much reduced computational cost , to avoid excessive sparsity . Unless otherwise specified , confidence adaptivity is used in both training and inference .","Anytime inference requires a model to make a progression of predictions which might be halted at any time. Prior research on anytime visual recognition has mostly focused on image classification. We propose the first unified and end-toend model approach for anytime pixel-level recognition. A cascade of “exits” is attached to the model to make multiple predictions and direct further computation. We redesign the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, and make full use of prior predictions, we develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. Our full model with redesigned exit architecture and spatial adaptivity enables anytime inference, achieves the same level of final accuracy, and even significantly reduces total computation. We evaluate our approach on semantic segmentation and human pose estimation. On Cityscapes semantic segmentation and MPII human pose estimation, our approach enables anytime inference while also reducing the total FLOPs of its base models by 44.4% and 59.1% without sacrificing accuracy. We compare our method with deep equilibrium networks, as well as a feature-based stochastic sampling approach, and we show that the accuracy-computation curve of our method dominates them.","The article presents a new approach to pixel-level visual recognition tasks called ""anytime inference"" which provides flexibility in model computation by making a progression of predictions between the first and last. This allows approximate results to be returned earlier in time-critical or low-resource situations. An ""early exiting framework"" is used where multiple predictors branch off from intermediate stages of the model to provide these early predictions which can be stopped once they are good enough. Redesigned prediction heads are also used for the early exits and a ""confidence adaptivity"" is introduced which decides whether or not to continue computation at each exit and position based on the confidence of its predictions. The approach was tested on two pixel-level visual recognition tasks, Cityscapes semantic segmentation, and MPII human pose estimation and yielded promising results.","In this article, the authors propose the first general-purpose anytime inference approach for pixel-level visual recognition tasks. The proposed approach redesigns the exiting framework for anytime inference, allowing the network to make early predictions and stop the inference at the current layer. The authors also introduce a spatially anytime network that makes predictions for different pixels at different points in time, and experiment with semantic segmentation and human pose estimation tasks. The proposed approach improves the accuracy-efficiency tradeoff of pixel-wise anytime inference and achieves better results than standard inference methods.","The paper presents adaptive and anytime methods for semantic segmentation and pose recognition. Both of these are ""pixel-level"" tasks where the model is expected to output a prediction for each pixel in the input image. Their adaptive method, that performs variable amounts of computation depending on the input and the budget, with the amount of computation devoted to a given pixel varying from one part of the image to another. This is done by adding early exits to the base model architecture, and modifying the convolution layers to perform sparse computation on a subset of locations followed by interpolation. Experiments are done on Cityscapes semantic segmentation and MPII pose estimation benchmarks.","1,2,0",2,1,1,0,,coverage,factuality,coverage,coverage,1,0,0,2,"B has a good coverage of concepts compared to C, which misses results.","Summary A presents ""anytime inference"" incorrectly.",good coverage,no information about results
20,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"1 INTRODUCTION . We consider the unsupervised learning scenario , which aims at learning distributed representations for complex natural data without human annotations . Despite great recent progress in selfsupervised learning ( Chen et al. , 2020a ; Caron et al. , 2021 ; Grill et al. , 2020 ; Caron et al. , 2020 ; 2018 ; He et al. , 2020 ; Chen and He , 2021 ) , the problem of properly evaluating the quality of the learned representations has not been fully explored . We approach this question by drawing an analogy with the evolution of human language . Recent literature ( Swarup and Gasser , 2008 ; Smith et al. , 2013 ) suggests that linguistic structure arises as a trade-off between expressiveness and learnability . On one hand , expressiveness is the ability to discriminate objects and concepts of the world . On the other hand , Kirby et al . ( 2014 ) states that language evolution undergoes an iterated learning process , where each individual acquires language by observing a limited amount of language outputs of others ( Chomsky , 1980 ) . This process induces a form of compression , which makes the language simpler and more learnable for the next generation . Linguistic structure , e.g. , compositionality , emerges between these opposing forces . A similar guiding principle can be applied when learning representations of natural data . Many existing methods in self-supervised learning can be considered as attempts to increase expressiveness of the representations , i.e. , by maximizing or controlling the mutual information between inputs and representations , either through reconstruction ( Vincent et al. , 2008 ; Higgins et al. , 2017 ) or contrastive learning ( Oord et al. , 2018 ; Bachman et al. , 2019b ) . We argue that in addition to maximizing expressiveness , a good representation should also be learnable , e.g . support generalization from few examples ( Laina et al. , 2020 ) . In this paper , we propose an evaluation framework for self-supervised learning relying on expressiveness and learnability . Compared with previous work ( e.g. , Wang and Isola , 2020 ) , our framework is independent of ( i ) the availability of downstream task labels or predefined data augmentation techniques , and ( ii ) architecture choices and dimensionality of the latent representations , which makes it applicable to all previous SSL methods . We propose to measure expressiveness in terms estimators of the intrinsic dimension ( ID ) of the data representations ( Ansuini et al. , 2019 ) . Taking inspiration from Laina et al . ( 2020 ) , we quantify learnability in terms of the learning speed ( in terms of number of examples ) of a KNN learner trained to predict the K-Means cluster labels for held out representations . We dub this method Cluster Learnability ( CL ) . We show that high ID and CL correlate with high downstream performance for 30 different pretrained checkpoints across different architectures . To further demonstrate the usefulness of our framework , we present a case-study in which we improve upon DeepCluster ( Caron et al. , 2018 ) , which is a self-supervised learning method that iterates between clustering the representations and fitting the cluster labels . Inspired by our CL measure , we refine the cluster labels by executing KNN at the end of each K-means clustering round . The resulting method , KNN-DeepCluster , is able to outperform DeepCluster in both ImageNet and STL10 . When visualizing the training trajectories on the CL-ID plane , we find that our method indeed improves learnability at convergence . The correlation between performance and learnability holds also for checkpoints obtained during training , which suggests the possibility of using our framework to guide the development of new SSL algorithm , without labels at hand . Our contributions are summarized below : • We analyze self-supervised learning through the lens of expressiveness and learnability , and we propose tractable way to estimate them with Intrinsic Dimension ( ID ) and Cluster Learnability ( CL ) . • We carry out a large-scale evaluation on ImageNet and STL10 with 30 state-of-the-art selfsupervised and supervised checkpoints . We show that CL-ID is more correlated with the Top1 accuracy than previous approaches , with a Pearson ’ s r of 0.93 . • We propose a modification to DeepCluster to increase its cluster learnability . The resulting algorithm , KNN-DeepCluster is able to outperform DeepCluster . 2 RELATED WORK . Representation Evaluation Several recent works address the question of representation evaluation in self-supervised learning . Whitney et al . ( 2021 ) propose to use the learning dynamic of the downstream classifiers to measure the representation complexity , however the method depends on extra human labels . Wang and Isola ( 2020 ) decompose the contrastive loss into an alignment loss and a uniformity loss , which can be used to analyze the representation . However , their method requires access to the projection head output , limiting their applicability to general architecture . Pretext tasks like jigsaw , rotation prediction are shown to be well correlated with the supervised evaluation ( Reed et al. , 2021 ; Deng and Zheng , 2021 ) or architecture search ( Liu et al. , 2020 ) . However , as is shown in our experiments , they only provide limited predictive power when applying to a large pretrained family of checkpoints including both supervised and self-supervised models . In addition , these methods depends on predefined data augmentation , which might be task specific . Manifold Intrinsic Dimension Intrinsic dimension can be thought of as the smallest number of variable needed to approximate the representation manifold . Note that it is different from the intrinsic dimension of loss landscape ( Li et al. , 2018 ) , which measures minimum dimension of parameter space needed to optimize . We build upon extensive work on estimating intrinsic dimension . Applying local neighborhood information to estimate the intrinsic dimension is not a new idea , and it is shown to be more superior to the global eigenvalue approach ( Pettis et al. , 1979b ) . Ansuini et al . ( 2019 ) apply the TwoNN estimator ( Facco et al. , 2017 ) to the non-linear representation manifold of modern deep neural nets . They find that the intrinsic dimension is inversely correlated to the classification accuracy , and that the deep layers have smaller intrinsic dimensions . Their work is further extended to confirm that natural images lies in a low-dimension manifold ( Pope et al. , 2021 ) , and lower ID datasets leads to better generalization . Our work presents a more nuanced view about ID . It suggests that in order to learning representation from scratch , a least amount of ID is needed to convey the information from the dataset , e.g. , a constant-value representation has an ID of 0 , but no capacity to generalize at all . More discussion can be found in Section 5 . Learnability , Ease-of-Transmission and Compression Learnability has been argued to be a hallmark of the human language in order to be effortlessly transmitted through generations ( Kirby et al. , 2014 ; Rafferty et al. , 2011 ; Beckner et al. , 2017 ; Zhou and Yurovsky , 2021 ; Kampen , 2004 ) , and it is also true for visual concepts like color ( Xu et al. , 2010 ) , categories ( Griffiths et al. , 2006 ) , shapes ( Portelance et al. , 2021 ) etc . In deep learning , it has been explored in the context of emergent communication ( Ren et al. , 2020 ; Guo et al. , 2019 ; Li and Bowling , 2019 ) , language drift ( Lu et al. , 2020 ) , and neural module networks ( Vani et al. , 2021 ) , but it is less explored for vision representation learning , except for a human study on just two SSL methods Laina et al . ( 2020 ) . Learnability has a tight connection to compression ( Chaitin , 2007 ) and prequential codelength ( Dawid , 1984 ) , which quantifies the compression levels with the online learning error . Existing works ( Blier and Ollivier , 2018 ) has use it to support the generalization ability of the learner ( e.g. , deep neural nets ) on the dataset ( e.g. , labeled images ) . However , we use it to quantify the learnability of the representation , in the sense that if the emerged Kmeans clustering is more learnable , then the same KNN learner could achieve a lower compression bound via prequential coding . 3 PROPOSED FRAMEWORK . In this section , we introduce our setup and define our measures of expressiveness and learnability . 3.1 NOTATION . We consider the following setting : we assume we have a dataset of images { xi } Ni=1 represented in a pixel space X ⊂ Rd , where xi ∼ P are sampled i.i.d from some natural image distribution P over X . We also consider representation mapsF : X → Rm , typically pretrained neural networks , which represent any image as an m-dimensional vector . Any such map forms a representation dataset Z = { zi } Ni=1 where zi = F ( xi ) . Given zi ∈ Z and an integer k ≥ 1 , we denote by NN ( zi , k ) the k-th nearest neighbor of zi . While we generally mean ‘ nearest ’ with respect to Euclidean distance , in practice we will also use the cosine distance function,1 D ( z1 , z2 ) = 2− 2 cos ( z1 , z2 ) ( 1 ) Let rik = D ( zi , NN ( zi , k ) ) be the distance of the data point zi to its k-th nearest neighbor . 3.2 INTRINSIC DIMENSION ( ID ) . To measure expressiveness , we propose to use the notion of intrinsic dimensionality ( ID ) of the data in the representation space ( Pettis et al. , 1979a ) . Intuitively , as the representation becomes more expressive , we expect it to be able to represent images coming from more fine-grained categories , which could have multiple disentangled features such as color , shape , positions in space , etc . If these disentangled features correspond to different directions in some non-linear manifold , ID should yield a good surrogate for the expressiveness of the representation . Inferring the intrinsic dimension of a highly nonlinear manifold is a challenging problem ( e.g. , Levina and Bickel , 2005 ) . In this work , we leverage the nearest neighbor-based method of Facco et al . ( 2017 ) to estimate ID , which uses only the first two nearest neighbors of each point . This estimator ( TwoNN ) is shown to be reliable with respect to representation dimensions and scalable to real-world datasets with deep neural networks ( Ansuini et al. , 2019 ) . Assuming the data density is approximately constant around data points up to the distance of their neighbors , this method exploits the simple relationship between the distribution of the ratio of first and second nearest neighbor distances and the intrinsic dimension . Formally , let µi = ri2/ri1 be the ratio of distances for the data point zi . We suppose the points are sampled on a manifold with intrinsic dimension d. It can be shown that , under the above local uniformity assumption , µi follows a Pareto distribution with parameter d+ 1 on [ 1 , ∞ ) . That is P ( µ1 , · · · , µN |d ) = dN N∏ i=1 µ − ( d+1 ) i ( 2 ) While d can be computed by maximizing the likelihood , we follow a much simpler method proposed by Ansuini et al . ( 2019 ) and estimate d with a linear regression on the empirical cumulate of the distribution of µ , which is shown to have good practical performance ( Facco et al. , 2017 ) . This estimator is exact for uniformly distributed data . It is also shown to remain asymptotically correct for non-uniform sampling of zi when the image dataset is large enough ( Ansuini et al. , 2019 ) . 1Both distances produce similar results in our experiments .","We argue that representations induced by self-supervised learning (SSL) methods should both be expressive and learnable. To measure expressiveness, we propose to use the Intrinsic Dimension (ID) of the dataset in representation space. Inspired by the human study of Laina et al. (2020), we introduce Cluster Learnability (CL), defined in terms of the learning speed of a KNN classifier trained to predict Kmeans cluster labels for held-out representations. By collecting 30 state-of-art checkpoints, both supervised and self-supervised, using different architectures, we show that ID and CL can be combined to predict downstream classification performance better than the existing techniques based on contrastive losses or pretext tasks, while having no requirements on data augmentation, model architecture or human labels. To further demonstrate the utility of our framework, we propose modifying DeepCluster (Caron et al., 2018) to improve the learnability of the representations. Using our modification, we are able to outperform DeepCluster on both STL10 and ImageNet benchmarks. The performance of intermediate checkpoints can also be well predicted under our framework, suggesting the possibility of developing new SSL algorithms without labels.","The article proposes a new framework for evaluating the quality of representations learned by self-supervised learning methods. The authors analyze the problem of self-supervised learning through the lens of expressiveness and learnability, and propose tractable ways to estimate these properties using intrinsic dimensionality and cluster learnability. They conduct a large-scale evaluation on ImageNet and STL10 with 30 state-of-the-art checkpoints, showing that their proposed measures are more correlated with downstream performance than previous approaches. The authors also propose a modification to DeepCluster to increase its cluster learnability, resulting in improved performance on ImageNet and STL10.","The article proposes an evaluation framework for self-supervised learning that measures expressiveness and learnability in representation learning. The framework draws an analogy with the evolution of human language and takes into consideration the ability to discriminate objects and concepts of the world, as well as an iterated learning process that induces compression and makes the language simpler and more learnable for the next generation. The proposed measures are Intrinsic Dimension (ID) and Cluster Learnability (CL), which are shown to be more correlated with downstream performance than previous approaches. The framework is applicable to all previous self-supervised learning methods and is used to improve the DeepCluster method with the proposed KNN-DeepCluster, which outperforms DeepCluster in both ImageNet and STL10.","This paper tries to give a measurement method to evaluate the learned model, which is highly correlated with the final test accuracy. The measurement method depends on two key factors: Intrinsic Dimension (ID) and CLuster Learnability (CL). This paper claims that the model with higher ID and CL performs better. Using ID and CL to predict top-1 accuracy can achieve a Pearson correlation coefficient of 0.93, which is better than existing predictors (e.g., alignment and uniformity). Inspired by the above observation, this paper proposes a modified DeepCluster algorithm to increase the final performance.","2,1,0",1,0,"2,1",0,,other,fluency,coverage,coverage,1,2,"0,1",2,I like how summary B emphasizes the analogy with human language for the learnability metric. But summary A is good too.,Summary C is not very fluent and does not characterize the paper contributions.,good balance of coverage and details,not much information on background 
21,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"1 Introduction . Neural architecture search ( NAS ) is a popular area of machine learning , which aims to automate the process of developing neural architectures for a given dataset . Since 2017 , a wide variety of NAS techniques have been proposed [ 78 , 45 , 32 , 49 ] . While the first NAS techniques trained thousands of architectures to completion and then evaluated the performance using the final validation accuracy [ 78 ] , modern algorithms use more efficient strategies to estimate the performance of partially-trained or even untrained neural networks [ 11 , 2 , 54 , 34 , 38 ] . Recently , many performance prediction methods have been proposed based on training a model to predict the final validation accuracy of an architecture just from an encoding of the architecture . Popular choices for these models include Gaussian processes [ 60 , 17 , 51 ] , neural networks [ 36 , 54 , 65 , 69 ] , tree-based methods [ 33 , 55 ] , and so on . However , these methods often require hundreds of fully-trained architectures to be used as training data , thus incurring high initialization time . In contrast , learning curve extrapolation methods [ 11 , 2 , 20 ] need little or no initialization time , but each individual prediction requires partially training the architecture , incurring high query time . Very recently , a few techniques have been introduced which are fast both in query time and initialization time [ 38 , 1 ] , computing predictions based on a single minibatch of data . Finally , using shared weights [ 45 , 4 , 32 ] is a popular paradigm for NAS [ 73 , 25 ] , although the effectiveness of these methods in ranking architectures is disputed [ 53 , 74 , 76 ] . Despite the widespread use of performance predictors , it is not known how methods from different families compare to one another . While there have been some analyses on the best predictors within ∗ { colin , yang } @ abacus.ai , { zelaa , fh } @ cs.uni-freiburg.de , robin @ robots.ox.ac.uk 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . each class [ 41 , 72 ] , for many predictors , the only evaluation is from the original work that proposed the method . Furthermore , no work has previously compared the predictors across different families of performance predictors . This leads to two natural questions : how do zero-cost methods , model-based methods , learning curve extrapolation methods , and weight sharing methods compare to one another across different constraints on initialization time and query time ? Furthermore , can predictors from different families be combined to achieve even better performance ? In this work , we answer the above questions by giving the first large-scale study of performance predictors for NAS . We study 31 predictors across four popular search spaces and four datasets : NAS-Bench-201 [ 13 ] with CIFAR-10 , CIFAR-100 , and ImageNet16-120 , NAS-Bench-101 [ 71 ] and DARTS [ 32 ] with CIFAR-10 , and NAS-Bench-NLP [ 21 ] with Penn TreeBank . In order to give a fair comparison among different classes of predictors , we run a full portfolio of experiments , measuring the Pearson correlation and rank correlation metrics ( Spearman , Kendall Tau , and sparse Kendall Tau ) , across a variety of initialization time and query time budgets . We run experiments using a training and test set of architectures generated both uniformly at random , as well as by mutating the highest-performing architectures ( the latter potentially more closely resembling distributions encountered during an actual NAS run ) . Finally , we test the ability of each predictor to speed up NAS algorithms , namely Bayesian optimization [ 36 , 54 , 69 , 51 ] and predictor-guided evolution [ 66 , 59 ] . Since many predictors so far had only been evaluated on one search space , our work shows which predictors have consistent performance across search spaces . Furthermore , by conducting a study with three axes of comparison ( see Figure 1 ) , and by comparing various types of predictors , we see a more complete view of the state of performance predictor techniques that leads to interesting insights . Notably , we show that the performance of predictors from different families are complementary and can be combined to achieve significantly higher performance . The success of these experiments opens up promising avenues for future work . Overall , our experiments bridge multiple areas of NAS research and act as recommendations for the best predictors to use under different runtime constraints . Our code , based on the NASLib library [ 52 ] , can be used as a testing ground for future performance prediction techniques . In order to ensure reproducibility of the original results , we created a table to clarify which of the 31 predictors had previously published results on a NAS-Bench search space , and how these published results compared to our results ( Table 7 ) . We also adhere to the NeurIPS 2021 checklist along with the specialized NAS best practices checklist [ 31 ] . Our contributions . We summarize our main contributions below . • We conduct the first large-scale study of performance predictors for neural architecture search by comparing model-based methods , learning curve extrapolation methods , zero-cost methods , and weight sharing methods across a variety of settings . • We release a comprehensive library of 31 performance predictors on four different search spaces . • We show that different families of performance predictors can be combined to achieve substantially better predictive power than any single predictor . 2 Related Work . NAS has been studied since at least the 1990s [ 19 , 58 ] , and has been revitalized in the last few years [ 78 ] . While initial techniques focused on reinforcement learning [ 78 , 45 ] and evolutionary search [ 37 , 49 ] , one-shot NAS algorithms [ 32 , 12 , 4 ] and predictor-based NAS algorithms [ 65 , 54 , 69 ] have recently become popular . We give a brief survey of performance prediction techniques in Section 3 . For a survey on NAS , see [ 15 ] . The most widely used type of search space in prior work is the cell-based search space [ 79 ] , where the architecture search is over a relatively small directed acyclic graph representing an architecture . A few recent works have compared different performance predictors on popular cell-based search spaces for NAS . Siems et al . [ 55 ] studied graph neural networks and tree-based methods , and found that gradient-boosted trees and graph isomorphism networks performed the best . However , the comparison was only on a single search space and dataset , and the explicit goal was to achieve maximum performance given a training set of around 60 000 architectures . Another recent paper [ 41 ] studied various aspects of supernetwork training , and separately compared four model-based methods : random forest , MLP , LSTM , and GATES [ 42 ] . However , the comparisons were again on a single search space and dataset and did not compare between multiple families of performance predictors . Other papers have proposed new model-based predictors and compared the new predictors to other model-based baselines [ 34 , 65 , 54 , 69 ] . Finally , a recent paper analyzed training heuristics to make weight-sharing more effective at ranking architectures [ 72 ] . To the best of our knowledge , no prior work has conducted comparisons across multiple families of performance predictors . 3 Performance Prediction Methods for NAS . In NAS , given a search space A , the goal is to find a∗ = argmina∈Af ( a ) , where f denotes the validation error of architecture a after training on a fixed dataset for a fixed number of epochs E. Since evaluating f ( a ) typically takes hours ( as it requires training a neural network from scratch ) , many NAS algorithms make use of performance predictors to speed up this process . A performance predictor f ′ is defined generally as any function which predicts the final accuracy or ranking of architectures , without fully training the architectures . That is , evaluating f ′ should take less time than evaluating f , and { f ′ ( a ) | a ∈ A } should ideally have high correlation or rank correlation with { f ( a ) | a ∈ A } . Each performance predictor is defined by two main routines : an initialization routine which performs general pre-computation , and a query routine which performs the final architecture-specific computation : it takes as input an architecture specification , and outputs its predicted accuracy . For example , one of the simplest performance predictors is early stopping : for any query ( a ) , train a for E/2 epochs instead of E [ 77 ] . In this case , there is no general pre-computation , so initialization time is zero . On the other hand , the query time for each input architecture is high because it involves training the architecture for E/2 epochs . In fact , the runtime of the initialization and query routines varies substantially based on the type of predictor . In the context of NAS algorithms , the initialization routine is typically performed once at the start of the algorithm , and the query routine is typically performed many times throughout the NAS algorithm . Some performance predictors also make use of an update routine , when part of the computation from initialization needs to be updated without running the full procedure again ( for example , in a NAS algorithm , a model may be updated periodically based on newly trained architectures ) . Now we give an overview of the main families of predictors . See Figure 1 ( left ) for a taxonomy of performance predictors . Model-based ( trainable ) methods . The most common type of predictor , the model-based predictor , is based on supervised learning . The initialization routine consists of fully training many architectures ( i.e. , evaluating f ( a ) for many architectures a ∈ A ) to build a training set of datapoints { a , f ( a ) } . Then a model f ′ is trained to predict f ( a ) given a . While the initialization time for model-based predictors is very high , the query time typically takes less than a second , which allows thousands of predictions to be made throughout a NAS algorithm . The model is also updated regularly based on the new datapoints . These predictors are typically used within BO frameworks [ 36 , 54 ] , evolutionary frameworks [ 66 ] , or by themselves [ 67 ] , to perform NAS . Popular choices for the model include tree-based methods ( where the features are the adjacency matrix representation of the architectures ) [ 33 , 55 ] , graph neural networks [ 36 , 54 ] , Gaussian processes [ 47 , 51 ] , and neural networks based on specialized encodings of the architecture [ 69 , 42 ] . Learning curve-based methods . Another family predicts the final performance of architectures using only a partially trained network , by extrapolating the learning curve . This is accomplished by fitting the partial learning curve to an ensemble of parametric models [ 11 ] , or by simply summing the training losses observed so far [ 50 ] . Early stopping as described earlier is also a learning curve-based method . Learning curve methods do not require any initialization time , yet the query time typically takes minutes or hours , which is orders of magnitude slower than the query time in model-based methods . Learning curve-based methods can be used in conjunction with multi-fidelty algorithms , such as Hyperband or BOHB [ 27 , 16 , 24 ] . Hybrid methods . Some predictors are hybrids between learning curve and model-based methods . These predictors train a model at initialization time to predict f ( a ) given both a and a partial learning curve of a as features . Models in prior work include an SVR [ 2 ] , or a Bayesian neural network [ 20 ] . Although the query time and initialization time are both high , hybrid predictors tend to have strong performance . Zero-cost methods . Another class of predictors have no initialization time and very short query times ( so-called “ zero-cost ” methods ) . These predictors compute statistics from just a single forward/backward propagation pass for a single minibatch of data , by computing the correlation of activations within a network [ 38 ] , or by adapting saliency metrics proposed in pruning-at-initialization literatures [ 23 , 1 ] . Similar to learning curve-based methods , since the only computation is specific to each architecture , the initialization time is zero . Zero-cost methods have recently been used to warm start NAS algorithms [ 1 ] . Weight sharing methods . Weight sharing [ 45 ] is a popular approach to substantially speed up NAS , especially in conjunction with a one-shot algorithm [ 32 , 12 ] . In this approach , all architectures in the search space are combined to form a single over-parameterized supernetwork . By training the weights of the supernetwork , all architectures in the search space can be evaluated quickly using this set of weights . To this end , the supernetwork can be used as a performance predictor . This results in NAS algorithms [ 32 , 28 ] which are significantly faster than sequential NAS algorithms , such as evolution or Bayesian optimization . Recent work has shown that although the shared weights are sometimes not effective at ranking architectures [ 53 , 74 , 76 ] , one-shot NAS techniques using shared weights still achieve strong performance [ 73 , 25 ] . Tradeoff between intialization and query time . The main families mentioned above all have different initialization and query times . The tradeoffs between initialization time , query time , and performance depend on a few factors such as the type of NAS algorithm and its total runtime budget , and different settings are needed in different situations . For example , if there are many architectures whose performance we want to estimate , then we should have a low query time , and if we have a high total runtime budget , then we can afford a high initialization time . We may also change our runtime budget throughout the run of a single NAS algorithm . For example , at the start of a NAS algorithm , we may want to have coarse estimates of a large number of architectures ( low initialization time , low query time such as zero-cost predictors ) . As the NAS algorithm progresses , it is more desirable to receive higher-fidelity predictions on a smaller set of architectures ( model-based or hybrid predictors ) . The exact budgets depend on the type of NAS algorithm . Choice of performance predictors . We analyze 31 performance predictors defined in prior work : BANANAS [ 69 ] , Bayesian Linear Regression [ 6 ] , BOHAMIANN [ 57 ] , BONAS [ 54 ] , DNGO [ 56 ] , Early Stopping with Val . Acc . ( e.g . [ 77 , 27 , 16 , 79 ] ) Early Stopping with Val . Loss . [ 50 ] , Fisher [ 1 ] , Gaussian Process ( GP ) [ 48 ] , GCN [ 75 ] , Grad Norm [ 1 ] , Grasp [ 64 ] , Jacobian Covariance [ 38 ] , LCE [ 11 ] , LCE-m [ 20 ] , LcSVR [ 2 ] , LGBoost/GBDT [ 33 ] , MLP [ 69 ] , NAO [ 35 ] , NGBoost [ 55 ] , OneShot [ 73 ] , Random Forest ( RF ) [ 55 ] , Random Search with Weight Sharing ( RSWS ) [ 26 ] , SemiNAS [ 34 ] , SNIP [ 23 ] , SoTL [ 50 ] , SoTL-E [ 50 ] , Sparse GP [ 3 ] , SynFlow [ 61 ] , Variational Sparse GP [ 63 ] , and XGBoost [ 55 ] . For any method that did not have an architecture encoding already defined ( such as the tree-based methods , GP-based methods , and Bayesian Linear Regression ) , we use the standard adjacency matrix encoding , which consists of the adjacency matrix of the architecture along with a one-hot list of the operations [ 71 , 68 ] . By open-sourcing our code , we encourage 104 105 106 Init . time ( seconds ) 101 102 103 Qu er y tim e ( s ec on ds ) .93 .93 .93 .93 .93 .93 .93 .93 .93 .93 .93.93 .4 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .57 .6 .58 .62 .65 .6 .63 .66 .6 .64 .67 .7 .65 .67 .7 .69 .72 .74 .84 .73 .75 .77 .85 .63 .69 .74 .77 .8 .82 .83 .84 .84 .63 .69 .74 .77 .8 .82 .83 .84 .84 .63 .69 .74 .77 .8 .82 .83 .84 .84 .63 .69 .74 .77 .8 .82 .83 .84 .84 .63 .69 .74 .77 .8 .82 .83 .84 .84 .69 .74 .77 .8 .82 .83 .84 .84 .69 .74 .77 .8 .82 .83 .84 .84 .74 .77 .8 .82 .83 .84 .84 .74 .77 .8 .82 .83 .84 .84 .77 .8 .82 .83 .84 .8 .82 .83 .84 .63 .69 .74 .77 .8 .82 .83 .84 .84 .28 .42 .46 .5 .53 .57 .6 .6 .6 .6 .63 .63 .63 .63 .63 .68 .68 .68 .68 .68 .68 .73 .73 .73 .73 .73 .73 .73 Kendall Tau on NAS-Bench-201 CIFAR-10 105 106 107 Init . time ( seconds ) 102 103 104 Qu er y tim e ( s ec on ds ) NAS-Bench-NLP 105 106 107 Init . time ( seconds ) 102 103 104 Qu er y tim e ( s ec on ds ) NAS-Bench-201 ImageNet16-120 104 105 106 Init . time ( seconds ) 101 102 103 Qu er y tim e ( s ec on ds ) NAS-Bench-101 104 105 Init . time ( seconds ) 101 102 103 Qu er y tim e ( s ec on ds ) DARTS 5.00 5.25 5.50 5.75 6.00 5.0 5.2 5.4 5.6 5.8 6.0 BANANAS Bayes . Lin . Reg . Early Stop ( Acc . ) GCN Jacob . Cov . LGBoost LcSVR NGBoost SoTL-E SemiNAS SynFlow XGBoost Figure 2 : The performance predictors with the highest Kendall Tau values for all initialization time and query time budgets on NAS-Bench-201 , NAS-Bench-101 , NAS-Bench-NLP and DARTS . For example , on NAS-Bench-201 CIFAR-10 ( left ) with an initialization time of 106 seconds and query time of 10 seconds , XGBoost achieves a Kendall Tau value of .73 which is the highest value out of the 31 predictors that we tested at that budget . implementing more ( existing and future ) performance predictors which can then be compared to the 31 which we focus on in this work . In Section B.1 , we give descriptions and detailed implementation details for each performance predictor . In Section D , we give a table that describes for which predictors we were able to reproduce published results , and for which predictors it is not possible ( e.g. , since some predictors were released before the creation of NAS benchmarks ) .","Early methods in the rapidly developing field of neural architecture search (NAS) required fully training thousands of neural networks. To reduce this extreme computational cost, dozens of techniques have since been proposed to predict the final performance of neural architectures. Despite the success of such performance prediction methods, it is not well-understood how different families of techniques compare to one another, due to the lack of an agreed-upon evaluation metric and optimization for different constraints on the initialization time and query time. In this work, we give the first large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. We test a number of correlationand rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. Our results act as recommendations for the best predictors to use in different settings, and we show that certain families of predictors can be combined to achieve even better predictive power, opening up promising research directions. Our code, featuring a library of 31 performance predictors, is available at https://github.com/automl/naslib.","Neural architecture search (NAS) refers to automating the process of developing neural network architectures for a given dataset. In recent years, various methods have been proposed to estimate the performance of partially-trained or untrained neural networks. This article presents a large-scale study of 31 performance predictors across four popular search spaces and four datasets and compares model-based, learning curve extrapolation, zero-cost, and weight sharing methods.","Sure, here is a summary of the article in 3 sentences: The article discusses the performance predictors for Neural Architecture Search (NAS) and compares the different types of predictors across four search spaces and four datasets. The main families of predictors are model-based methods, learning curve-based methods, hybrid methods, zero-cost methods, and weight sharing methods. The article finds that different families of predictors can be combined to achieve significantly better predictive power than any single predictor, and that weight sharing methods are particularly effective in speeding up NAS algorithms.","This paper compares 3 kinds of performance predictors, model based predictors, learning curve based predictor and zero nas predictors. They benchmark the performance of all 31 predictors against 5 benchmarks: NAS-Bench101, DARTS, NASBench201 and NLP.  In addition to this, they also incorporate SOTL-E and jacobian variance as addition features to NGBOOST and SemiNAS predictors and demonstrate that it improves the performance over all 3 of them. ","1,2,0",,,,,,,,,,,,,,,,,
22,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"We propose a novel interpretation technique to explain the behavior of structured output models , which learn mappings between an input vector to a set of output variables simultaneously . Because of the complex relationship between the computational path of output variables in structured models , a feature can affect the value of output through other ones . We focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space . In this paper , we assume an arbitrary structured output model is available as a black box and argue how considering the correlations between output variables can improve the explanation performance . The goal is to train a function as an interpreter for the target output variable over the input space . We introduce an energy-based training process for the interpreter function , which effectively considers the structural information incorporated into the model to be explained . The effectiveness of the proposed method is confirmed using a variety of simulated and real data sets . 1 INTRODUCTION . The impressive prediction performance of novel machine learning methods has motivated researchers of different fields to apply these models in challenging problems . However , their complex and non-linear inherence limit the ability to explain what they have learned . Interpretation gets more attention when we want to discover the reasons behind the model ’ s decision and be sure about the trustworthiness and fairness of a trained machine learning model in areas such as medicine , finance , and judgment . Additionally , interpreting a model with a satisfying prediction accuracy in a scientific problem , which results in understanding relationships behind the data , leads to new knowledge about the problem domain.Murdoch et al . ( 2019 ) In many real-world applications , the goal is to map an input variable to a high-dimensional structured output , e.g. , image segmentation and sequence labeling . In such problems , the output space includes a set of statistically related random variables . As considering these dependencies can increase the prediction accuracy , many structured output models have been introduced . Many of these methods use graphical models , including random fields , to capture the structural relations between variables . Most define an energy function over these random fields , with a global minimum at the ground truth . Therefore , an inference is needed to find the best configuration of output variables for input by minimizing the energy function in the prediction step . Early efforts to utilize deep neural networks in structured output problems adopt deep networks to extract high-level features from the input vector to incorporate them in calculating the energy function Peng et al . ( 2009 ) ; Chen et al . ( 2015 ) ; Schwing & Urtasun ( 2015 ) . The computational complexity of the inference step in models that use random fields limits their ability to incorporate complex structures and interactions between output variables . Recent works in Belanger & McCallum ( 2016 ) ; Gygli et al . ( 2017 ) ; Belanger et al . ( 2017 ) ; Graber et al . ( 2018 ) propose to adopt deep networks instead of random fields to model the structure of the output space . Nevertheless , complex interactions between problem variables in such models make their interpretation too challenging , specifically when we focus on the model behavior in predicting a single output variable . This paper attempts to interpret a structured output model by focusing on each output variable separately . Our approach to model interpretation is based on instance-wise feature selection . Its goal is to find the relative importance of each input feature in predicting a single output variable . The subset of important features can vary across the input space . The complicated interactions between computational paths of output variables in structured output models cause critical challenges for finding a subset of important features associated with each output variable . A feature may not be used directly in the computational path of output but affects its value through relations with other outputs . To compute the importance of a feature for a target output variable , we should aggregate its effect on all output variables that are correlated to this target . Existing approaches of model interpretation can be divided into two groups , model-based and post hoc analysis Murdoch et al . ( 2019 ) . The model-based interpretation approach encourages machine learning methods that readily provide insight into what the model learned . However , it leads to simple models that are not sufficiently effective for complex structured output problems . Here we follow the post hoc analysis and try to explain the behavior of a trained , structured output model provided as a black box . Many interpretation techniques to find the importance of features as a post hoc analysis have been introduced . Works in Zhou & Troyanskaya ( 2015 ) ; Zeiler & Fergus ( 2013 ) ; Zintgraf et al . ( 2017 ) make perturbations to some features and observe their impact on the final prediction . These techniques are computationally inefficient in situations where we search for the most valuable features . Since we should perform a forward propagation for all possible perturbations , in another trend , works in Simonyan et al . ( 2013 ) ; Bach et al . ( 2015 ) back-propagate an importance signal from the target output through the network to calculate the critical signal of features by calculating the gradient of the target w.r.t the input features . These models are computationally more efficient than perturbation-based techniques because they need only one pass of propagating . However , they need the structure of the network to be known . As this approach may cause a saturation problem , DeepLIFT Shrikumar et al . ( 2017 ) proposes that instead of propagating a gradient signal , the difference of the output from a reference value in terms of the difference of features from a reference value to be considered . In addition to these approaches , other ideas have also been introduced in model interpretation . Authors in Ribeiro et al . ( 2016 ) introduce LIME which trains a local interpretable surrogate model to simulate the behavior of a black box model in the vicinity of a sample . It randomly selects a set of instances of the input space around that sample and obtains the black box prediction for them , and trains the surrogate model by this new dataset . Therefore this interpretable model is a good approximation of the black box around the locality of the selected sample . Shapley value , a concept from the game theory , explains how to distribute an obtained payout between coalition players fairly . The work in Lundberg & Lee ( 2017 ) proposes the kernel SHAP for approximating the shapely value for each feature as its importance for a prediction . As an information-theoretic perspective on interpretation , the work in Chen et al . ( 2018 ) proposes to find a subset of features with the highest mutual information with the output . This subset is expected to involve the most important features for the output . Existing interpretation techniques can be applied to explain the behavior of a structured model , w.r.t . a single output , by ignoring other output variables . However , none of these approaches consider possible correlations between output variables and only analyze the marginal behavior of the black box on the target . In this paper , we attempt to incorporate the structural information between output variables during training the interpreter . As our goal is to present a local interpreter , which is trained globally as Chen et al . ( 2018 ) , we train a function over the input space which returns the index of most important features for decision making about the target output . Since the value of other output variables affects the value of the target , incorporating them into the training procedure of an interpreter function may lead to higher performance and decrease our uncertainty about the black box behavior . To the best of our knowledge , this is the first time an interpreter is designed mainly for structured output models , and dependencies between output variables are considered during training the interpreter . 2 PRELIMINARIES AND MOTIVATION . Structured output prediction models map an arbitrary n-dimensional feature vector x ∈ X to the output y ∈ Y where y = [ y1 , y2 , . . . , yd ] includes a set of correlated variables with known and unknown complex relationships and Y shows a set of valid configurations . Now we explain our intuition about an interpreter , which explains the behavior of a structured output model in predicting a single output variable . We assume a structured model is available as a black box about which we do not know . Our goal is to find indices of k important features of x which affect the black box prediction about the target output yt . As for different localities of the input space these indices may vary , the proposed interpreter is a function IN t ( x ; α ) : X → { 0 , 1 } n over the input space with a set of parameters α which returns an n-dimensional k-hot vector . In this vector , the value of 1 shows the indices of selected k important features for target output yt . We define Θsb as the set of all parameters and hidden variables inside the structured black box . The probabilistic graphical model of Fig . 1 describes dependencies between problem variables . In this figure , x shows the input variable , ysbt and y sb −t = { ysbi |i 6= t } show black box predictions and αIN t is the set of parameters of IN t. The bidirectional edge between ysbt and ysb−t emphasizes the correlation between the outputs of a structured model . In fact αIN t is determined based on Θsb and the black box architecture , and the final prediction of the ysbt does not directly affect its value . However , here , Θsb is a latent variable which makes active paths between αIN t and output values ysbt and y sb −t . Therefore αIN t and y sb −t are dependent random variables and we have : H ( αIN t |x , ysbt ) > H ( αIN t |x , ysbt , ysb−t ) ( 1 ) where H ( .| . ) shows the conditional entropy . We use the strict inequality because αIN t and ysb−t are dependent random variables . The left term measures our uncertainty when we train the interpreter only by observing the target output ysbt . This inequality confirms that the uncertainty is decreased when we consider observed ysb−t during estimating αIN t . Motivated by this fact we propose a training procedure for an interpreter IN t which incorporates the structural information of the output space by observing the black box prediction on all output variables . We call our method SOInter as we propose it to train an Interpreter specifically for Structured Output models . 3 PROPOSED METHOD . We consider psb ( y|x ) as the distribution by which the structured black box predicts the output as follows , ysb = arg max y psb ( y|x ) . ( 2 ) Our goal is to train the interpreter IN t ( x ; α ) which explores a subset of most important features that affects the value of black box prediction on the target output yt in each locality of the input space . As the desired interpreter detects the subset of most important features , we expect perturbating other ones does not change the black box prediction of the target yt . Motivated by this statement , we are encouraged to compare the black box prediction for the target output when a sample and its perturbated version are passed through the black box . The interpreter IN t ( x ; α ) returns a k-hot vector in which the value of 1 shows the index of a selected feature . We define x̃ , a perturbated version of x , as follows , x̃ = x IN t ( x ; α ) ( 3 ) in which only features selected by the interpreter are included , and other ones are filled with zeros . Therefore by passing x and x̃ through the black box , we expect the value of the tth element of predictions to be the same , and we can define a penalty over the value of the target in these two situations . However , since the structure of the black box is unknown and the arg max operation in equation 2 is non-differentiable , a loss function that directly compares these two output values can not be used to find the optimal interpreter . Therefore , in the following subsection , we try to achieve a penalty according to the difference between these values for the target , which can transfer the gradient through an interpreter block .","We propose a novel interpretation technique to explain the behavior of structured output models, which learn mappings between an input vector to a set of output variables simultaneously. Because of the complex relationship between the computational path of output variables in structured models, a feature can affect the value of output through other ones. We focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. In this paper, we assume an arbitrary structured output model is available as a black box and argue how considering the correlations between output variables can improve the explanation performance. The goal is to train a function as an interpreter for the target output variable over the input space. We introduce an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. The effectiveness of the proposed method is confirmed using a variety of simulated and real data sets.","This article proposes a novel interpretation technique for structured output models, which learn mappings between an input vector and a set of output variables simultaneously. The proposed method focuses on interpreting the behavior of a single output variable and considers the correlations between output variables during training an interpreter function. The method is trained globally and returns a k-hot vector indicating the most important features for decision-making about the target output variable, which are expected to involve the most important features for the output variable.",The article proposes a new technique for model interpretation that focuses on explaining the behavior of structured output models. The proposed technique aims to find the most important features utilized by the model to decide on the target output in each locality of the input space. The effectiveness of the proposed method is confirmed using various simulated and real-world datasets.,"This paper propose a method for interpreting structured output model. The key idea is to find an ""interpretation"" which explains an ""target"" output random variable based on subset of rest of the output variables. The training objective is on finding a small subset which keeps the target output random variable invariant. The proposed methodology is applied to explain a synthetic energy function and structured prediction energy networks. ","2,1,0",1,0,2,1,,coverage,coverage,coverage,informative,1,2,0,1,Summary B has a good coverage and not excessive details.,Summary C is more superficial.,good coverage,too general
23,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"1 INTRODUCTION . As information explosion has become one major factor affecting human life in the decade , recommender systems , which can filter useful information and contents of user ’ s potential interests , play an increasingly indispensable part in day-to-day activities . Recommendation problems can be generally formalized as matrix completion ( MC ) where one has a user-item rating matrix whose entries , which stand for interactions of users with items ( ratings or click behaviors ) , are partially observed . The goal of MC is to predict missing entries ( unobserved or future potential interactions ) in the matrix based on the observed ones . Modern recommender systems need to meet two important requirements in order for desirable effectiveness and practical utility . First of all , recommendation models should have enough expressiveness to capture diverse user interests and preferences so that the systems can accomplish personalized recommendation . Existing methods based on collaborative filtering ( CF ) or , interchangeably , matrix factorization ( MF ) have shown great power in this problem by factorizing the rating matrix into two classes of latent factors ( i.e. , embeddings ) for users and items respectively , and further leverage dot-product of two factors to predict potential ratings ( Koren et al. , 2009 ; Rendle et al. , 2009 ; Srebro et al. , 2004 ; Zheng et al. , 2016b ) . Equivalently , for each user , the methods consider a one-hot user index as input , assume a user-specific embedding function ( which maps a user index to a latent factor ) , and use the learnable latent factor to represent user ’ s preferences in a low-dimensional space . One can select proper dimension size to control balance between capacity and generalization . Recent works extend MF with complex architectures , like multi-layer perceptrons ( Dziugaite & Roy , 2015 ) , recurrent units ( Monti et al. , 2017 ) , autoregressive models ( Zheng et al. , 2016a ) , graph neural networks ( van den Berg et al. , 2017 ) , etc. , and achieve state-of-the-art results on most benchmarks . The second requirement stems from a key observation from real-world scenarios : recommender systems often interact with a dynamic open world where new users , who are not exposed to models during training , may appear in test stage . This requires that models trained on one group of users manage to adapt to unseen users . However , the above-mentioned CF models would fail in this situation since the user-specific embeddings are parametrized for specific users and need to be learned collaboratively with all other users in transductive setting . One brute-force way is to retrain the whole model with an augmented rating matrix , but extra time cost would be unacceptable for online systems . There are quite a few studies that propose inductive matrix completion models using user features ( Jain & Dhillon , 2013 ; Xu et al. , 2013 ; Cheng et al. , 2016 ; Ying et al. , 2018 ; Zhong et al. , 2018 ) . Their different thinking paradigm is to target a user-sharing mapping from user features to user representations , instead of from one-hot user indices used by CF models . Since the feature space is shared among users , such methods are able to adapt a model trained on existing users to unseen users . Nevertheless , feature-driven models often suffer from limited expressiveness with low-quality features that have weak correlation with target labels . For example , users with the same age and occupation ( commonly used features ) may have distinct ratings on movies and music . Unfortunately , high-quality features that can unveil user interests for personalized recommendation are often hard to collect due to increasingly concerned privacy issues . A following question arises : Can we have a recommendation model that guarantees enough expressiveness for personalized recommendation and enables inductive learning ? In fact , to simultaneously meet the two requirements is a non-trivial challenge when high-quality user features are unavailable . First , to achieve either of them , one needs to compromise on the other . In fact , the one-hot user indices ( together with learnable user-specific embeddings ) give a maximized capacity for learning distinct user preferences from historical rating patterns . To make inductive learning possible , one needs to construct a shared input feature space among users out of the rating matrix , as an alternative to one-hot user indices . However , the new constructed features have relatively insufficient expressive power . Second , the computation based on new feature space often bring extra costs for time and space , which limits model ’ s scalability to large-scale datasets . In this paper , we propose an inductive collaborative filtering model ( IRCF ) 1 as a general CF framework that achieves inductive learning for matrix completion and meanwhile guarantees enough expressiveness and scalability . As shown in Fig . 1 , we consider a base transductive matrix factorization model trained on one group of users ( called support users ) and a relation inference model that aims to estimate their relations to another group of users ( called query users ) w.r.t . historical rating patterns . The ( multiple ) estimated relational graphs enable attentively message passing from users to users in the latent space and compute user-specific representations in an inductive way . The output user representations can be used to compute product with item representations to predict ratings in the matrix , as is done by CF models . Compared with other methods , one key advantage of IRCF is the capability for inductively computing user-specific representations without using features . Besides , our method possesses the following merits . 1 ) Expressiveness : A general version of our model can minimize reconstruction loss to the same level as matrix factorization under a mild condition . Also , we qualitatively show its superior expressiveness than feature-driven and local-graphbased inductive models that may fail in some typical cases . Empirically , IRCF provides very close performance to transductive CF models when given sufficient training ratings . 2 ) Generalization : IRCF manages to achieve state-of-the-art results on new ( unseen ) users compared with inductive models . Also , IRCF gives much better accuracy than transductive models when training data becomes sparse and outperforms other competitors in extreme cold-start recommendation . 3 ) Scal- 1The codes will be released . ability : Our model enables mini-batch training and efficient inference . In experiments , IRCF is averagely ten times faster than local-graph-based inductive model . 4 ) Flexibility : As a general CF framework , IRCF is flexible to incorporate with various architectures ( e.g . MLP-based , GNN-based , autoregressive , etc . ) as a base transductive model as well as deal with implicit user feedbacks . 2 BACKGROUND AND RELATED WORKS . In this section , we present some background of the problem and discuss relationships to related works . We consider a general matrix completion ( MC ) problem which deals with a user-item rating matrix R = { rui } M×N where M and N are the numbers of users and items , respectively . For implicit feedback , rui is a binary entry which denotes whether user u rated ( or clicked on , reviewed , liked , purchased , etc . ) item i or not . For explicit feedback , rui records rating value of user u on item i . The entries of R are partially observed and the goal is to estimate the missing values in the matrix2 . In the following , we introduce related works and highlight their differences to our paper . In Fig . 5 , we provide an illustration for comparison with these methods . General Collaborative Filtering . Existing methods for MC are generally based on collaborative filtering ( CF ) or , interchangeably , matrix factorization ( MF ) where user u ( resp . item i ) corresponds to a d-dimensional latent factor ( i.e. , one-hot embedding ) pu ( resp . qi ) .Then one has a prediction model r̂ui = f ( pu , qi ) where f can be basically specified as simple dot product or some complex architectures , like neural networks , graph neural networks , etc . One advantage of CF models is that the user-specific embedding pu ( as learnable parameters ) can provide enough expressive power for learning diverse personal preferences from user historical behaviors and decent generalization ability through collaborative learning with all the users and items . Furthermore , the user embeddings possess rich profile information as representation of user preferences and can benefit various downstream tasks , like target advertisement , user-controllable recommendation ( Ma et al. , 2019 ; Cen et al. , 2020 ) , influence maximization ( Khalil et al. , 2017 ; Manchanda et al. , 2019 ) , friend recommendation , etc . However , such user-specific embedding limits the model in transductive learning and when it comes to new users during test , one has to retrain the model ( often associated with embeddings for both new and existing users as well as items ) with new augmented rating matrix . Admittedly , one can consider local updates for embeddings of new users with fixed item embeddings learned from existing users . Nevertheless , such operation makes model learning for users independent from each other , which is prone for over-fitting compared with collaborative learning in CF models . Besides , it requires ‘ incremental ’ learning for each new user , while IRCF can deliver on-the-fly inference in online systems . Feature-driven Recommendation . The CF models do not require any side information other than the rating matrix , but can not be trained inductively due to the learnable user-specific embedding pu . To address the issue , one can leverage side information such as attribute features to achieve inductive learning . Define user features ( like age , occupation , etc . ) as au and item features ( like movie genre , director , etc . ) as bi . The feature-driven model targets a prediction model r̂ui = g ( au , bi ) . Since the space of au is shared among users , a model trained on one group of users can adapt to other users without retraining . However , feature-driven models often provide limited performance since the shared feature space is not expressive enough compared to one-hot embedding space . Another issue is that high-quality features are hard to collect in practice . We note that our model does not require user features for inductive collaborative filtering . Inductive Matrix Completion . There are a few existing works that attempt to handle inductive matrix completion using only user-item rating matrix . ( Hartford et al. , 2018 ) ( F-EAE ) puts forward an exchangeable matrix layer that takes a whole rating matrix as input and inductively outputs prediction for missing ratings . However , the scalability of F-EAE is limited since it requires the whole rating matrix as input for training and inference for users , while IRCF enables mini-batch training and efficient inference . Besides , ( Zhang & Chen , 2020 ) ( IGMC ) proposes to use local subgraphs of user-item pairs in a bipartite graph of rating information as input features and further adopt graph neural networks as representation tool to encode subgraph structures for rating prediction . The model achieves inductive learning via replacing users ’ one-hot index embeddings by shared input features ( i.e. , index-free local subgraph structures ) . Differently , IRCF maintains the ability to give 2Also , in some situations , one targets a ranking list of items for each user as top-N recommendation . In this paper , we focus on predicting missing values in the matrix and leave top-N recommendation for future works . user-specific embeddings , which represent users ’ preferences and interests and can be used for downstream tasks ( like target advertisement , user-controllable recommendation , influence maximization , etc . ) while F-EAE and IGMC can merely output a prediction score without such user representation . Moreover , the expressiveness of IGMC is limited since the local subgraph structures can be indistinguishable for users with distinct behaviors ( see Section 3.2 for more discussions ) , while IRCF has equivalent expressiveness as original CF models . Item-based CF Models . Some previous works use item embeddings as representation for users . ( Cremonesi et al. , 2010 ; Kabbur et al. , 2013 ) proposes to use a combination of items rated by users to compute user embeddings and frees the model from learning parametric one-hot embeddings for users . Furthermore , there are quite a few auto-encoder architectures for recommendation problem , leveraging user ’ s rating vector ( ratings on all the items ) as observed input , estimate user embedding ( as latent variables ) based on that , and output prediction for missing values in the rating vector ( Sedhain et al. , 2015 ; Liang et al. , 2018 ) . With item embeddings and user ’ s rating history , these methods achieve inductive learning for users and can adapt to new users during test . On methodological level , IRCF has the following differences : 1 ) IRCF learns to use weighted combination of users ’ embeddings to compute embeddings for new users , and such combination weights possess interpretability for underlying social influence or user proximity ; 2 ) IRCF considers both users ’ and items ’ one-hot embeddings in general CF models that maintain better capacity than item-based CF models that only considers learnable parameters in item embedding space .","Collaborative filtering has shown great power in predicting potential user-item ratings by factorizing an observed user-item rating matrix into products of two sets of latent factors. However, the user-specific latent factors can only be learned in transductive setting and a model trained on existing users cannot adapt to new users without retraining the model. In this paper, we propose an inductive collaborative filtering framework that learns a hidden relational graph among users from the rating matrix. We first consider a base matrix factorization model trained on one group of users’ ratings and devise a relation inference model that estimates their underlying relations (as dense weighted graphs) to other users with respect to historical rating patterns. The relational graphs enable attentive message passing from users to users in the latent space and are updated in end-to-end manner. The key advantage of our model is the capability for inductively computing user-specific representations using no feature, with good scalability and superior expressiveness compared to other feature-driven inductive models. Extensive experiments demonstrate that our model achieves state-of-the-art performance for inductive learning on several matrix completion benchmarks, provides very close performance to transductive models when given many training ratings and exceeds them significantly on cold-start users.","The article discusses the problem of matrix completion in collaborative filtering, where the goal is to predict missing ratings in a user-item rating matrix. The authors propose a novel method called Inductive Recommender Framework (IRCF) that addresses the issue of limited expressiveness and scalability in existing methods. IRCF learns to use weighted combinations of users' embeddings to compute embeddings for new users, providing better adaptability to new users. The proposed method also considers both users' and items' one-hot embeddings in a general collaborative filtering framework, which maintains better capacity than item-based CF models.","This paper proposed an inductive collaborative filtering method, called IRCF. The goal is to possess expressiveness (against feature-driven methods) as well as generalization (against one-hot encoding based methods). In IRCF, there are a matrix factorization model for support users and a relation model for query users. The former is trained with transductive learning to obtain support users embeddings and item embeddings. The relation model then generates query user embeddings as weighted sum of support user embeddings by examining relational graph between support and query users.","Recommender systems play an increasingly important role in filtering information and recommending personalized content to users. Matrix completion is a formalized approach that predicts missing information in a user-item rating matrix based on observed patterns. Existing collaborative filtering (CF) or matrix factorization (MF) models use one-hot user indices and user-specific embedding functions, which do not allow for inductive learning, making it difficult to adapt to new or unseen users. In response, the authors propose an inductive CF model (IRCF) that uses a shared input feature space to facilitate attentive message passing and compute user-specific representations in an inductive way, which adapts well to new users and outperforms previous feature-driven and local-graph-based inductive models.","2,0,1",1,2,2,1,,coverage,clarity,coverage,coverage,2,0,0,2,Summary C has better coverage including background and conclusion.,Summary A do not state clearly the components of the method.,good coverage,too much background information
24,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"√ AT ) Bayesian regret for a problem with A actions after T rounds . We extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases . In this case we show that Thompson sampling can produce policies outside of the optimistic set and suffer linear regret in some instances . Finding a policy inside the optimistic set amounts to solving a convex optimization problem and we call the resulting algorithm ‘ variational Bayesian optimistic sampling ’ ( VBOS ) . The procedure works for any posteriors , i.e. , it does not require the posterior to have any special properties , such as log-concavity , unimodality , or smoothness . The variational view of the problem has many useful properties , including the ability to tune the exploration-exploitation tradeoff , add regularization , incorporate constraints , and linearly parameterize the policy . 1 Introduction . In this manuscript we consider online learning , in particular the multi-armed stochastic bandit problem [ 19 ] and its extension to bilinear saddle-point problems . Thompson sampling ( TS ) is a wellknown Bayesian algorithm to tackle this problem [ 37 , 35 ] . At each iteration TS samples an action according to the posterior probability that it is the optimal action . However , it never computes these probabilities , instead TS samples from the posterior of each arm and then acts greedily with respect to that sample , which is an implicit sample from the posterior probability of optimality . TS enjoys strong theoretical guarantees [ 1 , 35 ] as well as excellent practical performance [ 6 ] . Moreover , it has an elegant information-theoretic interpretation of an agent that is making a tradeoff between information accumulation and regret [ 34 ] . That being said it does have several weaknesses . Firstly , TS requires samples from the true posterior , which may be intractable for some problems . It is known that TS with even small error in its posterior samples can suffer linear regret [ 32 ] . In many cases of interest the posterior distribution is highly intractable to sample from and running MCMC chains , one for each arm , can be wasteful and slow and still not yield precise enough samples for good performance , though recent work has attempted to improve this situation [ 22 ] . Moreover , TS is unable to take into account other preferences , such as constraints or the presence of other agents . In particular , it was recently shown that TS can suffer linear regret in two-player zero sum games [ 27 ] and we shall demonstrate the same phenomenon in this work empirically for constrained bandits . Finally , when performing TS the user never has direct access to the posterior probability of optimality , only samples from that distribution ( though the distribution can be estimated by taking many samples ) . In some practical applications having access to the policy is desirable , for example to ensure safety constraints or to allocate budgets . 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . In this paper we present ‘ variational Bayesian optimistic sampling ’ ( VBOS ) , a new Bayesian approach to online learning . At every step the VBOS algorithm solves a convex optimization problem over the simplex . The solution to this problem is a policy that satisfies a particular ‘ optimism ’ condition . The problem is always convex , no matter what form the posteriors take . The only requirement is the ability to compute or estimate the cumulant generating function of the posteriors . VBOS and TS can be considered as part of the same family of algorithms since , in the bandit case , they both produce policies inside the optimistic set . That being said , VBOS has several advantages over TS . First , in some cases estimating or bounding the cumulant generating function may be easier than sampling from the posterior . Second , incorporating additional requirements such as constraints , opponents or tuning the exploration exploitation tradeoff is easy in VBOS - just change the optimization problem . Finally , when using VBOS we always have direct access to the policy , rather than just implicit samples from it , as well as an explicit upper bound on the expected value of the problem , which TS does not give us . Here we list the contributions of this work . 1.1 Contributions . 1 . We derive a set of ‘ optimistic ’ Bayesian policies for the stochastic multi-armed bandit that satisfy a particular optimism inequality and which includes the TS policy . 2 . We present a new , simple proof of a Õ ( √ AT ) regret-bound for a bandit with A arms after T timesteps that covers any policy in the optimistic set ( Õ ignores logarithmic factors ) . 3 . We derive variational Bayesian optimistic sampling ( VBOS ) , which is a policy in the optimistic set that can be computed by solving a convex optimization problem . 4 . We show that , unlike TS , the variational formulation easily handles more complicated problems , such as bilinear saddle-point problems which include as special cases zero-sum two player games and constrained bandits . 1.2 Preliminaries . A stochastic multi-armed bandit problem is a sequential learning problem in which an agent interacts with an environment in order to maximize its total cumulative reward . Initially the agent does not know how the rewards are distributed and must learn about them from experience . In each round t ∈ N the agent selects an action at ∈ { 1 , . . . , A } and receives a reward rt = µat + ηtat where η t at ∈ R is zero-mean noise associated with action at at time t. We define Ft = ( a1 , r1 , . . . , at−1 , rt−1 ) to be the sequence of actions and rewards observed by the agent prior to round t , and as shorthand we shall use the notation Et ( · ) = E ( · | Ft ) . An algorithm alg is a measurable mapping that takes the entire history at time t and produces a probability distribution , the policy , from which the agent samples its action . In order to assess the quality of an algorithm alg we consider the regret , or shortfall in cumulative rewards relative to the optimal value , Regret ( µ , alg , T ) = Eη , alg [ T∑ t=1 max i µi − rt ] , ( 1 ) where Eη , alg denotes the expectation with respect to the noise process η and any randomness in the algorithm . This quantity ( 1 ) depends on the unknown expected reward µ ∈ RA , which is fixed at the start of play and kept the same throughout . To assess the quality of learning algorithms designed to work across some familyM of problems we define BayesRegret ( φ , alg , T ) = Eµ∼φRegret ( µ , alg , T ) ( 2 ) where φ is a prior probability measure over µ ∈M that assigns relative importance to each problem instance and which we assume is known to the algorithm . 2 Thompson sampling and a variational approach . The algorithm that minimizes the Bayesian regret ( 2 ) is called the Bayes-optimal policy , but it is believed to be intractable to compute in most cases [ 10 ] . With that in mind we would like an algorithm that is computationally tractable and still guarantees a good bound on the Bayesian regret . TS is a Bayesian algorithm that at each time step samples possible values of each arm from their respective posteriors and acts greedily with respect to those samples ; see Algorithm 1 . This simple strategy provides a principled approach to the exploration-exploitation tradeoff in the stochastic multi-armed bandit problem . In this section we derive a set of ‘ optimistic ’ policies , which includes the TS policy , and we shall show that any algorithm that produces policies in this set satisfies a guaranteed Bayesian regret bound . For brevity we defer all proofs to the appendices . 2.1 Bounding the conditional expectation . The Bayesian regret ( 2 ) depends on the expectation of the maximum over a collection of random variables . Here we shall derive an upper bound on this quantity , starting with a generic upper bound on the conditional expectation . If X : Ω → Rd is a random variable in Ld1 ( i.e. , E|Xi| < ∞ for each i = 1 , . . . , d ) then we denote by ΨX : Rd → R ∪ { ∞ } the cumulant generating function of X − EX , i.e. , ΨX ( β ) = logE exp ( β > ( X − EX ) ) . This function is always convex , and we shall assume throughout this manuscript that it is also closed and proper for any random variables we encounter . With this definition we can present a bound on the conditional expectation . Lemma 1 . Let X : Ω → R be a random variable on ( Ω , F , P ) satisfying X ∈ L1 , and let A ∈ F be an event with P ( A ) > 0 . Then , for any τ ≥ 0 E [ X|A ] ≤ EX + τΨX ( 1/τ ) − τ logP ( A ) . ( 3 ) The right hand side of ( 3 ) involves the perspective of ΨX and so is convex in τ [ 4 ] and we can minimize over τ ≥ 0 to find the tightest bound , which we do next . Recall that the convex conjugate of function f : Rd → R is denoted f∗ : Rd → R and is given by f∗ ( y ) = supx ( x > y − f ( x ) ) [ 4 ] . Theorem 1 . Let X : Ω → R be a random variable such that the interior of the domain of ΨX is non-empty , then under the same assumptions as Lemma 1 we have 1 E [ X|A ] ≤ EX + ( Ψ∗X ) −1 ( − logP ( A ) ) . For example , if X ∼ N ( 0 , σ2 ) , then for any event A we can bound the conditional expectation as E [ X|A ] ≤ σ √ −2 logP ( A ) , no matter how X and A are related . This inequality has clear connections to Kullback ’ s inequality [ 8 ] and the Donsker-Varadhan variational representation of KL-divergence [ 11 , Thm . 3.2 ] . The function Ψ∗ is referred to as the Cramér function or the rate function in large deviations theory [ 39 ] . The inverse of the rate function comes up in several contexts , such as queuing theory and calculating optimal insurance premiums [ 21 ] . Cramér ’ s theorem tells us that the probability of a large deviation of the empirical average of a collection of IID random variables decays exponentially with more data , and that the decay constant is the rate function . It is no surprise then that the regret bound we derive will depend on the inverse of the rate function , i.e. , when the tails decay faster we incur less regret . 2.2 The optimistic set . Using Theorem 1 we next derive the maximal inequality we shall use to bound the regret . Lemma 2 . Let µ : Ω → RA , µ ∈ LA1 , be a random variable , let i ? = argmaxi µi ( ties broken arbitrarily ) and denote by Ψi : = Ψµi , then Emax i µi ≤ A∑ i=1 P ( i ? = i ) ( Eµi + ( Ψ∗i ) −1 ( − logP ( i ? = i ) ) ) . Note that Lemma 2 makes no assumption of independence of each entry of µ , the bound holds even in the case of dependency . 1If ΨX = 0 , then we take ( Ψ∗X ) −1 = 0 . In the context of online learning πTSi = P ( i ? = i ) is exactly the TS policy when µi have the laws of the posterior . For ease of notation we introduce the ‘ optimism ’ map Gtφ : ∆A → R which for a random variable µ : Ω→ RA distributed according to φ ( · | Ft ) is given by Gtφ ( π ) : = A∑ i=1 πi ( Etµi + ( Ψt∗i ) −1 ( − log πi ) ) , ( 4 ) where ∆A denotes the probability simplex of dimension A − 1 and Ψti : = Ψµi|Ft . Note that Gtφ is concave since Ψti is convex [ 4 ] . Moreover , if the random variables are non-degenerate then Gtφ is differentiable and the gradient can be computed using Danskin ’ s theorem [ 7 ] . With this notation we can write Et maxi µi ≤ Gtφ ( πTS ) . This brings us to the ‘ optimistic ’ set of probability distributions , which corresponds to the set of policies that satisfy that same inequality . Definition 1 . Let µ : Ω → RA be a random variable distributed according to φ ( · | Ft ) , then we define the optimistic set as Ptφ : = { π ∈ ∆A | Et max i µi ≤ Gtφ ( π ) } . Immediately we have two facts about the set Ptφ . First , it is non-empty since πTS ∈ Ptφ , and second , it is a convex set since the optimism map Gtφ is concave .","We consider online sequential decision problems where an agent must balance exploration and exploitation. We derive a set of Bayesian ‘optimistic’ policies which, in the stochastic multi-armed bandit case, includes the Thompson sampling policy. We provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys Õ( √ AT ) Bayesian regret for a problem with A actions after T rounds. We extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case we show that Thompson sampling can produce policies outside of the optimistic set and suffer linear regret in some instances. Finding a policy inside the optimistic set amounts to solving a convex optimization problem and we call the resulting algorithm ‘variational Bayesian optimistic sampling’ (VBOS). The procedure works for any posteriors, i.e., it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness. The variational view of the problem has many useful properties, including the ability to tune the exploration-exploitation tradeoff, add regularization, incorporate constraints, and linearly parameterize the policy.",This work develops an analog to Thompson sampling by upper-bounding the expected regret in sequential decision-making problems. The two terms in the upper bound loosely resemble the evidence lower bound of variational inference: the first term encourages selecting arms with high expected reward; the second term depends on the inverse of the rate function and penalizes heavy tails and encourages exploration. The resulting variational Thompson sampling algorithm is evaluated on a random game and a constrained bandit problem.,"The article presents a new Bayesian approach to online learning called 'variational Bayesian optimistic sampling' (VBOS) for the stochastic multi-armed bandit problem. The algorithm solves a convex optimization problem over the simplex at every step and produces a policy that satisfies a particular 'optimism' condition. VBOS has several advantages over Thompson sampling, including the ability to tune the exploration-exploitation tradeoff, add regularization, incorporate constraints, and linearly parameterize the policy.","The article discusses the problem of online learning in the context of multi-armed bandit problems, where the goal is to maximize the cumulative reward over a sequence of rounds. The author presents a new algorithm called ""variational Bayesian optimistic sampling"" (VBOS) that solves a convex optimization problem to produce a policy inside the ""optimistic set"", which includes the Thompson sampling (TS) policy. The article shows that VBOS enjoys several advantages over TS, including easier incorporation of additional requirements such as constraints or the presence of other agents, and an explicit upper bound on the expected value of the problem.","0,1,2",1,0,1,2,,coverage,"coverage, relevance (+ detail)",coverage,factuality,1,0,1,2,"Summary B has good coverage, including advantages of VBOS over TS.",Summary A goes is too detailed in the methods.,good coverage,"unfaithful wrt ""presence of other agents"""
25,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"1 Introduction . Graph is a widely-used data structure [ 51 ] , where the nodes are connected to each other through natural or handcrafted edges . Similar to other data structures , the representation learning for node classification faces the challenge of quantity-imbalance issue , where the labeling size varies among classes and the decision boundaries of trained classifiers are mainly decided by the majority classes [ 46 ] . There have been a series of studies [ 35 , 11 , 49 ] handling the Quantity-Imbalance Node Representation Learning ( short as QINL ) . However , different with other data structures , graph-structured data suffers from another aspect of the imbalance problem : the imbalance caused by the asymmetric and uneven topology of labeled nodes , where the decision boundaries are driven by the labeled nodes close to the topological class boundaries ( left of Figure 1 ) thus interfering with the model learning . Present Work . For the first time , we recognize the Topology-Imbalance Node Representation Learning ( short as TINL ) as a graph-specific imbalance learning topic , which mainly focus on the 1The code is available at https : //github.com/victorchen96/ReNode . 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . decision boundaries shift phenomena driven by the topology imbalance in graph and is an essential component for node imbalance learning . Comparing with the well-explored QINL that studies the imbalance caused by the numbers of labeled nodes , TINL explores the imbalance caused by the positions of labeled nodes and owns the following characteristics : • Ubiquity : Due to the complex connections of the graph nodes , the topology structure of nodes in different categories is naturally asymmetric , which makes TINL an essential characteristic in node representation learning . Hence , it is difficult to construct a completely symmetric labeling set even with an abundant annotation budget . • Perniciousness : The influence from labeled nodes decays with the topology distance [ 3 ] . The asymmetric topology of labeled nodes in different classes and the uneven distribution of labeled nodes in the same class will cause the influence conflict and influence insufficient problems ( left of Figure 1 ) respectively , resulting in a shift of decision boundaries . • Orthogonality : Quantity-imbalance studies [ 49 , 8 , 5 ] usually treat the labeled nodes of the same class as a whole and devise solutions based on the total numbers of each class , while TINL explores the influence of the unique position of each labeled node on decision boundaries . Thus , TINL is independent of QINL in terms of the object of study . Exploring TINL is of great importance for node representation learning due to its ubiquity and perniciousness . However , the methods [ 17 , 22 ] for quantity imbalance can be hardly applied to TINL because of the orthogonality . To remedy the topology-imbalance issue , thus promoting the node classification , we propose a model-agnostic training framework ReNode to re-weight the labeled nodes according to their positions . We devise the conflict detection-based Topology Relative Location ( Totoro ) metric to leverage the interaction among labeled nodes across the whole graph to locate their structural positions . Based on the Totoro metric , we further increase the training weights of nodes with small conflict that are highly likely to be close to topological class centers to make them play a more pivotal role during training , and vice versa ( right of Figure 1 ) . Empirical results of various imbalance scenarios ( TINL , QINL , large-scale graph ) and multiple graph neural networks ( GNNs ) demonstrate the effectiveness and generalizability of our method . Besides , we provide the sensitivity to topology imbalance as a new evaluation perspective for different GNN architectures . 2 Topology-Imbalance Node Representation Learning . 2.1 Notations and Preliminary . In this work , we follow the well-established semi-supervised node classification setting [ 47 , 18 ] to conduct analyses and experiments . Given an undirected and unweighted graph G = ( V , E , L ) , where V is the node set represented by the feature matrixX ∈ Rn∗d ( n = |V | is the node size and d is the node embedding dimension ) , E is the edge set which is represented by an adjacency matrix A ∈ Rn∗n , L ⊂ V is the labeled node set and usually we have |L| |V | , the node classification task is to train a classifier F ( usually a GNN ) to predict the class label y for the unlabeled node set U = V −L . The training sets for different classes are represented by ( C1 , C2 , · · · , Ck ) and k is the number of classes . The labeling ratio δ = L/V is the proportion of labeled nodes in all nodes . In this work , we focus on TINL in homogeneously-connected graphs and hope to inspire future studies on the critical topology-imbalance issue . 2.2 Understanding Topology Imbalance via Label Propagation . From Figure 1 , we can intuitively perceive the imbalance brought by the positions of labeled nodes ; in this part , we further explore the nature of topology imbalance with the well-known Label Propagation [ 50 ] algorithm ( short as LP ) and provide a uniform analysis framework for the comprehensive node imbalance issue . In LP , labels are propagated from the labeled nodes and aggregated along edges , which can also be viewed as a random walk process from labeled nodes . The convergence result Y after repeated propagation is regarded as the nodes soft-labels : Y = α ( I − ( 1− α ) A′ ) −1Y 0 , ( 1 ) where I is the identity matrix , α ∈ ( 0 , 1 ] is the random walk restart probability , A′ = D− 12AD− 12 is the adjacency matrix normalized by the diagonal degree matrixD , Y 0 is the initial label distribution where labeled nodes are represented by the one-hot vectors . The prediction label for the i-th node is qi = arg maxj Yij . LP is a simple yet successful model [ 37 ] and can be unified with GNN models owning the message-passing mechanism [ 41 ] . From Figure 2 ( a ) , we can empirically find that there is a significant correlation between the results of LP and GCN ( T/F indicates prediction is True/False ) . The LP prediction q can be viewed as the distribution of the ( labeled ) node influence [ 41 ] ( i.e . each node is mostly influenced by which class ’ s information ) ; hence the boundaries of the node influence can act as an effective reflection for the GNN model decision boundaries considering the high consistency between LP and GNN . Moreover , node influence offers a unified view of TINL and QINL : ideally , the node influence boundaries should be consistent with the true class boundaries , but both the labeled nodes ’ numbers ( QINL ) and positions ( TINL ) can cause a shift of the node influence boundaries from the true one , resulting in deviation of the model decision boundaries . Node imbalance issue is composed of topology- and quantity-imbalance . Figure 2 illustrates two examples of node influence boundary shift . In Figure 2 ( b ) , when the uniform selection is adopted to generate training set , both the quantity and the topology are imbalanced for model training ; then the large class with more total nodes ( denotes by blue color ) will own stronger influence than the small class with fewer total nodes ( denotes by red color ) due to the quantity advantage and the node influence boundary is shifted towards the small class . In Figure 2 ( c ) , when the quantity-balanced strategy is adopted for sampling training nodes , it will be easier for the small class to has more labeled nodes close to the class boundary and the boundary of the node influence is shifted into the large class . We can find that even when the training set is quantity-balanced , the topology-imbalance issue still exists and hinders the node classification learning . Hence , we can conclude that node imbalance learning is caused by the joint effect of TINL and QINL . Separately considering TINL or QINL will lead to a one-sided solution to node imbalance learning . 2.3 Measuring Topology Imbalance by Influence Conflict . Although we have realized that the imbalance of node topology interferes with model learning , how to measure the labeled node ’ s relative topological position to its class ( being far away from or close to the class center ) remains the key challenge in handling the topology-imbalance issue due to the complex graph connections and the unknown class labels for most nodes in the graph . As the nodes are homogeneously connected when constructing the graph , even nodes close to the class boundaries own similar characteristics to their neighbors . Thus it is unreliable to leverage the difference between the characteristics of one labeled node and its surrounding subgraphs to locate its topological position . Instead , we propose to utilize the node topology information by considering the node influence conflict across the whole graph and devise the Conflict Detection-based Topology Relative Location metric ( Totoro ) . Similar to Eq ( 1 ) , we calculate the Personalized PageRank [ 27 ] matrix P to measure node influence distribution from each labeled node : P = α ( I − ( 1− α ) A′ ) −1 . ( 2 ) Node influence conflict denotes topological position . According to related studies [ 41 , 19 , 2 ] , P can be viewed as the distribution of influence exerted outward from each node . We assume that if a labeled node v ∈ V encounters strong heterogeneous influence from the other classes ’ labeled nodes in the subgraph around node v where node v itself owns great influence , we have the conclusion that node v meets large influence conflict in message passing and it is close to topological class boundaries , and vice versa . Based on this hypothesis , we take the expectation of the influence conflict between the node v and the labeled nodes from other classes when node v randomly walks across the entire graph as a measurement of how topologically close node v is to the center of the class it belongs to . The Totoro value of node v is computed as : Tv = Ex∼Pv , : [ ∑ j∈ [ 1 , k ] , j 6=yv 1 |Cj | ∑ i∈Cj Pi , x ] , ( 3 ) where yv is the ground-truth label of node v , Pv indicates the personalized PageRank probability vector for the node v. A larger Totoro value Tv indicates that node v is topologically closer to class boundaries , and vice versa . The normalization item 1/|Cj | is added to make the influence from the different classes comparable when computing conflict . We visualize the node labels and the Totoro values ( scaled to [ 0 , 1 ] ) of labeled nodes in Figure 3 ( a ) . We can find that the labeled nodes with smaller Totoro values are farther away from the class boundaries , demonstrating the effectiveness of Totoro in locating the positions of labeled nodes . Besides , we sum the conflict of all the labeled nodes ∑ b∈L Tv to measure the overall conflict of the dataset , which can be viewed as the metric for the overall topology imbalance given the graph G and the training set L. Figure 3 ( b ) shows that there is a significant negative correlation between the overall conflict and the model performance , which further demonstrates the effectiveness of Totoro in measuring the intensity of topology imbalance at the dataset level .","The class imbalance problem, as an important issue in learning node representations, has drawn increasing attention from the community. Although the imbalance considered by existing studies roots from the unequal quantity of labeled examples in different classes (quantity imbalance), we argue that graph data expose a unique source of imbalance from the asymmetric topological properties of the labeled nodes, i.e., labeled nodes are not equal in terms of their structural role in the graph (topology imbalance). In this work, we first probe the previously unknown topology-imbalance issue, including its characteristics, causes, and threats to semisupervised node classification learning. We then provide a unified view to jointly analyzing the quantityand topologyimbalance issues by considering the node influence shift phenomenon with the Label Propagation algorithm. In light of our analysis, we devise an influence conflict detection–based metric Totoro to measure the degree of graph topology imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. Systematic experiments demonstrate the effectiveness and generalizability of our method in relieving topology-imbalance issue and promoting semi-supervised node classification. The further analysis unveils varied sensitivity of different graph neural networks (GNNs) to topology imbalance, which may serve as a new perspective in evaluating GNN architectures.1","This article introduces the topic of Topology-Imbalance Node Representation Learning (TINL) in graph structures. TINL focuses on the imbalance caused by the positions of labeled nodes in graphs, which interferes with the decision boundaries of trained classifiers. The article proposes a model-agnostic training framework, ReNode, that re-weights labeled nodes based on their positions. It introduces the Conflict Detection-based Topology Relative Location metric (Totoro) to measure the labeled node's relative topological position to its class and provides empirical results demonstrating the effectiveness of the proposed method.","Authors study a problem named topology imbalance which is led from the topological difference of labelled nodes in a given graph. It degrades the performance of node classifiers by 'shifting' the decision boundary. The main contributions of this paper are (1) it points out the topology imbalance problem in node classification tasks; (2) it proposes a metric (Totoro) to measure the topology imbalance; (3) it designs an annealing method to reweight samples based on their topological importance; (4) authors provide vast experimental results over multiple scenarios, datasets, and baselines.","The article discusses the problem of topology-imbalance node representation learning, which is a specific type of node imbalance that occurs when the positions of labeled nodes in a graph are asymmetric. The authors propose a model-agnostic training framework called ReNode that re-weights the labeled nodes according to their positions to address this issue. They also introduce a conflict detection-based Topology Relative Location (Totoro) metric that leverages the interaction among labeled nodes across the whole graph to locate their structural positions. The article demonstrates the effectiveness and generalizability of ReNode through empirical results in various imbalance scenarios and multiple graph neural networks.","1,0,2","0,2",1,2,0,,fluency,fluency,coverage,coherence,"1,2",0,2,1,Both B and C are good. B states the contributions of the paper clearly but C is better written.,Summary A is less fluent.,good coverage,not very coherent
26,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"1 INTRODUCTION q ( z ) p ( z ) r ( z ) Figure 1 : We propose an EBM prior using the product of a base prior p ( z ) and a reweighting factor r ( z ) , designed to bring the base prior closer to the aggregate posterior q ( z ) . Variational autoencoders ( VAEs ) ( Kingma & Welling , 2014 ; Rezende et al. , 2014 ) are one of the powerful likelihood-based generative models that have applications in image generation ( Brock et al. , 2018 ; Karras et al. , 2019 ; Razavi et al. , 2019 ) , music synthesis ( Dhariwal et al. , 2020 ) , speech generation ( Oord et al. , 2016 ; Ping et al. , 2020 ) , image captioning ( Aneja et al. , 2019 ; Deshpande et al. , 2019 ; Aneja et al. , 2018 ) , semi-supervised learning ( Kingma et al. , 2014 ; Izmailov et al. , 2020 ) , and representation learning ( Van Den Oord et al. , 2017 ; Fortuin et al. , 2018 ) . Although there has been tremendous progress in improving the expressivity of the approximate posterior , several studies have observed that VAE priors fail to match the aggregate ( approximate ) posterior ( Rosca et al. , 2018 ; Hoffman & Johnson , 2016 ) . This phenomenon is sometimes described as holes in the prior , referring to regions in the latent space that are not decoded to data-like samples . Such regions often have a high density under the prior but have a low density under the aggregate approximate posterior . The prior hole problem is commonly tackled by increasing the flexibility of the prior via hierarchical priors ( Klushyn et al. , 2019 ) , autoregressive models ( Gulrajani et al. , 2016 ) , a mixture of approximate posteriors ( Tomczak & Welling , 2018 ) , normalizing flows ( Xu et al. , 2019 ; Chen et al. , 2016 ) , resampled priors ( Bauer & Mnih , 2019 ) , and energy-based models ( Pang et al. , 2020 ; Vahdat et al. , 2018b ; a ; 2020 ) . Among them , energy-based models ( EBMs ) ( Du & Mordatch , 2019 ; Pang et al. , 2020 ) have shown promising results in learning expressive priors . However , they require running iterative MCMC steps during training which is computationally expensive , especially when the energy function is represented by a neural network . Moreover , they scale poorly to hierarchical models where an EBM is defined on each group of latent variables . Our key insight in this paper is that a trainable prior is brought as close as possible to the aggregate posterior as a result of training a VAE . The mismatch between the prior and the aggregate posterior can be reduced by simply reweighting the prior to re-adjust its likelihood in the area of mismatch with the aggregate posterior . To represent this reweighting mechanism , we formulate the prior using an EBM that is defined by the product of a reweighting factor and a base trainable prior as shown in Fig . 1 . We represent the reweighting factor using neural networks and the base prior using Normal distributions . Instead of expensive MCMC sampling , we use noise contrastive estimation ( NCE ) ( Gutmann & Hyvärinen , 2010 ) for training the EBM prior . We show that NCE naturally trains the reweighting factor in our prior by learning a binary classifier to distinguish samples from a target distribution ( i.e. , samples from the approximate posterior ) vs. samples from a noise distribution ( i.e. , the base trainable prior ) . However , since NCE ’ s success depends on how close the noise distribution is to the target distribution , we first train the VAE with the base prior to bring it close to the aggregate posterior . And then , we train the EBM prior using NCE . In this paper , we make the following contributions : i ) We propose an EBM prior termed noise contrastive prior ( NCP ) which is trained by contrasting samples from the aggregate posterior to samples from a base prior . NCPs are learned as a post-training mechanism to replace the original prior with a more flexible prior , which can improve the generative performance of VAEs with any structure . ii ) We also show how NCPs are trained on hierarchical VAEs with many latent variable groups . We show that training hierarchical NCPs scales easily to many groups , as they are trained for each latent variable group in parallel . iii ) Finally , we demonstrate that NCPs improve the generative quality of VAEs by a large margin across datasets . 2 BACKGROUND . We first review VAEs , their extension to hierarchical VAEs , and the prior hole problem . Variational Autoencoders : VAEs learn a generative distribution p ( x , z ) = p ( z ) p ( x|z ) where p ( z ) is a prior distribution over the latent variable z and p ( x|z ) is a likelihood function that generates the data x given z. VAEs are trained by maximizing a variational lower bound on the log-likelihood log p ( x ) : log p ( x ) ≥ Ez∼q ( z|x ) [ log p ( x|z ) ] − KL ( q ( z|x ) ||p ( z ) ) : = LVAE ( x ) , ( 1 ) where q ( z|x ) is an approximate posterior and KL is the Kullback–Leibler divergence . The final training objective is formulated by Epd ( x ) [ LVAE ( x ) ] where pd ( x ) is the data distribution ( Kingma & Welling , 2014 ) . Hierarchical VAEs ( HVAEs ) : To increase the expressivity of both prior and approximate posterior , earlier work adapted a hierarchical latent variable structure ( Vahdat & Kautz , 2020 ; Kingma et al. , 2016 ; Sønderby et al. , 2016 ; Gregor et al. , 2016 ) . In HVAEs , the latent variable z is divided into K separate groups , z = { z1 , . . . , zK } . The approximate posterior and the prior distributions are then defined by q ( z|x ) = ∏K k=1 q ( zk|z < k , x ) and p ( z ) = ∏K k=1 p ( zk|z < k ) . Using these , the training objective becomes : LHVAE ( x ) : = Eq ( z|x ) [ log p ( x|z ) ] − K∑ k=1 Eq ( z < k|x ) [ KL ( q ( zk|z < k , x ) ||p ( zk|z < k ) ) ] , ( 2 ) where q ( z < k|x ) = ∏k−1 i=1 q ( zi|z < i , x ) is the approximate posterior up to the ( k − 1 ) th group1 . The Prior Hole Problem : Let q ( z ) , Epd ( x ) [ q ( z|x ) ] denote the aggregate ( approximate ) posterior . In Appendix B.1 , we show that maximizing Epd ( x ) [ LVAE ( x ) ] with respect to the prior parameters corresponds to bringing the prior as close as possible to the aggregate posterior by minimizing KL ( q ( z ) ||p ( z ) ) w.r.t . p ( z ) . Formally , the prior hole problem refers to the phenomenon that p ( z ) fails to match q ( z ) . 1For k = 1 , the expectation inside the summation is simplified to KL ( q ( z1|x ) ||p ( z1 ) ) . 3 NOISE CONTRASTIVE PRIORS ( NCPS ) . One of the main causes of the prior hole problem is the limited expressivity of the prior that prevents it from matching the aggregate posterior . Recently , energy-based models have shown promising results in representing complex distributions . Motivated by their success , we introduce the noise contrastive prior ( NCP ) pNCP ( z ) = 1Z r ( z ) p ( z ) , where p ( z ) is a base prior distribution , e.g. , a Normal , r ( z ) is a reweighting factor , and Z = ∫ r ( z ) p ( z ) dz is the normalization constant . The function r : Rn → R+ maps the latent variable z ∈ Rn to a positive scalar , and can be implemented using neural nets . The reweighting factor r ( z ) can be trained using MCMC sampling as discussed in Appendix A . However , MCMC requires expensive sampling iterations that scale poorly to hierarchical VAEs . To address this , we describe a noise contrastive estimation based approach to train pNCP ( z ) without MCMC sampling . 3.1 LEARNING THE REWEIGHTING FACTOR WITH NOISE CONTRASTIVE ESTIMATION . Recall that training VAEs closes the gap between the prior and the aggregate posterior by minimizing KL ( q ( z ) ||p ( z ) ) with respect to prior . Assuming the base prior p ( z ) to be fixed , KL ( q ( z ) ||pNCP ( z ) ) is zero when r ( z ) = q ( z ) /p ( z ) . However , since we do not have the density function for q ( z ) , we can not compute the ratio explicitly . Instead , in this paper , we propose to estimate r ( z ) using noise contrastive estimation ( Gutmann & Hyvärinen , 2010 ) , also known as the likelihood ratio trick that has been popularized in machine learning by predictive coding ( Oord et al. , 2018 ) and generative adversarial networks ( GANs ) ( Goodfellow et al. , 2014 ) . Since , we can generate samples from both p ( z ) and q ( z ) 2 , we train a binary classifier to distinguish samples from q ( z ) and samples from the base prior p ( z ) by minimizing the binary cross-entropy loss : min D − Ez∼q ( z ) [ logD ( z ) ] − Ez∼p ( z ) [ log ( 1−D ( z ) ) ] . ( 3 ) Here , D : Rn → ( 0 , 1 ) is a binary classifier that generates the classification prediction probabilities . Eq . ( 3 ) is minimized when D ( z ) = q ( z ) q ( z ) +p ( z ) . Denoting the classifier at optimality by D ∗ ( z ) , we estimate the reweighting factor r ( z ) = q ( z ) p ( z ) ≈ D∗ ( z ) 1−D∗ ( z ) . The appealing advantage of this estimator is that it is obtained by simply training a binary classifier rather than using expensive MCMC sampling . 3.2 TWO-STAGE TRAINING FOR NOISE CONTRASTIVE PRIORS . To properly learn the reweighting factor , NCE training requires the base prior distribution to be close to the target distribution . Intuitively , if p ( z ) is very close to q ( z ) ( i.e. , p ( z ) ≈ q ( z ) ) , the optimal classifier will have a large loss value in Eq . ( 3 ) , and we will have r ( z ) ≈ 1 . If p ( z ) is instead far from q ( z ) , the binary classifier will easily learn to distinguish samples from the two distributions and it will not learn the likelihood ratios correctly . If p ( z ) is roughly close to q ( z ) , then the binary classifier can learn the ratios . To ensure that the base prior distribution is close to the target aggregate posterior distribution , we propose a two-stage training algorithm . In the first stage , we train the VAE with only the base prior p ( z ) . From Appendix B.1 , we know that at the end of training , p ( z ) is as close as possible to q ( z ) . In the second stage , we freeze the VAE model including the approximate posterior q ( z|x ) , the base prior p ( z ) , and the likelihood p ( x|z ) , and we only train the reweighting factor r ( z ) using Eq . ( 3 ) . The second stage can be thought of as replacing the base distribution p ( z ) with a more expressive distribution of the form pNCP ( z ) ∝ r ( z ) p ( z ) . Hence , NCP matches the prior to the aggregate posterior q ( z ) by using the reweighting factors . Note that our proposed method is generic as it only assumes that we can draw samples from q ( z ) and p ( z ) , which applies to any VAE . Our training is illustrated in Fig . 2 . 2We generate samples from the aggregate posterior q ( z ) = Epd ( x ) [ q ( z|x ) ] via ancestral sampling : draw data from the training set ( x ∼ pd ( x ) ) and then sample from z ∼ q ( z|x ) .","Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in various domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs’ poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.","The authors highlight an important problem in VAE - the prior-hole problem - which is that the approximate posterior and the simple gaussian prior do not match in spite of the KL term in the ELBO which makes sampling an issue - leading to the prior putting probability mass on latents that are not decoded to high probability mass regions in data manifold. Prior approaches have overcome this problem by increasing the expressivity of the prior through autoregressive models, and/or using hierarchical latents, EBMs with MCMC sampling. This paper proposes a very simple two stage method - (1) train a regular VAE, (2) train a binary classifier in NCE style to distinguish samples from prior and approx. posterior; use the re-weighting term from the NCE score to sample from a better re-weighted prior - either through langevin dynamics or re-sampling. The authors combine this approach with the use of hierarchical latents and produce really good performing generative models on a host of benchmarks with good looking samples.","In this paper, the authors propose a new approach to address the prior hole problem in variational autoencoders (VAEs) by introducing a trainable prior termed noise contrastive prior (NCP). The NCP is learned by contrasting samples from the approximate posterior to samples from a base prior, and is trained using noise contrastive estimation. The authors show that the NCP can be used to improve the generative quality of VAEs across various datasets. They also demonstrate that the NCP can be easily scaled to hierarchical VAEs with many latent variable groups. The proposed method is generic and only requires the ability to draw samples from the approximate posterior and the base prior. The authors illustrate the training process of the NCP in a two-stage manner, where the first stage involves training the VAE with the base prior and the second stage involves training the reweighting factor using noise contrastive estimation.","The article proposes a Noise Contrastive Prior (NCP) as a technique to improve the generative performance of Variational Autoencoders (VAEs). NCP uses a product of a base prior and a reweighting factor to represent the prior, which is trained by contrasting samples from the approximate posterior with samples from a base prior. The trainability of the prior allows it to be adjusted quickly, resolving the issue of the ""prior hole problem,"" where holes in the prior create regions in latent space that are not sampled by data-like samples. The reweighting factor, represented by neural networks, is trained using Noise Contrastive Estimation (NCE), which requires binary classification of samples from the target distribution and samples from the noise distribution. The prior is trained via a two-stage algorithm, where the VAE is first trained with only the base prior, allowing the NCE stage to replace the base prior with the more flexible NCP. The article shows the effectiveness of NCP across datasets compared to other techniques that failed to address the prior hole. Finally, the authors demonstrate that NCP can easily scale up for hierarchical VAEs with many latent variable groups.","0,2,1",1,2,1,2,,tone,coverage,"fluency, coverage",coverage,2,1,2,1,"Both C and A are good, but A uses a more informal language.",Summary B does not motivate the problem (prior hole problem),fluent and good coverage,no information about results
27,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"1 INTRODUCTION . Q-learning ( Watkins & Dayan , 1992 ; Sutton & Barto , 2018 ) lies at the heart of many of the recent successes of deep reinforcement learning ( RL ) ( Mnih et al. , 2015 ; Silver et al. , 2016 ) , with recent advancements ( e.g. , van Hasselt ( 2010 ) ; Bellemare et al . ( 2017 ) ; Wang et al . ( 2016 ) ; Hessel et al . ( 2017 ) ) helping to make it among the most widely used methods in applied RL . Despite these successes , many properties of Q-learning are poorly understood , and it is challenging to successfully apply deep Q-learning in practice . When combined with function approximation , Q-learning can become unstable ( Baird , 1995 ; Boyan & Moore , 1995 ; Tsitsiklis & Roy , 1996 ; Sutton & Barto , 2018 ) . Various modifications have been proposed to improve convergence or approximation error ( Gordon , 1995 ; 1999 ; Szepesvári & Smart , 2004 ; Melo & Ribeiro , 2007 ; Maei et al. , 2010 ; Munos et al. , 2016 ) ; but it remains difficult to reliably attain both robustness and scalability . Recently , Lu et al . ( 2018 ) identified a source of error in Q-learning with function approximation known as delusional bias . It arises because Q-learning updates the value of state-action pairs using estimates of ( sampled ) successor-state values that can be mutually inconsistent given the policy class induced by the approximator . This can result in unbounded approximation error , divergence , policy cycling , and other undesirable behavior . To handle delusion , the authors propose a policy-consistent backup operator that maintains multiple Q-value estimates organized into information sets . Each information set has its own backed-up Q-values and corresponding “ policy commitments ” responsible for inducing these values . Systematic management of these sets ensures that only consistent choices of maximizing actions are used to update Q-values . All potential solutions are tracked to prevent premature convergence on any specific policy commitments . Unfortunately , the proposed algorithms use tabular representations of Q-functions , so while this establishes foundations for delusional bias , the function approximator is used neither for generalization nor to manage the size of the state/action space . Consequently , this approach is not scalable to RL problems of practical size . In this work , we propose CONQUR ( CONsistent Q-Update Regression ) , a general framework for integrating policy-consistent backups with regression-based function approximation for Q-learning and for managing the search through the space of possible regressors ( i.e. , information sets ) . With suitable search heuristics , our framework provides a computationally effective means for minimizing the effects of delusional bias in Q-learning , while admitting scaling to practical problems . Our main contributions are as follows . First we define novel augmentations of standard Q-regression to increase the degree of policy consistency across training batches . While testing exact consistency is expensive , we introduce an efficient soft-consistency penalty that promotes consistency of new labels with earlier policy commitments . Second , drawing on the information-set structure of Lu et al . ( 2018 ) , we define a search space over Q-regressors to allow consideration of multiple sets of policy commitments . Third , we introduce heuristics for guiding the search over regressors , which is critical given the combinatorial nature of information sets . Finally , we provide experimental results on the Atari suite ( Bellemare et al. , 2013 ) demonstrating that CONQUR can offer ( sometimes dramatic ) improvements over Q-learning . We also show that ( easy-to-implement ) consistency penalization on its own ( i.e. , without search ) can improve over both standard and double Q-learning . 2 BACKGROUND . We assume a discounted , infinite horizon Markov decision process ( MDP ) , M = ( S , A , P , p0 , R , γ ) . The state space S can reflect both discrete and continuous features , but we take the action space A to be finite ( and practically enumerable ) . We consider Q-learning with a function approximator Qθ to learn an ( approximately ) optimal Q-function ( Watkins , 1989 ; Sutton & Barto , 2018 ) , drawn from some approximation class parameterized by Θ ( e.g. , the weights of a neural network ) . When the approximator is a deep network , we generically refer to the algorithm as DQN , the method at the heart of many recent RL successes ( Mnih et al. , 2015 ; Silver et al. , 2016 ) . For online Q-learning , at a transition s , a , r , s′ , the Q-update is given by : θ ← θ + α ( r + γmax a′∈A Qθ ( s ′ , a′ ) −Qθ ( s , a ) ) ∇θQθ ( s , a ) . ( 1 ) Batch versions of Q-learning , including DQN , are similar , but fit a regressor repeatedly to batches of training examples ( Ernst et al. , 2005 ; Riedmiller , 2005 ) . Batch methods are usually more data efficient and stable than online Q-learning . Abstractly , batch Q-learning works through a sequence of ( possibly randomized ) data batches D1 , · · ·DT to produce a sequence of regressors Qθ1 , . . . , QθT = Qθ , estimating the Q-function.1 For each ( s , a , r , s′ ) ∈ Dk , we use a prior estimator Qθk−1 to bootstrap the Q-label q = r + γmaxa′ Qθk−1 ( s ′ , a′ ) . We then fit Qθk to this training data using a suitable regression procedure with an appropriate loss function . Once trained , the ( implicit ) induced policy πθ is the greedy policy w.r.t . Qθ , i.e. , πθ ( s ) = arg maxa∈AQθ ( s , a ) . Let F ( Θ ) , resp . G ( Θ ) , be the corresponding class of expressible Q-functions , resp . greedy policies . Intuitively , delusional bias occurs whenever a backed-up value estimate is derived from action choices that are not ( jointly ) realizable in G ( Θ ) ( Lu et al. , 2018 ) . Standard Q-updates back up values for each ( s , a ) pair by independently choosing maximizing actions at the corresponding next states s′ . However , such updates may be “ inconsistent ” under approximation : if no policy in G ( Θ ) can jointly express all past action choices , backed up values may not be realizable by any expressible policy . Lu et al . ( 2018 ) show that delusion can manifest itself with several undesirable consequences . Most critically , it can prevent Q-learning from learning the optimal representable policy in G ( Θ ) ; it can also cause divergence . To address this , they propose a non-delusional policy consistent Q-learning ( PCQL ) algorithm that provably eliminates delusion . We refer to the original paper for details , but review the main concepts we need to consider below . The first key concept is that of policy consistency . For any S ⊆ S , an action assignment σS : S → A associates an action σ ( s ) with each s ∈ S. We say σ is policy consistent if there is a greedy policy π ∈ G ( Θ ) s.t . π ( s ) = σ ( s ) for all s ∈ S. We sometimes equate a set SA of state-action pairs with an implied assignment π ( s ) = a for all ( s , a ) ∈ SA . If SA contains multiple pairs with the same state s , but different actions a , it is a multi-assignment ( though we loosely use the term “ assignment ” in both cases when there is no risk of confusion ) . In ( batch ) Q-learning , each successive regressor uses training labels generated by assuming maximizing actions ( under the prior regressor ) are taken at its successor states . Let σk reflect the collection of states and corresponding maximizing actions taken to generate labels for regressor Qθk ( assume it is policy consistent ) . Suppose we train Qθk by bootstrapping on Qθk−1 and consider a training sample ( s , a , r , s′ ) . Q-learning generates label r + γmaxa′ Qθk−1 ( s ′ , a′ ) for input ( s , a ) . Notice , 1We describe our approach using a straightforward form of batch Q-learning , but it can accommodate many variants , e.g. , where the regressor used for bootstrapping is some earlier Q-estimator , or the estimators generating the max-actions and the value estimates are different as in double Q-learning ( van Hasselt , 2010 ; Hasselt et al. , 2016 ) ; indeed , we experiment with such variants . however , that taking action a∗ = argmaxa′ Qθk ( s ′ , a′ ) at s′ may not be policy consistent with σk . Thus Q-learning will estimate a value for ( s , a ) assuming the execution of a policy that can not be realized given the limitations of the approximator . The PCQL algorithm ( Lu et al. , 2018 ) prevents this by insisting that any action assignment σ used to generate bootstrapped labels is consistent with earlier assignments . Notice that this means Q-labels will often not be generated using maximizing actions relative to the prior regressor . The second key concept is that of information sets . One will generally not be able to use maximizing actions to generate labels , so tradeoffs can be made when deciding which actions to assign to different states . Indeed , even if it is feasible to assign a maximizing action a to state s early in training , say at batch k , since it may prevent assigning a maximizing a′ to s′ later , say batch k + ` , we may want to consider a different assignment to s to give more flexibility to maximize at other states later . PCQL doesn ’ t try to anticipate the tradeoffs—rather it maintains multiple information sets , each corresponding to a different assignment to the states seen in the training data so far . Each gives rise to a different Q-function estimate , resulting in multiple hypotheses . At the end of training , the best hypothesis is the one maximizing expected value w.r.t . an initial state distribution . PCQL provides strong convergence guarantees , but it is a tabular algorithm : the function approximator retricts the policy class , but is not used to generalize Q-values . Furthermore , its theoretical guarantees come at a cost : it uses exact policy consistency tests—tractable for linear approximators , but not practical for large problems ; and it maintains all consistent assignments . As a result , PCQL can not be used for large RL problems of the type tackled by DQN . 3 THE CONQUR FRAMEWORK . We develop the CONQUR framework to provide a practical approach to reducing delusion in Qlearning , specifically addressing the limitations of PCQL identified above . CONQUR consists of three main components : a practical soft-constraint penalty that promotes policy consistency ; a search space to structure the search over multiple regressors ( information sets , action assignments ) ; and heuristic search schemes ( expansion , scoring ) to find good Q-regressors . 3.1 PRELIMINARIES . We assume a set of training data consisting of quadruples ( s , a , r , s′ ) , divided into ( possibly nondisjoint ) batches D1 , . . . DT for training . This perspective is quite general : online RL corresponds to |Di| = 1 ; off-line batch training ( with sufficiently exploratory data ) corresponds to a single batch ( i.e. , T = 1 ) ; and online or batch methods with replay are realized when the Di are generated by sampling some data source with replacement . For any data batch D , let χ ( D ) = { s′ : ( s , a , r , s′ ) ∈ D } denote the collection of successor states of D. An action assignment σD for D is an assignment ( or multi-assignment ) from χ ( D ) to A : this dictates which action σD ( s′ ) is considered “ maximum ” for the purpose of generating a Q-label for pair ( s , a ) ; i.e. , ( s , a ) will be assigned training label r + γQ ( s′ , σ ( s′ ) ) rather than r + γmaxa′∈AQ ( s ′ , a′ ) . The set of all such assignments is Σ ( D ) = Aχ ( D ) ; note that it grows exponentially with |D| . Given Q-function parameterization Θ , we say σD is Θ-consistent ( w.r.t . D ) if there is some θ ∈ Θ s.t . πθ ( s′ ) = σ ( s′ ) for all s′ ∈ χ ( D ) .2 This is simple policy consistency , but with notation that emphasizes the policy class . Let ΣΘ ( D ) denote the set of all Θ-consistent assignments over D. The union σ1 ∪ σ2 of two assignments ( over D1 , D2 , resp . ) is defined in the usual way .","Delusional bias is a fundamental source of error in approximate Q-learning. To date, the only techniques that explicitly address delusion require comprehensive search using tabular value estimates. In this paper, we develop efficient methods to mitigate delusional bias by training Q-approximators with labels that are “consistent” with the underlying greedy policy class. We introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class. We also propose a search framework that allows multiple Q-approximators to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-learning in a variety of Atari games, sometimes dramatically.","The CONQUR framework proposes a means of minimizing 'delusional bias', a source of error that arises when Q-learning algorithms update the value of state-action pairs using estimates of successor-state values that may be mutually inconsistent given the policy class induced by an approximator. The framework introduces a 'soft-consistency' penalty that promotes the consistency of new labels with earlier policy commitments, and defines a search space over 'Q-regressors' to allow consideration of multiple sets of policy commitments. The authors provide heuristics for guiding the search over regressors, which is critical given the combinatorial nature of information sets, and provide experimental evidence suggesting CONQUR can be more effective than standard Q-learning. The Q-learning algorithm remains widely used in deep reinforcement learning, and researchers have long sought to improve its properties, as it can become unstable when combined with function approximation, making it challenging to apply in practice. The algorithm can result in unbounded approximation error, divergence, policy cycling and other undesirable effects.","The article discusses the problem of delusion in Q-learning, which can occur when a Q-learning algorithm updates its value estimates based on inconsistent samples. The authors propose a new framework called CONQUR, which integrates policy-consistent backups with regression-based function approximation to minimize the effects of delusion in Q-learning. CONQUR uses a search space to guide the search over multiple regressors, and heuristics to efficiently search for good Q-regressors. The article demonstrates the effectiveness of CONQUR through experimental results on the Atari suite, showing that it can offer improvements over standard Q-learning and easy-to-implement consistency penalization alone.","A recent paper by Lu et al introduced delusional bias in Q-learning, an error due to the max in the Bellman backup not being consistent with the policy representation implied by the greedy operator applied to the approximated value function. That work proposed a consistent algorithm for small and finite state spaces, which essentially enumerates over realizable policies. This paper proposes an algorithm for overcoming delusional bias in large state spaces. The idea is to add to the Q-learning objective a smooth penalty term that induces approximate consistency, and search over possible Q-function approximators. Several heuristic methods are proposed for this search, and results are demonstrated in Atari domains.","1,2,0",,,,,,,,,,,,,,,,,
28,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"1 INTRODUCTION . Recently , deep reinforcement learning ( RL ) has achieved multiple breakthroughs in a range of challenging domains ( e.g . Silver et al . ( 2016 ) ; Berner et al . ( 2019 ) ; Andrychowicz et al . ( 2020b ) ; Vinyals et al . ( 2019 ) ) . A part of this success is related to an ever-growing toolbox of tricks and methods that were observed to boost the RL algorithms ’ performance ( e.g . Hessel et al . ( 2018 ) ; Haarnoja et al . ( 2018b ) ; Fujimoto et al . ( 2018 ) ; Wang et al . ( 2020 ) ; Osband et al . ( 2019 ) ) . This state of affairs benefits the field but also brings challenges related to often unclear interactions between the individual improvements and the credit assignment related to the overall performance of the algorithm Andrychowicz et al . ( 2020a ) ; Ilyas et al . ( 2020 ) . In this paper , we present a comprehensive empirical study of multiple tools from the RL toolbox applied to the continuous control in the OpenAI Gym MuJoCo setting . These are presented in Section 4 and Appendix B . Our insights include : • The normally distributed action noise , commonly used for exploration , hinders training . • The current state-of-the-art methods are unstable under several stability criteria . • The critics ’ initialization plays a major role in ensemble-based actor-critic exploration , while the training is mostly invariant to the actors ’ initialization . • The approximated posterior sampling exploration ( Osband et al. , 2013 ) outperforms approx- imated UCB exploration combined with weighted Bellman backup ( Lee et al. , 2020 ) . • The weighted Bellman backup ( Lee et al. , 2020 ) can not replace the clipped double Q- Learning ( Fujimoto et al. , 2018 ) . To address some of the issues listed above , we introduce the Ensemble Deep Deterministic Policy Gradient ( ED2 ) algorithm1 , see Section 3 . ED2 brings together existing RL tools in a novel way : it is 1Our code is based on SpinningUp ( Achiam , 2018 ) . We open-source it at : https : //github.com/ ed2-paper/ED2 . an off-policy algorithm for continuous control , which constructs an ensemble of streamlined versions of TD3 agents and achieves the state-of-the-art performance in OpenAI Gym MuJoCo , substantially improving the results on the two hardest tasks – Ant and Humanoid . Consequently , ED2 does not require knowledge outside of the existing RL toolbox , is conceptually straightforward , and easy to code . 2 BACKGROUND . We model the environment as a Markov Decision Process ( MDP ) . It is defined by the tuple ( S , A , R , P , γ , p0 ) , where S is a continuous multi-dimensional state space , A denotes a continuous multi-dimensional action space , P is a transition kernel , γ ∈ [ 0 , 1 ) stands for a discount factor , p0 refers to an initial state distribution , and R is a reward function . The agent learns a policy from sequences of transitions τ = [ ( st , at , rt , st+1 , d ) ] Tt=0 , called episodes or trajectories , where at ∼ π ( ·|st ) , st+1 ∼ P ( ·|st , at ) , rt = R ( st , at , st+1 ) , d is a terminal signal , and T is the terminal time-step . A stochastic policy π ( a|s ) maps each state to a distribution over actions . A deterministic policy µ : S −→ A assigns each state an action . All algorithms that we consider in this paper use a different policy for collecting data ( exploration ) and a different policy for evaluation ( exploitation ) . In order to keep track of the progress , the evaluation runs are performed every ten thousand environment interactions . Because of the environments ’ stochasticity , we run the evaluation policy multiple times . Let { Ri } Ni=1 be a set of ( undiscounted ) returns from N evaluation episodes { τi } Ni=1 , i.e . Ri = ∑ rt∈τi rt . We evaluate the policy using the average test return R̄ = 1N ∑N i=1Ri and the standard deviation of the test returns σ = √ 1 N−1 ∑N i=1 ( Ri − R̄ ) 2 . We run experiments on four continuous control tasks and their variants , introduced in the appropriate sections , from the OpenAI Gym MuJoCo suite ( Brockman et al. , 2016 ) presented in Figure 1 . The agent observes vectors that describe the kinematic properties of the robot and its actions specify torques to be applied on the robot joints . See Appendix D for the details on the experimental setup . 3 ENSEMBLE DEEP DETERMINISTIC POLICY GRADIENTS . For completeness of exposition , we present ED2 before the experimental section . The ED2 architecture is based on an ensemble of Streamlined Off-Policy ( SOP ) agents ( Wang et al. , 2020 ) , meaning that our agent is an ensemble of TD3-like agents ( Fujimoto et al. , 2018 ) with the action normalization and the ERE replay buffer . The pseudo-code listing can be found in Algorithm 1 , while the implementation details , including a more verbose version of pseudo-code ( Algorithm 3 ) , can be found in Appendix E. In the data collection phase ( Lines 1-9 ) , ED2 selects one actor from the ensemble uniformly at random ( Lines 1 and 9 ) and run its deterministic policy for the course of one episode ( Line 4 ) . In the evaluation phase ( not shown in Algorithm 1 ) , the evaluation policy averages all the actors ’ output actions . We train the ensemble every 50 environment steps with 50 stochastic gradient descent updates ( Lines 10-13 ) . ED2 concurrently learns K · 2 Q-functions , Qφk,1 and Qφk,2 where k ∈ K , by mean square Bellman error minimization , in almost the same way that SOP learns its two Q-functions . The only difference is that we have K critic pairs that are initialized with different random weights and then trained independently with the same batches of data . Because of the different initial weights , each Q-function has a different bias in its Q-values . The K actors , πθk , train maximizing their corresponding first critic , Qφk,1 , just like SOP . Algorithm 1 ED2 - Ensemble Deep Deterministic Policy Gradients Input : init . params for policy θk and Q-functions φk,1 , φk,2 , k ∈ [ 1 ... K ] ; replay buffer D ; 1 : Sample the current policy index c ∼ U ( [ 1 ... K ] ) . 2 : Reset the environment and observe the state s. 3 : repeat 4 : Execute action a = µθc ( s ) . µ uses the action normalization 5 : Observe and store ( s , a , r , s′ , d ) in the replay buffer D. 6 : Set s← s′ 7 : if episode is finished then 8 : Reset the environment and observe initial state s. 9 : Sample the current policy index c ∼ U ( [ 1 ... K ] ) . 10 : if time to update then 11 : for as many as steps done in the environment do 12 : Sample a batch of transitions B = { ( s , a , r , s′ , d ) } ⊂ D . uses ERE 13 : Update the parameters θk , φk,1 and φk,2 by one gradient step . 14 : until convergence Utilizing the ensembles requires several design choices , which we summarize below . The ablation study of ED2 elements is provided in Appendix C . Ensemble . Used : We train the ensemble of 5 actors and 5 critics ; each actor learns from its own critic and the whole ensemble is trained on the same data . Not used : We considered different actor-critic configurations , initialization schemes and relations , as well as the use of random prior networks ( Osband et al. , 2018 ) , data bootstrap ( Osband et al. , 2016 ) , and different ensemble sizes . We also change the SOP network sizes and training intensity instead of using the ensemble . Besides the prior networks in some special cases , these turn out to be inferior as shown in Section 4 and Appendix B.1 . Exploration . Used : We pick one actor uniformly at random to collect the data for the course of one episode . The actor is deterministic ( no additive action noise is applied ) . These two choices ensure coherent and temporally-extended exploration similarly to Osband et al . ( 2016 ) . Not used : We tested several approaches to exploration : using the ensemble of actors , UCB ( Lee et al. , 2020 ) , and adding the action noise in different proportions . These experiments are presented in Appendix B.2 . Exploitation . Used : The evaluation policy averages all the actors ’ output actions to provide stable performance . Not used : We tried picking an action with the biggest value estimate ( average of the critics ’ Qfunctions ) in evaluation ( Huang et al. , 2017 ) . Interestingly , both policies had similar results , see Appendix B.3 . Action normalization . Used : We use the action normalization introduced by Wang et al . ( 2020 ) . Not used : We experimented with the observations and rewards normalization , which turned out to be unnecessary . The experiments are presented in Appendix B.4 . Q-function updates . Used : We do 50 SGD updates ( ADAM optimizer ( Kingma and Ba , 2015 ) , MSE loss ) to the actors and the critics every 50 environment interactions , use Clipped Double Q-Learning ( Fujimoto et al. , 2018 ) . Not used : We also examined doing the updates at the end of each episode ( with the proportional number of updates ) , using the Hubert loss , and doing weighted Bellman backups ( Lee et al. , 2020 ) . However , we found them to bring no improvement to our method , as presented in Appendix B.5 . 4 EXPERIMENTS . In this section , we present our comprehensive study and the resulting insights . The rest of the experiments verifying that our design choices perform better than alternatives are in Appendix B . Unless stated otherwise , a solid line in the figures represents an average , while a shaded region shows a 95 % bootstrap confidence interval . We used 30 seeds for ED2 and the baselines and 7 seeds for the ED2 variants . 4.1 THE NORMALLY DISTRIBUTED ACTION NOISE , COMMONLY USED FOR EXPLORATION , HINDERS TRAINING . In this experiment , we deprive SOP of its exploration mechanism , namely additive normal action noise , and call this variant deterministic SOP ( det . SOP ) . It causes relatively minor deterioration in the Humanoid performance , has no significant influence on the Hopper or Walker performance , and substantially improves the Ant performance , see Figure 2 . This result shows that no additional exploration mechanism , often in a form of an exploration noise ( Lillicrap et al. , 2016 ; Fujimoto et al. , 2018 ; Wang et al. , 2020 ) , is required for the diverse data collection and it can even hinder training . ED2 leverages this insight and constructs an ensemble of deterministic SOP agents presented in Section 3 . Figure 3 shows that ED2 magnifies the beneficial effect coming from the deterministic exploration . ED2 achieves state-of-the-art performance on the OpenAI Gym MuJoCo suite . Figure 4 shows the results of ED2 contrasted with three strong baselines : SUNRISE ( Lee et al. , 2020 ) , SOP ( Wang et al. , 2020 ) , and SAC ( Haarnoja et al. , 2018b ) . For completeness , we plot the Humanoid velocities in Figure 5 which shows that our method accelerates to a much higher velocity than the baselines .","The growth of deep reinforcement learning (RL) has brought multiple exciting tools and methods to the field. This rapid expansion makes it important to understand the interplay between individual elements of the RL toolbox. We approach this task from an empirical perspective by conducting a study in the continuous control setting. We present multiple insights of fundamental nature, including: a commonly used additive action noise is not required for effective exploration and can even hinder training; the performance of policies trained using existing methods varies significantly across training runs, epochs of training, and evaluation runs; the critics’ initialization plays the major role in ensemble-based actor-critic exploration, while the training is mostly invariant to the actors’ initialization; a strategy based on posterior sampling explores better than the approximated UCB combined with the weighted Bellman backup; the weighted Bellman backup alone cannot replace the clipped double Q-Learning. As a conclusion, we show how existing tools can be brought together in a novel way, giving rise to the Ensemble Deep Deterministic Policy Gradients (ED2) method, to yield state-of-the-art results on continuous control tasks from OpenAI Gym MuJoCo. From the practical side, ED2 is conceptually straightforward, easy to code, and does not require knowledge outside of the existing RL toolbox.","This paper conducted an experimental study over a range of tricks that are often exploited to facilitate ensemble deep reinforcement learning. The experiment results show several interesting findings. For example, it was found that commonly used additive action noise may not be necessary for effective exploration. Meanwhile, experiments show that the initialization of critics perhaps has a higher impact on learning performance than the initialization methods adopted for actors. These findings can be quite important to guide future design of more effective ensemble reinforce learning algorithms.","Researchers have made significant progress in deep reinforcement learning (RL) in recent years, but there are still challenges related to understanding the interactions between individual improvements and credit assignment. To address these challenges, the authors present a comprehensive empirical study of multiple tools from the RL toolbox applied to continuous control tasks in the OpenAI Gym MuJoCo setting. They find that the normally distributed action noise, commonly used for exploration, hinders training and that the current state-of-the-art methods are unstable under several stability criteria. The authors introduce the Ensemble Deep Deterministic Policy Gradient (ED2) algorithm, which constructs an ensemble of streamlined versions of TD3 agents and achieves state-of-the-art performance on the OpenAI Gym MuJoCo suite.","The article presents Ensemble Deep Deterministic Policy Gradient (ED2), an algorithm that brings together multiple tools from the deep reinforcement learning (RL) toolbox and achieves state-of-the-art performance in continuous control tasks. Through a comprehensive empirical study, the authors offer various insights, such as deterministic exploration being sufficient for diverse data collection and the initialization of critics playing a major role in ensemble-based actor-critic exploration. The study also highlights the limitations of some state-of-the-art methods and provides recommendations to address those limitations. Overall, ED2 is easy to code, requires no knowledge outside of the existing RL toolbox and can be applied to a variety of challenging domains.","0,2,1","0,1,2",,1,2,,tie,,"coverage, fluency",redundancy,"0,1,2",,2,1,"All summaries seem good, each with their own style and slightly different focus.",See above.,good coverage and fluent,repetitive
29,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"1 Introduction . As we use machine learning models in daily tasks , such as medical diagnostics [ 6 , 19 ] , speech assistants [ 31 ] etc. , being able to trust the predictions being made has become increasingly important . To understand the underlying reasoning process of complex machine learning models a sub-field of explainable artificial intelligence ( XAI ) [ 2 , 17 , 36 ] called local explanations , has seen promising results [ 35 ] . Local explanation methods [ 27 , 39 ] often approximate an underlying black box model by fitting an interpretable proxy , such as a linear model or tree , around the neighborhood of individual predictions . These methods have the advantage of being model-agnostic and locally interpretable . Traditionally , off-the-shelf local explanation frameworks , such as the Shapley value in game theory [ 38 ] and the learning-based Local Interpretable Model-agnostic Explanation ( LIME ) [ 35 ] have been shown to work well on classification tasks with a small number of classes . In particular , there has been work on image classification [ 35 ] , sentiment analysis [ 8 ] , and evidence selection for question answering [ 32 ] . However , to the best of our knowledge , there has been less work studying explanations over models with sequential output and large class sizes at each time step . An attempt by [ 1 ] aims at explaining machine translation by aligning the sentences in source and target languages . Nonetheless , unlike translation , where it is possible to find almost all word alignments of the input and output sentences , many text generation tasks are not alignment-based . We further explore explanations over sequences that contain implicit and indirect relations between the input and output utterances . 1Our code is available at https : //github.com/Pascalson/LERG . 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . In this paper , we study explanations over a set of representative conditional text generation models – dialogue response generation models [ 45 , 55 ] . These models typically aim to produce an engaging and informative [ 3 , 24 ] response to an input message . The open-ended sentences and multiple acceptable responses in dialogues pose two major challenges : ( 1 ) an exponentially large output space and ( 2 ) the implicit relations between the input and output texts . For example , the open-ended prompt “ How are you today ? ” could lead to multiple responses depending on the users ’ emotion , situation , social skills , expressions , etc . A simple answer such as “ Good . Thank you for asking. ” does not have an explicit alignment to words in the input prompt . Even though this alignment does not exist , it is clear that “ good ” is the key response to “ how are you ” . To find such crucial corresponding parts in a dialogue , we propose to extract explanations that can answer the question : “ Which parts of the response are influenced the most by parts of the prompt ? ” To obtain such explanations , we introduce LERG , a novel yet simple method that extracts the sorted importance scores of every input-output segment pair from a dialogue response generation model . We view this sequence prediction as the uncertainty estimation of one human response and find a linear proxy that simulates the certainty caused from one input segment to an output segment . We further derive two optimization variations of LERG . One is learning-based [ 35 ] and another is the derived optimal similar to Shapley value [ 38 ] . To theoretically verify LERG , we propose that an ideal explanation of text generation should adhere to three properties : unbiased approximation , intra-response consistency , and causal cause identification . To the best of our knowledge , our work is the first to explore explanation over dialog response generation while maintaining all three properties . To verify if the explanations are both faithful ( the explanation is fully dependent on the model being explained ) [ 2 ] and interpretable ( the explanation is understandable by humans ) [ 14 ] , we conduct comprehensive automatic evaluations and user study . We evaluate the necessity and sufficiency of the extracted explanation to the generation model by evaluating the perplexity change of removing salient input segments ( necessity ) and evaluating the perplexity of only salient segments remaining ( sufficiency ) . In our user study , we present annotators with only the most salient parts in an input and ask them to select the most appropriate response from a set of candidates . Empirically , our proposed method consistently outperforms baselines on both automatic metrics and human evaluation . Our key contributions are : • We propose a novel local explanation method for dialogue response generation ( LERG ) . • We propose a unified formulation that generalizes local explanation methods towards sequence generation and show that our method adheres to the desired properties for explaining conditional text generation . • We build a systematic framework to evaluate explanations of response generation including automatic metrics and user study . 2 Local Explanation . Local explanation methods aim to explain predictions of an arbitrary model by interpreting the neighborhood of individual predictions [ 35 ] . It can be viewed as training a proxy that adds the contributions of input features to a model ’ s predictions [ 27 ] . More formally , given an example with input features x = { xi } Mi=1 , the corresponding prediction y with probability f ( x ) = Pθ ( Y = y|x ) ( the classifier is parameterized by θ ) , we denote the contribution from each input feature xi as φi ∈ R and denote the concatenation of all contributions as φ = [ φ1 , ... , φM ] T ∈ RM . Two popular local explanation methods are the learning-based Local Interpretable Model-agnostic Explanations ( LIME ) [ 35 ] and the game theory-based Shapley value [ 38 ] . LIME interprets a complex classifier f based on locally approximating a linear classifier around a given prediction f ( x ) . The optimization of the explanation model that LIME uses adheres to : ξ ( x ) = argmin ϕ [ L ( f , ϕ , πx ) + Ω ( ϕ ) ] , ( 1 ) where we sample a perturbed input x̃ from πx ( x̃ ) = exp ( −D ( x , x̃ ) 2/σ2 ) taking D ( x , x̃ ) as a distance function and σ as the width . Ω is the model complexity of the proxy ϕ . The objective of ξ ( x ) is to find the simplest ϕ that can approximate the behavior of f around x . When using a linear classifier dialog input text outputG intent ( a ) Controllable dialogue models input text positive negative ( b ) Explanation of classifier dialog input text outputG intent ( c ) Our concept Figure 1 : The motivation of local explanation for dialogue response generation . ( c ) = ( a ) + ( b ) . φ as the ϕ to minimize Ω ( ϕ ) [ 35 ] , we can formulate the objective function as : φ = argmin φ Ex̃∼πx ( Pθ ( Y = y|x̃ ) −φT z ) 2 , ( 2 ) where z ∈ { 0 , 1 } M is a simplified feature vector of x̃ by a mapping function h such that z = h ( x , x̃ ) = { � ( xi ∈ x̃ ) } Mi=1 . The optimization means to minimize the classification error in the neighborhood of x sampled from πx . Therefore , using LIME , we can find an interpretable linear model that approximates any complex classifier ’ s behavior around an example x. Shapley value takes the input features x = { xi } Mi=1 as M independent players who cooperate to achieve a benefit in a game [ 38 ] . The Shapley value computes how much each player xi contributes to the total received benefit : ϕi ( x ) = � x̃⊆x\ { xi } |x̃| ! ( |x|− |x̃|− 1 ) ! |x| ! [ Pθ ( Y = y|x̃ ∪ { xi } ) − Pθ ( Y = y|x̃ ) ] . ( 3 ) To reduce the computational cost , instead of computing all combinations , we can find surrogates φi proportional to ϕi and rewrite the above equation as an expectation over x sampled from P ( x̃ ) : φi = |x| |x|− 1ϕi = Ex̃∼P ( x̃ ) [ Pθ ( Y = y|x̃ ∪ { xi } ) − Pθ ( Y = y|x̃ ) ] , ∀i , ( 4 ) where P ( x̃ ) = 1 ( |x|−1 ) ( |x|−1|x̃| ) is the perturb function.2 We can also transform the above formulation into argmin : φi = argmin φi Ex̃∼P ( x̃ ) ( [ Pθ ( Y = y|x̃ ∪ { xi } ) − Pθ ( Y = y|x̃ ) ] − φi ) 2 . ( 5 ) 3 Local Explanation for Dialogue Response Generation . We aim to explain a model ’ s response prediction to a dialogue history one at a time and call it the local explanation of dialogue response generation . We focus on the local explanation for a more fine-grained understanding of the model ’ s behavior . 3.1 Task Definition As depicted in Figure 1 , we draw inspiration from the notions of controllable dialogue generation models ( Figure 1a ) and local explanation in sentiment analysis ( Figure 1b ) . The first one uses a concept in predefined classes as the relation between input text and the response ; the latter finds the features that correspond to positive or negative sentiment . We propose to find parts within the input and output texts that are related by an underlying intent ( Figure 1c ) . We first define the notations for dialogue response generation , which aims to predict a response y = y1y2 ... yN given an input message x = x1x2 ... xM . xi is the i-th token in sentence x with length M and yj is the j-th token in sentence y with length N . To solve this task , a typical sequence-to-sequence model f parameterized by θ produces a sequence of probability masses < Pθ ( y1|x ) , Pθ ( y2|x , y1 ) , ... , Pθ ( yN |x , y < N ) > [ 45 ] . The probability of y given x can then be computed as the product of the sequence Pθ ( y|x ) = Pθ ( y1|x ) Pθ ( y2|x , y1 ) ... Pθ ( yN |x , y < N ) . 2� x̃⊆x\ { xi } P ( x̃ ) = 1 ( |x|−1 ) � x̃⊆x\ { xi } 1/ �|x|−1 |x̃| � = 1 ( |x|−1 ) � |x̃| �|x|−1 |x̃| � / �|x|−1 |x̃| � = ( |x|−1 ) ( |x|−1 ) = 1 . This affirms that the P ( x̃ ) is a valid probability mass function . To explain the prediction , we then define a new explanation model Φ ∈ RM×N where each column Φj ∈ RM linearly approximates single sequential prediction at the j-th time step in text generation . To learn the optimal Φ , we sample perturbed inputs x̃ from a distribution centered on the original inputs x through a probability density function x̃ = π ( x ) . Finally , we optimize Φ by ensuring u ( ΦTj z ) ≈ g ( x̃ ) whenever z is a simplified embedding of x̃ by a mapping function z = h ( x , x̃ ) , where we define g as the gain function of the target generative model f , u as a transform function of Φ and z and L as the loss function . Note that z can be a vector or a matrix and g ( · ) , u ( · ) can return a scalar or a vector depending on the used method . Therefore , we unify the local explanations ( LIME and Shapley value ) under dialogue response generation as : Definition 1 : A Unified Formulation of Local Explanation for Dialogue Response Generation Φj = argmin Φj L ( g ( yj |x̃ , y < j ) , u ( ΦTj h ( x̃ ) ) ) , for j = 1 , 2 , ... , N . ( 6 ) The proofs of unification into Equation 6 can be found in Appendix A . However , direct adaptation of LIME and Shapley value to dialogue response generation fails to consider the complexity of text generation and the diversity of generated examples . We develop disciplines to alleviate these problems .","In comparison to the interpretation of classification models, the explanation of sequence generation models is also an important problem, however it has seen little attention. In this work, we study model-agnostic explanations of a representative text generation task – dialogue response generation. Dialog response generation is challenging with its open-ended sentences and multiple acceptable responses. To gain insights into the reasoning process of a generation model, we propose a new method, local explanation of response generation (LERG), that regards the explanations as the mutual interaction of segments in input and output sentences. LERG views the sequence prediction as uncertainty estimation of a human response and then creates explanations by perturbing the input and calculating the certainty change over the human response. We show that LERG adheres to desired properties of explanation for text generation, including unbiased approximation, consistency, and cause identification. Empirically, our results show that our method consistently improves other widely used methods on proposed automaticand humanevaluation metrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can extract both explicit and implicit relations between input and output segments. 1","This paper studies the model-agnostic explanations of dialogue response generation and proposes a new called local explanation of response generation (LERG). LERG extracts the sorted importance scores of every input-output segment pair from a dialogue response generation model. It views the sequence prediction as uncertainty estimation of a human response and creates explanations by perturbing the input and calculating the certainty change over the human response. The authors show the proposed LERG adhere to three properties of  an ideal explanation of text generation: (1) unbiased approximation, (2) intra-response consistency and (3) causal cause identification. The experiments on a popular benchmark DailyDialog empirically verify the effectiveness of the proposed approach.","The article discusses the importance of explainable AI (XAI) and local explanations for complex machine learning models in applications such as medical diagnostics and speech assistants. The authors propose a novel method called LERG for local explanations in dialogue response generation models, which aim to produce engaging and informative responses to input messages. The challenges in this task are the exponentially large output space and implicit relations between input and output texts. LERG extracts sorted importance scores for every input-output segment pair and adheres to three ideal properties for explaining text generation: unbiased approximation, intra-response consistency, and causal cause identification. Comprehensive automatic evaluations and a user study show that LERG consistently outperforms baselines on both automatic metrics and human evaluation.","The article discusses the problem of explaining the predictions of a complex model used for dialogue response generation. The authors propose a novel method called Local Explanation for Dialogue Response Generation (LERG), which aims to extract explanations that can answer the question "" Which parts of the response are influenced the most by parts of the prompt?"". LERG introduces a novel optimization formulation that generalizes local explanation methods towards sequence generation, and adheres to desired properties for explaining conditional text generation. The authors build a systematic framework to evaluate explanations of response generation, including automatic metrics and user study.","0,1,2",0,1,1,2,,coverage,coverage (+ background),"coverage, coherence",informative,0,1,1,2,Summary has better coverage of relevant info.,Summary B adds too much background info.,Summary B covers many aspects mentioned in the abstract and is most coherent as a summary. ,Summary C's information is too general comparing to the other two summaries. 
30,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"1 Introduction . As machine learning algorithms are widely used in practice for making high-stakes decisions affecting human lives , there has been a huge body of work on FAIR-ML trying to ensure ‘ fairness ’ in the solutions returned by these algorithms . There are two large intersecting bodies of work : one body ’ s main focus in to understand what ‘ fairness ’ means ( e.g . [ 26 , 39 , 38 , 36 , 14 , 25 ] ) in various different contexts , and the second body ’ s focus has been on addressing the algorithmic challenges brought forth by these considerations ( e.g. , [ 24 , 9 , 33 , 6 , 29 , 5 ] ) . This paper falls in the second class . In particular , we consider an individual fairness model proposed by Jung , Kannan , and Lutz [ 25 ] for a k-clustering problem . Given points ( clients ) X in a space with metric distance d , find k points ( facilities ) S ⊆ X , minimizing ( ∑ v∈X d ( v , S ) p ) 1/p where d ( v , S ) is v ’ s distance to the closest point in S. This includes k-CENTER , k-MEDIAN and the popular k-MEANS objective for p =∞ , p = 1 and p = 2 respectively , problems which have been extensively studied [ 12 , 23 , 27 , 4 , 2 ] in the algorithms literature . Jung et . al . [ 25 ] proposed that in this context a solution would be deemed individually fair , if for every client v ∈ X there is an open facility not too far from it . More precisely , if there is a facility within distance r ( v ) which is the smallest radius ∗Supported by NSF grant # 2041920 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . around v that contains n/k points . The rationale behind n/k is that if all the points had equal chance of getting picked as a facility , each client v would expect a facility to be open within distance r ( v ) . Furthermore , this formulation encourages a solution to open more facilities in dense areas of the metric which is motivated by the facility location application of clustering . Jung et al . [ 25 ] gave a solution where every client v was served within a radius of 2r ( v ) , which as a jargon is called 2-approximate fair solution . However , their solution did not explicitly consider the “ objective ” function ( k-MEANS/k-MEDIAN , for instance ) in the clustering problem , which is often used as a proxy to measure the quality of the clustering . This was addressed in a follow up paper by Mahabadi and Vakilian [ 29 ] who gave a ( 7 , O ( p ) ) -approximation with respect to the ℓp-norm objective . That is , they give a local-search based solution which is 7-approximately fair , but the objective is violated by some Cp-factor where the constant C is rather large ( for p = 1 , the factor is 84 ) . The theoretical running time of their algorithm is Õ ( pk5n4 ) . 1.1 Our Contributions . The main contribution of our paper is to give improved algorithms for this problem using linear programming rounding . Our study stems from two observations : one , that the problem at the core of Jung et al . [ 25 ] was in fact studied as “ weighted/priority k-CENTER problem ” by Plesník [ 32 ] , and that if all the r ( v ) ’ s were the same ( which may not at all be the case ) , then the clustering problem has also been studied under the guise of centridian/ordered median problem [ 3 , 8 , 10 ] . Combining ideas from these two bodies of work , we design an ( 8 , 8 ) algorithm for the FAIR-k-MEDIAN problem , which obtains an 8-approximation for both cost and fairness ( our cost guarantees improve as p grows ) . Result 1 . There is an ( 8 , 21+2/p ) -approximation algorithm for FAIR- ( p , k ) -CLUSTERING that runs in LP solving time plus Õ ( n2 ) , overall Õ ( kn4 ) . In particular , we have ( 8 , 8 ) -approximation and ( 8 , 4 ) -approximation algorithms for FAIR-k-MEDIAN and FAIR-k-MEANS , respectively . Remark . The approximation factor compares itself with the optimal solution which opens centers at the points in X . This is also the notion considered by [ 29 ] and [ 25 ] . If the optimal solution is allowed to open centers anywhere in the real space , then one has an additional factor 2 loss in the approximation [ 15 ] . Whether this extra hit of factor 2 is necessary , we leave as an interesting algorithmic question . Although solving an LP may seem prohibitive in practice , we can obtain a much faster running time by implementing a sparsification routine ( inspired by [ 32 , 21 ] ) with a marginal hit in the fairness and clustering cost ( see Lemma 5 for details ) . Empirically , this greatly decreases the running time , and is often faster than the [ 29 ] implementation . In our experiments , we also find that our theoretical bounds are too pessimistic . Indeed , we show that our algorithm ’ s cost is at most % 1 more than the optimal clustering cost ( which does not have any fairness violation ) , almost always , and never more than % 15 in the rest . Furthermore , our maximum fairness violation is at most a factor of 1.27 which is much better than our theoretical guarantee of 8 . We also do a more fine-grained analysis of the fairness violation : consider a vector where each coordinate stands for a clients “ unfairness ” indicating the ratio of its distance to r ( v ) . When we plot this as a histogram , we find that most of the mass is shifted to the “ left ” , that is , the percentage of clients who satisfy their fairness constraints is significantly larger than in the [ 29 ] solution . This seems to suggest the linear program , which is trying to minimize the cost , itself tries to increase the number of fairly treated clients . We leave a theoretical investigation of this phenomenon for future work . Our experiments also demonstrate the price of fairness . We find that our linear programs , which maintains absolute fairness , have objective value considerably larger than that of [ 29 ] . On the other hand , if we tune the “ fairness violation ” of the linear program to match that of [ 29 ] , then the objective value of our algorithm drops . We run experiments to further elaborate on the inherent cost of fairness in our datasets by demonstrating how the optimal cost changes with respect to varying degrees of fairness relaxation . It is worth noting that our algorithm works for arbitrary values of r ( v ) ≥ 0 for points v ( and this may be true for [ 29 ] as well ) , and we present our results thus . This setting might be of interest in applications where the “ fair radius ” may not be n/k but something more nuanced . 1.2 Other related work . k-CENTER has a 2-approximation due to Gonzales , and Hochbaum and Shmoys [ 17 , 21 ] and they prove it is NP-hard to get better approximations . k-MEDIAN , and k-MEANS are hard to approximate within factors better than 1.73 and 3.94 [ 19 ] respectively with current best approximations being 2.67 by [ 7 ] and 9 by [ 2 ] . Also recently , there has been an improvement on lower-bounds for approximating Continuous k-MEDIAN and k-MEANS where centers can be picked anywhere in the real space . By Cohen-Addad , Karthik , and Lee [ 15 ] , it is NP-hard to approximate Continuous k-MEDIAN and k-MEANS within factors 2− o ( 1 ) and 4− o ( 1 ) respectively . FAIR-k-CENTER is a special case of Priority k-CENTER where the radii r ( v ) in which a point v demands a center at that distance , are general values . [ 32 ] introduced this problem and gave a best possible 2-approximation . Gørtz and Wirth [ 18 ] study the problem for asymmetric metrics and prove that it is NP-hard to obtain any non-trivial approximation . [ 5 ] give a 9-approximation for the problem in presence of outliers and further generalize to constant approximations for general constraints on the solution centers . Another very closely related problem is Chance-k-Coverage introduced in [ 20 ] in which for any point v , in addition to r ( v ) , a probability p ( v ) is given and the goal is to find a distribution on possible solutions such that a solution drawn from this distribution covers v with probability at least p ( v ) . This also has a 9-approximation by [ 20 ] . A clustering problem related to FAIR-k-MEDIAN and FAIR-k-MEANS is the Simultaneous kClustering in which the goal is to find a solution with approximation guarantees with respect to any monotone , symmetric norm . This problem has an O ( 1 ) -approximation due to Chakrabarty and Swamy [ 11 ] with a line of previous work including [ 3 , 10 , 8 ] . Another similar notion of individual fairness is introduced by Chen et al . [ 13 ] in which a solution is fair if there is no group of size at least n/k for which there exists a facility that would reduce the connection cost of all members of the group if opened . [ 13 ] give a ( 1 + √ 2 ) -approximation for ℓ1 , ℓ2 , and ℓ∞ norm distances for the setting where facilities can be places anywhere in the real space . Micha and Shah [ 30 ] modified the approach to give close to 2-approximation for ℓ2 and proved the previous results for ℓ1 and ℓ∞ are indeed tight . Recently , Abbasi et al . [ 1 ] introduced a new notion of fairness centered around providing equitable group representations . In this problem , the points are partitioned into groups and the goal is to minimize the maximum clustering cost a group can have in the solution . The idea is that the cluster centers should represent each group fairly , and this is captured by ensuring that each group ’ s clustering cost is comparable with the others . This notion is orthogonal to the fairness criteria we study in this paper ; Although we could tune the maximum distance the points in a group can have to our solution , that does not give any guarantees with respect to the objective cost and vice versa . 2 Preliminaries . In this section , we formally define our problems , establish some notations , and describe a classic clustering routine due to Hochbaum and Shmoys [ 21 ] with modifications by Plesník [ 32 ] . Given a subset S ⊆ X , we use d ( v , S ) to denote v ’ s distance to the closest point in S. Definition 1 ( FAIR- ( p , k ) -CLUSTERING Problem ) . The input is a metric space ( X , d ) , radius function r : X → R+ , and integers p , k ≥ 1 . The goal is to find S ⊆ X of size at most k such that d ( v , S ) ≤ r ( v ) for all v ∈ X and the clustering cost ( ∑ v∈X d ( v , S ) p ) 1/p is minimized . Let opt be the clustering cost of an optimal solution . For α , β ≥ 1 , an ( α , β ) -approximate solution is S ⊆ X of size at most k with d ( v , S ) ≤ αr ( v ) for all v ∈ X and ( ∑ v∈X d ( v , S ) p ) 1/p≤ βopt . In plain English , the fairness approximation is α while the objective/cost approximation is β . Our main result is the following . Theorem 1 . There is an ( 8 , 21+2/p ) -approximation algorithm for FAIR- ( p , k ) -CLUSTERING that runs in LP solving time plus Õ ( n2 ) , overall Õ ( kn4 ) . The algorithm relies on rounding a solution to the following LP for FAIR- ( p , k ) -CLUSTERING2 where the optimal LP objective is at most optp . The variable yu denotes the amount by which u ∈ X 2The LP and its rounding is slightly different for p = ∞ ; we omit this from this version . is open as a center . xvu for v , u ∈ X is the amount by which v is assigned to u . The constraints respectively capture the conditions : every client must connect to someone , k centers are opened , no client can connect to an unopened center , and crucially that a client can not travel to a center further than r ( v ) . optp ≥ min ∑ v , u∈X d ( v , u ) pxvu ( LP ) ∑ u∈X xvu = 1 , ∀v ∈ X ( LP1 ) ∑ u∈X yu = k ( LP2 ) xvu ≤ yu , ∀v , u ∈ X ( LP3 ) xvu = 0 , ∀v , u : d ( v , u ) > r ( v ) ( LP4 ) 0 ≤ xvu , yu ≤ 1 , ∀v , u ∈ X . From here on , we use the notation B ( u , r ) for u ∈ X and r ∈ R+ to denote the points in a ball of radius r around u . That is B ( u , r ) : = { v ∈ X : d ( u , v ) ≤ r } . Also , for any set of points U ⊆ X , let y ( U ) : = ∑ u∈U yu . Considering ( LP4 ) and ( LP1 ) the following holds . Fact 1 . If y is from a feasible LP solution , then y ( B ( v , r ( v ) ) ) ≥ 1 for all v ∈ X . We now describe a routine Filter due to [ 32 , 21 ] which is used as a subroutine in our main algorithm . Assume all the points are initially “ uncovered ” . The routine takes a function R : X → R+ , sorts the points in order of increasing R ( v ) ’ s . Then it considers the first point in this order , calls it a “ representative ” and “ covers ” all the points v at distance at most 2r ( v ) from it . Call these points D ( v ) . Repeating this procedure until all the points are covered , forms a partition on X such that the representatives are “ far apart ” , while each non-representative is assigned to a “ nearby ” representative , among other useful properties listed in Fact 2 and Fact 3 . Algorithm 1 Filter Input : Metric ( X , d ) , radius function R : X → R+ 1 : U ← X ▷ The set of “ uncovered points ” 2 : S ← ∅ ▷ The set of “ representatives ” 3 : while U ̸= ∅ do 4 : u← argminv∈U R ( v ) ▷ The first point in U in non-decreasing R order 5 : S ← S ∪ u 6 : D ( u ) ← { v ∈ U : d ( u , v ) ≤ 2R ( v ) } ▷ D ( u ) = B ( u , 2R ( v ) ) ∩ U and includes u itself 7 : U ← U\D ( u ) 8 : end while Output : S , { D ( u ) : u ∈ S } Fact 2 . [ 32 , 21 ] The following is true for the output of Filter : ( a ) ∀u , v ∈ S , d ( u , v ) > 2max { R ( u ) , R ( v ) } , ( b ) The balls { B ( u , R ( u ) ) : u ∈ S } are mutually disjoint , ( c ) The set { D ( u ) : u ∈ S } partitions X , ( d ) ∀u ∈ S , ∀v ∈ D ( u ) , R ( u ) ≤ R ( v ) , and ( e ) ∀u ∈ S , ∀v ∈ D ( u ) , d ( u , v ) ≤ 2R ( v ) . Fact 3 . For any u ∈ S and w ∈ B ( u , R ( u ) ) , the unique closest point in S to w is u . Proof . Suppose otherwise . That is , there exists v ∈ S not equal to u s.t . d ( w , v ) ≤ d ( w , u ) . Then d ( u , v ) ≤ d ( u , w ) + d ( w , v ) ≤ 2d ( w , u ) ≤ 2R ( u ) which contradicts Fact 2 as we must have d ( u , v ) > 2max { R ( u ) , R ( v ) } . To elaborate on the importance of Algorithm 1 and build some intuition , we point out the following theorem of [ 32 , 25 ] . Theorem 2 . Take S the output of Algorithm 1 on a FAIR- ( p , k ) -CLUSTERING instance with R : = r. Then if the instance is feasible , |S| ≤ k and d ( v , S ) ≤ 2r ( v ) for all v ∈ X . Proof . By Fact 2 we know that { D ( u ) : u ∈ S } partitions X so for any v ∈ X , there exists a u ∈ S for which v ∈ D ( u ) . Plus , d ( v , u ) ≤ 2R ( v ) = 2r ( v ) . Now it only remains to prove |S| ≤ k. To see this , let S∗ be some feasible solution and observe that two different u , w ∈ S , can not be covered by the same center in S∗ . Since otherwise , if there exists f ∈ S∗ for which d ( u , f ) ≤ r ( u ) and d ( w , f ) ≤ r ( w ) by triangle inequality d ( u , v ) ≤ d ( u , f ) + d ( f , w ) ≤ r ( u ) + r ( v ) and this contradicts d ( u , w ) > 2max { r ( u ) , r ( v ) } from Fact 2 . But of course , the above theorem does not give any guarantees for the clustering cost ( unless p =∞ ) . It might be the case that many points are paying close to 0 towards the clustering cost in the optimal solution , but are made to connect to a point much farther in the above procedure . 3 Algorithm for FAIR- ( p , k ) -CLUSTERING problem Now we are ready to describe our algorithm Fair-Round which establishes Theorem 1 . At a highlevel , we run our Filter routine by defining the input function R in a manner that is conscious of the FAIR- ( p , k ) -CLUSTERING cost : given ( x∗ , y∗ ) which is an optimal solution to ( LP ) , for any v ∈ X let Cv be v ’ s contribution to the LP cost i.e . Cv : = ∑ u∈X d ( v , u ) px∗vu and define R ( v ) : = min { r ( v ) , ( 2Cv ) 1/p } . Let us ponder for a bit to see what changes from Theorem 2 . For the output S , we still have the fairness guarantee d ( v , S ) ≤ 2r ( v ) for all v but since ( 2Cv ) 1/p might be less than r ( v ) for any v , we can not guarantee that |S| ≤ k. Thankfully , in this case , we can still prove |S| ≤ 2k ( Corollary 1 ) . The rest of the algorithm is deciding on a subset of at most k points out of this S to return as the final solution , while ensuring the fairness and cost guarantees are still within constant factor of the optimal . This idea is very similar to existing ideas in [ 3 , 12 ] which look at the problem without fairness considerations . Recall that { D ( u ) : u ∈ S } partitions X and each u ∈ S is responsible for covering all the points in D ( u ) . Here , we could simply move each point in D ( u ) to u and divert the y value of each point to its closest point in S. Note that , y ( S ) = k ( Fact 4 ) and similar to the proof of Theorem 2 we could show yu ≥ 1/2 ( Lemma 1 ) for all u ∈ S hence |S| ≤ 2k ( Corollary 1 ) . If this leads to some y-value reaching 1 , we open those centers . For u ∈ S with yu < 1 , if we do not decide to include it in the final solution , we promise to open Su , its closest point in S other than itself . In this case , all the |D ( u ) | points on u are delegated to Su . Using the fact that yu < 1 , we can prove that fairness guarantee ( Lemma 3 ) approximately holds for the points in D ( u ) even after this delegation . To get the clustering cost guarantee , we need to do more work . Observe that , currently , u is already fractionally assigned to v by 1− yu . So if instead of yu ≥ 1/2 we had yu = 1/2 we could ensure that already u is assigned to v by 1/2 thus integrally assigning u to v only doubles the cost . This is why we need to do more work to get yu ∈ { 1/2 , 1 } for u ∈ S ( see Lemma 2 ) and then bound the clustering cost in Lemma 4 . Fact 4. y ( S ) = k and remains so after Line 11 . Proof . We initialize y to y∗ for which y∗ ( X ) = k by LP2 . The algorithm then transfers y ( X ) to S by Line 11 without changing the total . Afterwards , the modifications in Lines 13 and 19 move the y mass around in S but the total value y ( S ) remains unchanged . Lemma 1 . After Line 14 of Algorithm 2 , 1/2 ≤ yu ≤ 1 for all u ∈ S. Proof . First we argue that yu ≥ 1/2 for all u ∈ S by the end of Line 11 . Fix u ∈ S. Per Fact 3 y ( B ( u , R ( u ) ) ) is entirely moved to yu . By definition of R ( u ) there are two cases : Case I , R ( u ) = r ( u ) thus yu ≥ 1 as y∗ ( B ( v , r ( v ) ) ) ≥ 1 for all v ∈ X according to Fact 1 . Case II , R ( u ) = ( 2Cu ) 1/p then by Markov ’ s inequality y∗ ( B ( u , R ( u ) ) ) ≥ 1/2 and after this point , yu is never decreased to below 1/2 . More precisely , Cu = ∑ v∈X d ( u , v ) px∗uv ≥ ∑ v∈X : d ( u , v ) > R ( u ) d ( u , v ) px∗uv ≥ 2Cu ∑ v∈X : d ( u , v ) > R ( u ) x∗uv . Considering ∑ v∈X x ∗ uv = 1 by LP1 , this implies ∑ v∈X : d ( u , v ) ≤R ( u ) x∗uv ≥ 1/2 thus y∗ ( B ( u , R ( u ) ) ) ≥ 1/2 . As for proving yu ≤ 1 , it might indeed be the case that yu > 1 by the end of Line 11 but the loop ending at Line 14 can guarantee yu ≤ 1 for all u ∈ S. This is because y ( S ) = k ( Fact 4 ) and we already checked in the beginning of Algorithm 2 that |S| > k. Corollary 1 . S in Algorithm 2 has size at most 2k . Algorithm 2 Fair-Round : FAIR- ( p , k ) -CLUSTERING bi-criteria approximation Input : Metric ( X , d ) , radius function r : X → R+ , and ( x∗ , y∗ ) an optimal solution of LP 1 : Cv ← ∑ u∈X d ( v , u ) px∗vu ∀v ∈ X ▷ v ’ s cost share in the LP objective 2 : R ( v ) ← min { r ( v ) , ( 2Cv ) 1/p } ∀v ∈ X 3 : S , { D ( u ) : u ∈ S } ← Filter ( ( X , d ) , R ) 4 : if |S| ≤ k then 5 : return S 6 : end if 7 : ( x , y ) ← ( x∗ , y∗ ) 8 : for all v ∈ X\S do ▷ Direct y mass from outside of S to the closest point in S 9 : u← closest point in S to v 10 : yu ← yu + yv , yv ← 0 ▷ Note : May cause yu to increase above 1 11 : end for 12 : while There are u , v ∈ S with yu > 1 and yv < 1 do ▷ Ensure yu ≤ 1 for all u ∈ S 13 : yu ← yu − δ , yv ← yv + δ , where δ : = min { 1− yv , yu − 1 } 14 : end while ▷ Remark : By now , 1/2 ≤ yu ≤ 1 for all u ∈ S ( see Lemma 1 ) . 15 : Su ← closest point in S\u to u ∀u ∈ S 16 : while There are u , v ∈ S with 17 : 1/2 < yu < 1 , and yv < 1 , 18 : d ( u , Su ) p|D ( u ) | > d ( v , Sv ) p|D ( v ) | do ▷ Move y mass from u to v if u is costlier 19 : yu ← yu − δ , yv ← yv + δ , where δ : = min { 1− yv , yu − 1/2 } 20 : end while ▷ Remark : At this point , yu ∈ { 1/2 , 1 } for all u ∈ S ( see Lemma 2 ) . 21 : T ← u ∈ S with yu = 1 22 : Consider the forest of arbitrary rooted trees on vertices u ∈ S with edges ( u , Su ) . Let O be odd-level vertices in S\T , and E be even-level vertices in S\T . 23 : if |E| ≤ |O| then 24 : T ← T ∪ E 25 : else 26 : T ← T ∪O 27 : end if Output : T Lemma 2 . After Line 20 of Algorithm 2 , yu ∈ { 1/2 , 1 } for all u ∈ S. Proof . Suppose not . Since y ( S ) = k ( Fact 4 ) and k is an integer , by Lemma 1 , there has to be at least two u , v ∈ S with yu , yv ∈ ( 1/2 , 1 ) which is a contradiction , since either one of them has to be changed to 1/2 or 1 in the while loop ending at Line 20 . Lemma 3 . For all v ∈ X , d ( v , T ) ≤ 8r ( v ) . Proof . Fix v ∈ X . Since { D ( u ) : u ∈ S } partitions X there exists u ∈ S such that v ∈ D ( u ) . According to Fact 2 d ( v , u ) ≤ 2R ( v ) ≤ 2r ( v ) by definition of R. If u ends up in T we are done . Else , it has to be that yu < 1 and Su ∈ T . In what follows , we prove that if yu < 1 at Line 20 then d ( u , Su ) ≤ 6r ( v ) . This implies d ( v , Su ) ≤ 8r ( v ) hence the lemma . We know that initially y∗ ( B ( v , r ( v ) ) ) ≥ 1 per Fact 1 but since v /∈ S , the y mass on B ( v , r ( v ) ) has been moved to S by Line 11 . If for all w ∈ B ( v , r ( v ) ) their closest point in S was u , all of y ( B ( v , r ( v ) ) ) would be moved to u then yu = 1 by the end of Line 20 . So there must exist w ∈ B ( v , r ( v ) ) with x∗vw > 0 along with u′ ∈ S , u′ ̸= u , such that d ( w , u′ ) ≤ d ( w , u ) which made yw to be moved to u′ . By definition of Su , d ( u , Su ) ≤ d ( u , u′ ) . Applying the triangle inequality twice gives : d ( u , Su ) ≤ d ( u , u′ ) ≤ d ( u , w ) + d ( w , u′ ) ≤ 2d ( u , w ) ≤ 2 ( d ( u , v ) + d ( v , w ) ) ≤ 2 ( 2r ( v ) + r ( v ) ) = 6r ( v ) where the last inequality comes from Fact 2 stating d ( u , v ) ≤ 2r ( v ) , and from the fact that x∗vw > 0 implies d ( v , w ) ≤ r ( v ) by LP4 . Lemma 4 . ( ∑ v∈X d ( v , T ) p ) 1/p ≤ ( ( 2p+2 ) ∑v , u∈X Cv ) 1/p ≤ 21+2/popt . Proof . For the proof , we compare ∑ v∈X d ( v , T ) p with the optimal LP cost ∑ v∈X Cv which is at most optp . Fix v ∈ X and u ∈ S for which v ∈ D ( u ) . By Fact 2 d ( v , u ) ≤ 2R ( v ) ≤ 2 ( 2Cv ) 1/p per definition of R. So moving D ( u ) to u for all u ∈ S has an additive cost of∑ u∈S ∑ v∈D ( u ) d ( v , u ) p ≤ 2p+1 ∑ v∈X Cv . From now on , assume there are D ( u ) collocated points on a u ∈ S. Moving around the y mass up to Line 11 adds a multiplicative factor of 2p loss in the approximation ratio . The logic is : if u ∈ S was relying on w ∈ X in the LP solution , meaning , x∗uw > 0 and yw was moved to a u′ ∈ S ( due to d ( w , u′ ) ≤ d ( w , u ) ) then the cost u has to pay to connect to u′ is d ( u , u′ ) p ≤ ( d ( u , w ) + d ( w , u′ ) ) p ≤ 2pd ( u , w ) p which is a 2p factor worse than the LP cost d ( u , w ) p it was paying to connect to w earlier . At this point ( Line 14 ) , note that the cost incurred by u is |D ( u ) |d ( u , Su ) p ( 1 − yu ) . To elaborate on this , corresponding to y , we define an x such that ( x , y ) is feasible for LP . Let xuu = yu and xuSu = 1− yu so that LP4 is satisfied for u . Then the cost incurred by u is |D ( u ) | ( d ( u , u ) pxuu + d ( u , Su ) pxuSu ) = |D ( u ) |d ( u , Su ) p ( 1 − yu ) . The while loop ending at Line 20 does not increase the value of the objective function : As we decrease yu and increase yv , only if the cost incurred by u is bigger than that of v. The last multiplicative factor 2 loss comes from when u ∈ S\T . In which case , yu = 1/2 and Su ∈ T . So by assigning u to Su we pay |D ( u ) |d ( u , Su ) p which is twice more than before ( as 1 − yu = 1/2 ) . Observe that this last step is why we needed to do all the work to get yu ∈ { 1/2 , 1 } in Lemma 2 . Putting the three steps together , the overall cost is at most 2× 2p + 2p+1 = 2p+2 times the LP cost or at most 21+2/popt . Proof of Theorem 1 . Using Corollary 1 and the fact that |T | ≤ |S|/2 by construction , we have |T | ≤ k. Lemmas 3 and 4 give the fairness and approximation guarantees . As for runtime , notice that Algorithm 2 runs in time Õ ( n2 ) . But the runtime is dominated by the LP solving time . According to [ 37 ] , finding a ( 1 + ε ) -approximation to LP takes time O ( kn2/ϵ2 ) . Setting ε = 1/n gives the O ( kn4 ) runtime . As evident , the runtime is dominated by the LP solving time . We end this section by descibing the sparsification pre-processing that when applied to the original instance , can tremendously decrease the LP solving time in practice while incurring only a small loss in fairness and clustering cost . One reason why the LP takes a lot of time is because there are many variables ; xvu for every v ∈ X and every u within distance r ( v ) of v. To fix this , we first run the Filter algorithm on the data set with R ( v ) = δr ( v ) , where δ is a tune-able parameter . It is not too hard to quantify the loss in fairness and cost as a function of δ , and we do so in the lemma below . More importantly , note that when δ = 1 , then the number of variables goes down from n to O ( k ) ; this is because of the definition of r ( v ) which guarantees ≈ n/k points in the radius r ( v ) around v. Therefore , running the pre-processing step would make the number of remaining points ≈ k. In our experiments we set δ to be much smaller , and yet observe a great drop in our running times . Algorithm 3 Sparsification + Fair-Round Input : Metric ( X , d ) , radius function r : X → R+ , parameter δ > 0 1 : S , { D ( u ) : u ∈ S } ← Filter ( ( X , d ) , δr ) 2 : ( x′ , y′ ) ← solve LP only on points S with objective function ∑ v , u∈S d ( v , u ) pxvu|D ( v ) | 3 : x∗vw ← x′uw∀v , w ∈ X , u ∈ S : v ∈ D ( u ) ▷ v ’ s assignment is identical to its representative u 4 : y∗u ← y′u if u ∈ S and 0 otherwise Output : Fair-Round ( ( X , d ) , ( 1 + δ ) r , ( x∗ , y∗ ) ) Lemma 5 . Algorithm 3 outputs an ( 8 ( 1 + δ ) , 21+2/p ( 1 + ( δϕ ) p ) 1/p ) -approximation where ϕ = ( ∑ v∈X r ( v ) p ) 1/p/opt . Proof . Observe that ( x∗ , y∗ ) is not a feasible LP solution anymore . Nevertheless , it will be feasible for when r is dilated by a factor of ( 1 + δ ) in the LP4 . Here is how we argue LP4 holds in this case : For any v , w ∈ X for which x∗vw > 0 , recall u ∈ S is chosen so v ∈ D ( u ) and x∗vw : = x′uw > 0 meaning d ( u , w ) ≤ r ( u ) . We have d ( v , w ) ≤ d ( v , u ) +d ( u , w ) ≤ δr ( v ) + r ( u ) ≤ ( 1+ δ ) r ( v ) . The last two inequalities are by Fact 2 as d ( v , u ) ≤ δr ( v ) and r ( u ) ≤ r ( v ) . As for the LP cost of ( x∗ , y∗ ) , it is not longer upper-bounded by optp but rather by optp + ∑ u∈S ∑ v∈D ( u ) d ( v , u ) p. This additive term is at most ∑ v∈X ( δr ( v ) ) p ≤ ( δϕopt ) p. Plugging this into Lemma 4 finishes the proof .","We study data clustering problems with lp-norm objectives (e.g. k-MEDIAN and k-MEANS) in the context of individual fairness. The dataset consists of n points, and we want to find k centers such that (a) the objective is minimized, while (b) respecting the individual fairness constraint that every point v has a center within a distance at most r(v), where r(v) is v’s distance to its (n/k)th nearest point. Jung, Kannan, and Lutz [FORC 2020] introduced this concept and designed a clustering algorithm with provable (approximate) fairness and objective guarantees for the l∞ or k-CENTER objective. Mahabadi and Vakilian [ICML 2020] revisited this problem to give a local-search algorithm for all lp-norms. Empirically, their algorithms outperform Jung et. al.’s by a large margin in terms of cost (for kMEDIAN and k-MEANS), but they incur a reasonable loss in fairness. In this paper, our main contribution is to use Linear Programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. We prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal. Furthermore, our theoretical fairness guarantees are comparable with MV20 in theory, and empirically, we obtain noticeably fairer solutions. Although solving the LP exactly might be prohibitive, we demonstrate that in practice, a simple sparsification technique drastically improves the run-time of our algorithm.","Sure, here is a summary of the article in 2 sentences: The paper addresses the problem of fair clustering, which aims to minimize the clustering cost while ensuring that the distance between the centroid of each cluster and its representative point is close to a given radius. The authors propose an (8,8)-approximation algorithm for the FAIR-k-MEDIAN problem, which is a variant of the clustering problem that takes into account fairness considerations.","The paper presents improved algorithms for the Fair Individual k-Center Clustering problem - a problem in which a solution is deemed fair if there is a facility within certain radius around each point, defined in a metric space, and each point is connected to an assigned facility. The algorithms are based on solving a linear program and achieve an 8-approximation for both cost and fairness, with empirical results demonstrating that they perform better than previous work.",The paper studies the problem of \ell_p norm and center-based clustering with a fairness consistent. The fairness constraint roughly says that no point should be further from its assigned center than it is from its (n/k)-th furthest neighbor. The authors consider a Linear Programming based (approximation) algorithm for this problem.,"2,1,0",,,,,,,,,,,,,,,,,
31,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"1 Introduction . The question of which functions can be well approximated by neural networks is crucial for understanding when these models are successful , and has always been at the heart of the theoretical study of neural networks ( e.g. , Hornik et al. , 1989 ; Pinkus , 1999 ) . While early works have mostly focused on shallow networks with only two layers , more recent works have shown benefits of deep networks for approximating certain classes of functions ( Eldan & Shamir , 2016 ; Mhaskar & Poggio , 2016 ; Telgarsky , 2016 ; Daniely , 2017 ; Yarotsky , 2017 ; Schmidt-Hieber et al. , 2020 ) . Unfortunately , many of these approaches rely on constructions that are not currently known to be learnable using efficient algorithms . A separate line of work has considered over-parameterized networks with random neurons ( Neal , 1996 ) , which also display universal approximation properties while additionally providing efficient algorithms based on kernel methods or their approximations such as random features ( Rahimi & Recht , 2007 ; Bach , 2017b ) . Many recent results on gradient-based optimization of certain over-parameterized networks have been shown to be equivalent to kernel methods with an architecture-specific kernel called the neural tangent kernel ( NTK ) and thus also fall in this category ( e.g. , Jacot et al. , 2018 ; Li & Liang , 2018 ; Allen-Zhu et al. , 2019b ; Du et al. , 2019a ; b ; Zou et al. , 2019 ) . This regime has been coined lazy ( Chizat et al. , 2019 ) , as it does not capture the common phenomenon where weights move significantly away from random initialization and thus may not provide a satisfying model for learning adaptive representations , in contrast to other settings such as the mean field or active regime , which captures complex training dynamics where weights may move in a non-trivial manner and adapt to the data ( e.g. , Chizat & Bach , 2018 ; Mei et al. , 2018 ) . Nevertheless , ∗Work done while at Inria . †Center for Data Science , New York University . New York , USA . ‡Inria - Département d ’ Informatique de l ’ École Normale Supérieure . PSL Research University . Paris , France . one benefit compared to the mean field regime is that the kernel approach easily extends to deep architectures , leading to compositional kernels similar to the ones of Cho & Saul ( 2009 ) ; Daniely et al . ( 2016 ) . Our goal in this paper is to study the role of depth in determining approximation properties for such kernels , with a focus on fully-connected deep ReLU networks . Our approximation results rely on the study of eigenvalue decays of integral operators associated to the obtained dot-product kernels on the sphere , which are diagonalized in the basis of spherical harmonics . This provides a characterization of the functions in the corresponding reproducing kernel Hilbert space ( RKHS ) in terms of their smoothness , and leads to convergence rates for non-parametric regression when the data are uniformly distributed on the sphere . We show that for ReLU networks , the eigenvalue decays for the corresponding deep kernels remain the same regardless of the depth of the network . Our key result is that the decay for a certain class of kernels is characterized by a property related to differentiability of the kernel function around the point where the two inputs are aligned . In particular , the property is preserved when adding layers with ReLU activations , showing that depth plays essentially no role for such networks in kernel regimes . This highlights the limitations of the kernel regime for understanding the power of depth in fully-connected networks , and calls for new models of deep networks beyond kernels ( see , e.g. , Allen-Zhu & Li , 2020 ; Chen et al. , 2020 , for recent works in this direction ) . We also provide applications of our result to other kernels and architectures , and illustrate our results with numerical experiments on synthetic and real datasets . Related work . Kernels for deep learning were originally derived by Neal ( 1996 ) for shallow networks , and later for deep networks ( Cho & Saul , 2009 ; Daniely et al. , 2016 ; Lee et al. , 2018 ; Matthews et al. , 2018 ) . Smola et al . ( 2001 ) ; Minh et al . ( 2006 ) study regularization properties of dot-product kernels on the sphere using spherical harmonics , and Bach ( 2017a ) derives eigenvalue decays for such dot-product kernels arising from shallow networks with positively homogeneous activations including the ReLU . Extensions to shallow NTK or Laplace kernels are studied by Basri et al . ( 2019 ) ; Bietti & Mairal ( 2019b ) ; Geifman et al . ( 2020 ) . The observation that depth does not change the decay of the NTK was previously made by Basri et al . ( 2020 ) empirically , and Geifman et al . ( 2020 ) provide a lower bound on the eigenvalues for deep networks ; our work makes this observation rigorous by providing tight asymptotic decays . Spectral properties of wide neural networks were also considered in ( Cao et al. , 2019 ; Fan & Wang , 2020 ; Ghorbani et al. , 2019 ; Xie et al. , 2017 ; Yang & Salman , 2019 ) . Azevedo & Menegatto ( 2014 ) ; Scetbon & Harchaoui ( 2020 ) also study eigenvalue decays for dot-product kernels but focus on kernels with geometric decays , while our main focus is on polynomial decays . Additional works on over-parameterized or infinite-width networks in lazy regimes include ( Allen-Zhu et al. , 2019a ; b ; Arora et al. , 2019a ; b ; Brand et al. , 2020 ; Lee et al. , 2020 ; Song & Yang , 2019 ) . Concurrently to our work , Chen & Xu ( 2021 ) also studied the RKHS of the NTK for deep ReLU networks , showing that it is the same as for the Laplace kernel on the sphere . They achieve this by studying asymptotic decays of Taylor coefficients of the kernel function at zero using complex-analytic extensions of the kernel functions , and leveraging this to obtain both inclusions between the two RKHSs . In contrast , we obtain precise descriptions of the RKHS and regularization properties in the basis of spherical harmonics for various dot-product kernels through spectral decompositions of integral operators , using ( real ) asymptotic expansions of the kernel function around endpoints . The equality between the RKHS of the deep NTK and Laplace kernel then easily follows from our results by the fact that the two kernels have the same spectral decay . 2 Review of Approximation with Dot-Product Kernels . In this section , we provide a brief review of the kernels that arise from neural networks and their approximation properties . 2.1 Kernels for wide neural networks . Wide neural networks with random weights or weights close to random initialization naturally lead to certain dot-product kernels that depend on the architecture and activation function , which we now present , with a focus on fully-connected architectures . Random feature kernels . We first consider a two-layer ( shallow ) network of the form f ( x ) = 1√ m ∑m j=1 vjσ ( w > j x ) , for some activation function σ . When wj ∼ N ( 0 , I ) ∈ Rd are fixed and only vj ∈ R are trained with ` 2 regularization , this corresponds to using a random feature approximation Rahimi & Recht ( 2007 ) of the kernel k ( x , x′ ) = Ew∼N ( 0 , I ) [ σ ( w > x ) σ ( w > x′ ) ] . ( 1 ) If x , x′ are on the sphere , then by spherical symmetry of the Gaussian distribution , one may show that k is invariant to unitary transformations and takes the form k ( x , x′ ) = κ ( x > x′ ) for a certain function κ . More precisely , if σ ( u ) = ∑ i≥0 aihi ( u ) is the decomposition of σ in the basis of Hermite polynomials hi , which are orthogonal w.r.t . the Gaussian measure , then we have ( Daniely et al. , 2016 ) : κ ( u ) = ∑ i≥0 a2iu i . ( 2 ) Conversely , given a kernel function of the form above with κ ( u ) = ∑ i≥0 biu i with bi ≥ 0 , one may construct corresponding activations using Hermite polynomials by taking σ ( u ) = ∑ i aihi ( u ) , ai ∈ { ± √ bi } . ( 3 ) In the case where σ is s-positively homogeneous , such as the ReLU σ ( u ) = max ( u , 0 ) ( with s = 1 ) , or more generally σs ( u ) = max ( u , 0 ) s , then the kernel ( 1 ) takes the form k ( x , x′ ) = ‖x‖s‖x′‖sκ ( x > x′‖x‖‖x′‖ ) for any x , x′ . This leads to RKHS functions of the form f ( x ) = ‖x‖sg ( x‖x‖ ) , with g in the RKHS of the kernel restricted to the sphere ( Bietti & Mairal , 2019b , Prop . 8 ) . In particular , for the step and ReLU activations σ0 and σ1 , the functions κ are given by the following arc-cosine kernels ( Cho & Saul , 2009 ) :1 κ0 ( u ) = 1 π ( π − arccos ( u ) ) , κ1 ( u ) = 1 π ( u · ( π − arccos ( u ) ) + √ 1− u2 ) . ( 4 ) Note that given a kernel function κ , the corresponding activations ( 3 ) will generally not be homogeneous , thus the inputs to a random network with such activations need to lie on the sphere ( or be appropriately normalized ) in order to yield the kernel κ . Extension to deep networks . When considering a deep network with more than two layers and fixed random weights before the last layer , the connection to random features is less direct since the features are correlated through intermediate layers . Nevertheless , when the hidden layers are wide enough , one still approaches a kernel obtained by letting the widths go to infinity ( see , e.g. , Daniely et al. , 2016 ; Lee et al. , 2018 ; Matthews et al. , 2018 ) , which takes a similar form to the multi-layer kernels of Cho & Saul ( 2009 ) : kL ( x , x′ ) = κL ( x > x′ ) : = κ ◦ · · · ◦ κ︸ ︷︷ ︸ L−1 times ( x > x′ ) , for x , x′ on the sphere , where κ is obtained as described above for a given activation σ , and L is the number of layers . We still refer to this kernel as the random features ( RF ) kernel in this paper , noting that it is sometimes known as the “ conjugate kernel ” or NNGP kernel ( for neural network Gaussian process ) . It is usually good to normalize κ such that κ ( 1 ) = 1 , so that we also have κL ( 1 ) = 1 , avoiding exploding or vanishing behavior for deep networks . In practice , this corresponds to using an activation-dependent scaling in the random weight initialization , which is commonly used by practitioners ( He et al. , 2015 ) . 1Here we assume a scaling √ 2/m instead of √ 1/m in the definition of f , which yields κ ( 1 ) = 1 , a useful normalization for deep networks , as explained below . Neural tangent kernels . When intermediate layers are trained along with the last layer using gradient methods , the resulting problem is non-convex and the statistical properties of such approaches are not well understood in general , particularly for deep networks . However , in a specific over-parameterized regime , it may be shown that gradient descent can reach a global minimum while keeping weights very close to random initialization . More precisely , for a network f ( x ; θ ) parameterized by θ with large width m , the model remains close to its linearization around random initialization θ0 throughout training , that is , f ( x ; θ ) ≈ f ( x ; θ0 ) + 〈θ− θ0 , ∇θf ( x ; θ0 ) 〉 . This is also known as the lazy training regime ( Chizat et al. , 2019 ) . Learning is then equivalent to a kernel method with another architecture-specific kernel known as the neural tangent kernel ( NTK , Jacot et al. , 2018 ) , given by kNTK ( x , x′ ) = lim m→∞ 〈∇f ( x ; θ0 ) , ∇f ( x′ ; θ0 ) 〉 . ( 5 ) For a simple two-layer network with activation σ , it is then given by kNTK ( x , x′ ) = ( x > x′ ) Ew [ σ′ ( w > x ) σ′ ( w > x′ ) ] + Ew [ σ ( w > x ) σ ( w > x′ ) ] . ( 6 ) For a ReLU network with L layers with inputs on the sphere , taking appropriate limits on the widths , one can show ( Jacot et al. , 2018 ) : kNTK ( x , x′ ) = κLNTK ( x > x′ ) , with κ1NTK ( u ) = κ1 ( u ) = u and for ` = 2 , . . . , L , κ ` ( u ) = κ1 ( κ ` −1 ( u ) ) κ ` NTK ( u ) = κ ` −1NTK ( u ) κ0 ( κ ` −1 ( u ) ) + κ ` ( u ) , ( 7 ) where κ0 and κ1 are given in ( 4 ) .","Deep networks are often considered to be more expressive than shallow ones in terms of approximation. Indeed, certain functions can be approximated by deep networks provably more efficiently than by shallow ones, however, no tractable algorithms are known for learning such deep models. Separately, a recent line of work has shown that deep networks trained with gradient descent may behave like (tractable) kernel methods in a certain over-parameterized regime, where the kernel is determined by the architecture and initialization, and this paper focuses on approximation for such kernels. We show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their “shallow” two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. Our main theoretical result relies on characterizing such eigenvalue decays through differentiability properties of the kernel function, which also easily applies to the study of other kernels defined on the sphere.","The article discusses the question of which functions can be well approximated by neural networks, which is crucial for understanding when these models are successful. The authors investigate the role of depth in determining approximation properties for dot-product kernels related to fully-connected deep ReLU networks. They show that, for ReLU networks, the eigenvalue decays for the corresponding deep kernels remain the same regardless of the depth of the network, with the decay for a certain class of kernels characterised by a property related to differentiability of the kernel function. The authors conclude that the kernel regime has limitations for understanding the power of depth in fully-connected networks and call for new models of deep networks beyond kernels. The article highlights the limitations of the kernel regime and provides a brief review of the kernels that arise from neural networks and their approximation properties.","This paper analyzed the expressive power of kernels by studying the reproducing kernel Hilbert space (RKHS) associated with the kernels. Specifically, the authors analyzed the eigenvalue decays in terms of the power series expansions of the kernel function around some points, which is related to the RKHS of the kernel. This analysis can be used to recover some previous results. Besides, using this analysis, the authors have shown several interesting results, including that NTK (which corresponds to fully-connected ReLU networks with infinite width, small learning rate, and proper initialization) with any depth has the same RKHS. The main result also has other corollaries about other kinds of kernels, e.g., Laplace kernel and infinitely differentiable kernels. Experiments were done to validate the theoretical results on synthetic datasets and MNIST/Fashion-MNIST.","The article discusses the approximation properties of neural networks, specifically the kernel regime, which is a regime where the weights of the network are close to random initialization. The authors show that for ReLU networks, the eigenvalue decays of the integral operators associated to the obtained dot-product kernels on the sphere remain the same regardless of the depth of the network. They provide a characterization of the functions in the corresponding reproducing kernel Hilbert space (RKHS) in terms of their smoothness, and illustrate their results with numerical experiments on synthetic and real datasets. The article also compares the approximation properties of the neural tangent kernel (NTK) and the Laplace kernel, showing that they have the same spectral decay. Finally, the authors highlight the limitations of the kernel regime for understanding the power of depth in fully-connected networks, and call for new models of deep networks beyond kernels (e.g., Allen-Zhu & Li, 2020; Chen et al., 2020).","1,0,2",1,0,2,1,,coverage,clarity,"coherence, factuality",relevance,0,1,2,0,Very good coverage and balance of details.,"Not a bad summary, but it fails to state the motivation more clearly.",Summary C is most coherent and most accurate in describing the main topic of the work. Although there are also some facts mentioned that cannot be verified using the abstract. ,Summary A's point about the main topic of the paper seems to be off the point mentioned in the abstract. 
32,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"semantics = α1Emb ( That ) + α2Emb ( ’ s ) + · · ·+ αnEmb ( her ) . But the weighted sum of “ That ’ s ” , “ the ” , “ last ” , “ straw ” can hardly represent the semantics of the phrase . We argue that the phrases play an important role in attention . If we combine some words into phrases , a more reasonable representation with compositions is semantics = α1Emb ( That ’ s ) +Emb2 ( the last straw ) +α3Emb ( for ) +α4Emb ( her ) . While recent studies prefer to use the attention mechanism to represent the natural language , few noticed the word compositions . In this paper , we study the problem of representing such compositional attentions in phrases . In this paper , we proposed a new attention architecture called PhraseTransformer . Besides representing the words of the sentence , we introduce hypernodes to represent the candidate phrases in attention . PhraseTransformer has two phases . The first phase is used to attend over all word/phrase pairs , which is similar to the standard Transformer . The second phase is used to represent the inductive bias within each phrase . Specially , we incorporate the non-linear attention in the second phase . The non-linearity represents the the semantic mutations in phrases . The experimental performance has been greatly improved . In WMT16 English-German translation task , the BLEU increases from 20.90 ( by Transformer ) to 34.61 ( by PhraseTransformer ) . 1 INTRODUCTION . The word-to-word attention ( Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) updates the state of a word by the weighted sum of the current states of the words . The weights depend on the attention scores . Such mechanism incurs two major problems . Firstly , it is unfeasible to incorporate the semantics of phrases in the attention . For example , in figure 1a and figure 1b , using the attention over phrase “ the last straw ” is more intuitive than using the attention over the three individual words . However , such word composition ( i.e . “ the last straw ” ) can not be represented atomically by word level attentions . Secondly , the literal meaning of each individual words can be quite different from the implied meaning of the phrase . The weighted sum in attention can not capture the semantic mutation . For example , it ’ s hard to represent the semantics of the unpleasantness , the struggling , and being unable to continue by the weighted sum of individual words “ the ” , “ last ” , and “ straw ” . Deepening the attention layers as in Devlin et al . ( 2019 ) ; Liu et al . ( 2019 ) can not solve these issues . Instead of representing the natural language via word-to-word attentions , incorporating phrase as atoms in the attention provides a more intuitive representation . In figure 1b , if we regard “ the last straw ” as an atom node in the attention in figure 1b , it is much more intuitive to represent the semantics . For multiple words that form semantic compositions ( e.g . idioms , chunkings , phrases ) , their semantics should be computed uniformly as an atom in the attention mechanism . The traditional word level attention mechanism should be expanded to represent the attentions over such compositions . To this end , we make a very simple framework called PhraseTransformer , based on Transformer . We leverage the concept of hypernodes in hypergraph . The traditional Transformer only consists of That ’ s the last straw for her . semantics = α1Emb ( That ’ s ) + α2Emb ( the ) + · · ·+ αnEmb ( her ) ( a ) Word level self-attention That ’ s the last straw for her . semantics = α1Emb ( That ’ s ) + α2Emb ( the last straw ) + α2Emb ( for ) + αnEmb ( her ) ( b ) Attention over adjacent phrases . Representation learning is not only popular but also important . semantics =α1Emb ( Representation learning ) + α2Emb ( is ) + α3Emb ( not only ... but also ) + α3Emb ( hot ) + α4Emb ( important ) ( c ) Attention over nonadjacent phrases . nodes representing single words . From the perspective of graph , Transformer is a complete bipartite graph , that the information transfers through all node pairs according to their attention scores . In hypergraph , a hypernode consists of one or more nodes . Likewise , we use such hypernodes in PhraseTransformer to represent the semantics of the composition . Each hypernode represents multiple words in a composition . The information propagation in PhraseTransformer has two stages : the propagation across arbitrary node pairs and the propagation within a hypernode . Attention of arbitrary node pairs works similar to the classical Transformer . We compute the attention scores for all nodes and all hypernodes . The information propagates accordingly . We use hypernodes to represent the atomic information propagation of semantic compositions in attentions . Non-linear attention within each phrase reflects the inductive bias of the phrases . The nodes within a phrase should be updated by each other via attention . For example , the states of the hypernode the last straw should be updated by the node of the , last , straw etc . And the states of node straw should also be updated by the node of the last straw . To do this , we limit the attention in Transformer to nodes within each phrase . Compared to the attention of arbitrary node pairs , a significant difference of the attention within each phrase is that we incorporate nonlinearity . As suggested in figure 1a , the literal meaning of each individual words can be quite different from the implied meaning of the phrase . We use the non-linearity to represent the semantic mutations of phrases . Our proposed PhraseTransformer is able to capture any multi-word compositions . In this paper , we mainly study one of the simplest form of phrase : a sequence of adjacent words . For detecting phrases with adjacent words , we do not utilize existing identification algorithms ( e.g . chunking ) . We roughly added all sequences of words in the sentence whose length is less than a given threshold . The attention mechanism itself decides the weights of these candidate phrases by assigning them different attention scores during the inference . We summarize the contributions of this paper below : • We propose to represent semantic compositions in the attention mechanism . We show that such representation is more intuitive than the traditional word-to-word attention . • We incorporate the hypernodes in Transformer as PhraseTransformer . The semantics of a composition can be computed atomically via cross-node attentions . The inductive bias is reflected by the information propagation within each hypernode . • We evaluate our approach over two widely used types of semantic compositions , i.e. , chunking and semantic dependency . We conduct extensive experiments to verify the effectiveness of our proposed approaches . 2 MODEL . 2.1 ARCHITECTURE . In this subsection , we show how the PhraseTransformer works on an abstract level . In the attention , we use “ simple node ” and “ hypernode ” to denote nodes that correspond to a single word and a phrase , respectively . We denote both the simple nodes and hypernodes as nodes . The PhraseTransformer contains n simple nodes , where n is the length of the sentence , and m hypernodes representing m phrases . For each node s , we denote the words it corresponds to as word ( s ) . For simple nodes , |word ( s ) | = 1 . For hypernodes , |word ( s ) | > 1 . For example , in figure 2 , “ straw ” is represented by a single node s1 . word ( s1 ) = ( straw ) . The phrase “ the last straw ” is represented by a hypernode s2 , word ( s2 ) = ( the , last , straw ) . The attention in PhraseTransformer has two phases . The first phase uses the node-to-node attention over all node pairs . This is represented by the blue edges in figure 2 . For example , the state of the placeholder [ CLS ] will be updated according to its attention scores to nodes of “ That ’ s ” , “ for ” , “ the last straw ” , etc . Similarly , the states of a hypernode ( e.g . “ the last straw ” ) will be updated according to its attention scores to all nodes ( e.g . “ That ’ s ” , “ for ” , “ last straw ” ) . The second phase uses limited attention mechanism to update the states within each phrase . In figure 2 , the orange edges show the attention within the phrase “ the last straw ” . The state of s2 will be updated according to its attention scores to “ the ” , “ last ” , “ straw ” , but will not be updated by “ for ” or “ her ” . And the state of s1 will also be updated by s2 . 2.2 PHRASE GENERATION . As mentioned in the introduction section , a hypernode represents a phrase formed by multiple words . In this paper , we consider the phrases formed by adjacent words in the original sentences . For a given length threshold k and a word sequence w1 , · · · , wn , we generate hypernodes that consists of adjacent word sequence whose length is less than or equal to k. More specifically , we generate all hypernode s such that word ( s ) = ( wi , wi+1 , · · ·wj ) , s.t . j − i < k ( 1 ) Our proposed neural network is feasible for any phrase generation strategies . We also compare with other the phrase generation w.r.t . the dependency parsing trees . The details of such strategy are shown in section 4.3 . 2.3 MUTLI-HEAD ATTENTION OVER ALL NODE PAIRS . The attention over all node pairs in the first phase works similar to the standard Transformer ( Vaswani et al. , 2017 ) . Given a sequence of vectors H ∈ Rn+m , where m is the number of hypernodes , we compute the attention of all nodes by : Attn ( Q , K , V ) = softmax ( QKᵀ√ d ) V ( 2 ) , where Q = HWQ , K = HWK , V = HWV , and WQ , WK , WV are trainable parameters . By following Transformer ( Vaswani et al. , 2017 ) , we use multi-head attention to jointly attend to different representation subspaces : MultiHead ( H ) = Concat ( head1 , · · · , headh ) WO ( 3 ) , where headi = Attn ( HW Q i , HW K i , HW V i ) , h is the number of heads . 2.4 NON-LINEAR ATTENTION WITHIN EACH PHRASE . The attention within each phrase captures the inductive bias of the phrase . For example , we use the semantics of “ representation ” , “ learning ” to update the semantics of phrase “ representation learning ” , and vice versa . We consider two nodes have an edge between them if the words of one node is the subset of the words of another node . More specific , for two nodes s1 , s2 , we consider their attention within a phrase iff word ( s1 ) ⊆ word ( s2 ) or word ( s2 ) ⊆Word ( s1 ) ( 4 ) . Given a sequence of vectors H ∈ Rn+m , the attention within phrases are computed by : Attn′ ( Q , K , V ) = σ ( softmax ( QKᵀ√ d ◦A ) V ) ( 5 ) where Q = HWQ ′ , K = HWK ′ , V = HWV ′ , ◦ denotes the Hadamard product , A ∈ [ 0 , 1 ] ( n+m ) × ( n+m ) denotes the adjacency of the nodes : Ai , j = { 1 if word ( s1 ) ⊆ word ( s2 ) or word ( s2 ) ⊆ word ( s1 ) 0 otherwise . ( 6 ) Note that we use an active function σ in Attn′ ( Q , K , V ) , which is not used in the standard Transformer . We found that the literal meaning of each individual words can be quite different from the implied meaning of the phrase . For example , in figure 1a , the semantics of “ the last straw ” is not a simple combination of the semantics of “ the ” , “ last ” , “ straw ” . So using a weighted sum of the three words to represent the phrase can not precisely represent its semantics . We use the non-linearity to represent the semantic mutation . In this paper , we use the sigmoid function as σ . We also use multi-head attention in the attention within phrases to jointly attend to different representation subspaces : : MultiHead′ ( H , A ) = Concat ( head′1 , · · · , head′h ) WO ( 7 ) where head′i = Attn ′ ( QWQ ′ i , KW K′ i , V W V ′ i ) .","How to represent the sentence “That’s the last straw for her”? The answer of the self-attention is a weighted sum of each individual words, i.e. semantics = α1Emb(That) + α2Emb(’s) + · · ·+ αnEmb(her) . But the weighted sum of “That’s”, “the”, “last”, “straw” can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention. If we combine some words into phrases, a more reasonable representation with compositions is semantics = α1Emb(That’s)+Emb2(the last straw)+α3Emb(for)+α4Emb(her) . While recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called PhraseTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. PhraseTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by PhraseTransformer).","The article proposes a new attention mechanism called PhraseTransformer, which represents the semantics of phrases in a more intuitive way than traditional word-to-word attention. PhraseTransformer incorporates hypernodes to represent the atomic information propagation of semantic compositions in attentions, and non-linear attention within each phrase reflects the inductive bias of the phrases. The proposed approach is evaluated over two types of semantic compositions, chunking and semantic dependency, and shows improved performance compared to the standard Transformer.","The article proposes a new attention architecture called PhraseTransformer which aims to address the shortcomings of the traditional word-to-word attention mechanism in the representation of semantic compositions. The PhraseTransformer uses hypernodes to represent the atomic information propagation of semantic compositions in attention, and the attention mechanism is expanded to represent attentions over such compositions. Experimental results show that the proposed approach effectively captures multi-word compositions and achieves better performance than the standard Transformer in machine translation tasks.","This paper addresses an issue of compositionality in self-attention models such Transformer. A simple idea of composing multiple words into a phrase as a hypernode and representing it using a non-linear function to capture the semantic mutation is proposed. In the machine translation and PoS tagging tasks, the proposed PhraseTransformer achieves impressive gain, especially +13.7 BLEU score compared to the Transformer.","2,1,0",1,"2,0",1,2,,coverage,coverage,coverage,factuality,1,"0,2",1,0,Summary B has good coverage and better represents the experimental results.,"Summary A explains better the motivation and method but not the results. Summary C, the opposite.",Summary B represents the main motivation and topic of the paper the best. ,Summary A includes non-factual information and omit the results on translation tasks. 
33,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"1 Introduction . The transformer architecture , originally designed for the natural language processing ( NLP ) tasks [ 42 ] , has shown promising performance on various vision problems recently [ 10 , 40 , 27 , 49 , 4 , 47 , 35 , 5 ] . Different from convolutional neural networks ( CNNs ) , vision transformer models use self-attention layers to capture long-term dependencies , which are able to learn more diverse interactions between spatial locations . The pure multi-layer perceptrons ( MLP ) models [ 38 , 39 ] further simplify the vision transformers by replacing the self-attention layers with MLPs that are applied across spatial locations . Since fewer inductive biases are introduced , these two kinds of models have the potential to learn more generic and flexible interactions among spatial locations from raw data . One primary challenge of applying self-attention and pure MLP models to vision tasks is the considerable computational complexity that grows quadratically as the number of tokens increases . Therefore , typical vision transformer style models usually consider a relatively small resolution for the intermediate features ( e.g . 14× 14 tokens are extracted from the input images in both ViT [ 10 ] and MLP-Mixer [ 38 ] ) . This design may limit the applications of downstream dense prediction tasks like detection and segmentation . A possible solution is to replace the global self-attention with several local self-attention like Swin transformer [ 27 ] . Despite the effectiveness in practice , local self-attention brings quite a few hand-made choices ( e.g. , window size , padding strategy , etc . ) and limits the receptive field of each layer . ∗Equal contribution . †Corresponding author . 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . In this paper , we present a new conceptually simple yet computationally efficient architecture called Global Filter Network ( GFNet ) , which follows the trend of removing inductive biases from vision models while enjoying the log-linear complexity in computation . The basic idea behind our architecture is to learn the interactions among spatial locations in the frequency domain . Different from the self-attention mechanism in vision transformers and the fully connected layers in MLP models , the interactions among tokens are modeled as a set of learnable global filters that are applied to the spectrum of the input features . Since the global filters are able to cover all the frequencies , our model can capture both long-term and short-term interactions . The filters are directly learned from the raw data without introducing human priors . Our architecture is largely based on the vision transformers only with some minimal modifications . We replace the self-attention sub-layer in vision transformers with three key operations : a 2D discrete Fourier transform to convert the input spatial features to the frequency domain , an element-wise multiplication between frequency-domain features and the global filters , and a 2D inverse Fourier transform to map the features back to the spatial domain . Since the Fourier transform is used to mix the information of different tokens , the global filter is much more efficient compared to the self-attention and MLP thanks to the O ( L logL ) complexity of the fast Fourier transform algorithm ( FFT ) [ 7 ] . Benefiting from this , the proposed global filter layer is less sensitive to the token length L and thus is compatible with larger feature maps and CNN-style hierarchical architectures without modifications . The overall architecture of GFNet is illustrated in Figure 1 . We also compare our global filter with prevalent operations in deep vision models in Table 1 . Our experiments on ImageNet verify the effectiveness of GFNet . With a similar architecture , our model outperform the recent vision transformer and MLP models including DeiT [ 40 ] , ResMLP [ 39 ] and gMLP [ 26 ] . When using the hierarchical architecture , GFNet can further enlarge the gap . GFNet also works well on downstream transfer learning and semantic segmentation tasks . Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency , generalization ability and robustness . 2 Related works . Vision transformers . Since Dosovitskiy et al . [ 10 ] introduce transformers to the image classification and achieve a competitive performance compared to CNNs , transformers begin to exhibit their potential in various vision tasks [ 3 , 4 , 49 ] . Recently , there are a large number of works which aim to improve the transformers [ 40 , 41 , 27 , 44 , 18 , 11 , 48 ] . These works either seek for better training strategies [ 40 , 11 ] or design better architectures [ 27 , 44 , 48 ] or both [ 41 , 11 ] . However , most of the architecture modification of the transformers [ 44 , 18 , 27 , 48 ] introduces additional inductive biases similar to CNNs . In this work , we only focus on the standard transformer architecture [ 10 , 40 ] and our goal is to replace the heavy self-attention layer ( O ( L2 ) ) to an more efficient operation which can still model the interactions among different spatial locations without introducing the inductive biases associated with CNNs . MLP-like models . More recently , there are several works that question the importance of selfattention in the vision transformers and propose to use MLP to replace the self-attention layer in the transformers [ 38 , 39 , 26 ] . The MLP-Mixer [ 38 ] employs MLPs to perform token mixing and channel mixing alternatively in each block . ResMLP [ 39 ] adopts a similar idea but substitutes the Layer Normalization with an Affine transformation for acceleration . The recently proposed gMLP [ 26 ] uses a spatial gating unit to re-weight tokens in the spatial dimension . However , all of the above models include MLPs to mix the tokens spatially , which brings two drawbacks : ( 1 ) like the self-attention in the transformers , the spatial MLP still requires computational complexity quadratic to the length of tokens . ( 2 ) unlike transformers , MLP models are hard to scale up to higher resolution since the weights of the spatial MLPs have fixed sizes . Our work follows this trend and successfully resolves the above issues in MLP-like models . The proposed GFNet enjoys log-linear complexity and can be easily scaled up to any resolution . Applications of Fourier transform in vision . Fourier transform has been an important tool in digital image processing for decades [ 32 , 1 ] . With the breakthroughs of CNNs in vision [ 14 , 13 ] , there are a variety of works that start to incorporate Fourier transform in some deep learning method [ 24 , 46 , 9 , 22 , 6 ] for vision tasks . Some of these works employ discrete Fourier transform to convert the images to the frequency domain and leverage the frequency information to improve the performance in certain tasks [ 22 , 46 ] , while others utilize the convolution theorem to accelerate the CNNs via fast Fourier transform ( FFT ) [ 24 , 9 ] . FFC [ 6 ] replaces the convolution in CNNs with an Local Fourier Unit and perform convolutions in the frequency domain . Very recent works also try to leverage Fourier transform to develop deep learning models to solve partial differential equations [ 25 ] and NLP tasks [ 23 ] . In this work , we propose to use learnable filters to interchange information globally among the tokens in the Fourier domain , inspired by the frequency filters in the digital image processing [ 32 ] . We also take advantage of some properties of FFT to reduce the computational costs and the number of parameters . 3 Method . 3.1 Preliminaries : discrete Fourier transform . We start by introducing the discrete Fourier transform ( DFT ) , which plays an important role in the area of digital signal processing and is a crucial component in our GFNet . For clarity , We first consider the 1D DFT . Given a sequence of N complex numbers x [ n ] , 0 ≤ n ≤ N − 1 , the 1D DFT converts the sequence into the frequency domain by : X [ k ] = N−1∑ n=0 x [ n ] e−j ( 2π/N ) kn : = N−1∑ n=0 x [ n ] W knN ( 3.1 ) Algorithm 1 Pseudocode of Global Filter Layer . # x : the token features , B x H x W x D ( where N = H * W ) # K : the frequency-domain filter , H x W_hat x D ( where W_hat = W // 2 + 1 , see Section 3.2 for details ) X = rfft2 ( x , dim= ( 1 , 2 ) ) X_tilde = X * K x = irfft2 ( X_tilde , dim= ( 1 , 2 ) ) rfft2/irfft2 : 2D FFT/IFFT for real signal where j is the imaginary unit and WN = e−j ( 2π/N ) . The formulation of DFT in Equation ( 3.1 ) can be derived from the Fourier transform for continuous signal by sampling in both the time domain and the frequency domain ( see Appendix A for details ) . Since X [ k ] repeats on intervals of length N , it is suffice to take the value of X [ k ] at N consecutive points k = 0 , 1 , . . . , N − 1 . Specifically , X [ k ] represents to the spectrum of the sequence x [ n ] at the frequency ωk = 2πk/N . It is also worth noting that DFT is a one-to-one transformation . Given the DFT X [ k ] , we can recover the original signal x [ n ] by the inverse DFT ( IDFT ) : x [ n ] = 1 N N−1∑ k=0 X [ k ] ej ( 2π/N ) kn . ( 3.2 ) For real input x [ n ] , it can be proved that ( see Appendix A ) its DFT is conjugate symmetric , i.e. , X [ N − k ] = X∗ [ k ] . The reverse is true as well : if we perform IDFT to X [ k ] which is conjugate symmetric , a real discrete signal can be recovered . This property implies that the half of the DFT { X [ k ] : 0 ≤ k ≤ dN/2e } contains the full information about the frequency characteristics of x [ n ] . DFT is widely used in modern signal processing algorithms for mainly two reasons : ( 1 ) the input and output of DFT are both discrete thus can be easily processed by computers ; ( 2 ) there exist efficient algorithms for computing the DFT . The fast Fourier transform ( FFT ) algorithms take advantage of the symmetry and periodicity properties of W knN and reduce the complexity to compute DFT from O ( N2 ) to O ( N logN ) . The inverse DFT ( 3.2 ) , which has a similar form to the DFT , can also be computed efficiently using the inverse fast Fourier transform ( IFFT ) . The DFT described above can be extend to 2D signals . Given the 2D signal X [ m , n ] , 0 ≤ m ≤ M − 1 , 0 ≤ n ≤ N − 1 , the 2D DFT of x [ m , n ] is given by : X [ u , v ] = M−1∑ m=0 N−1∑ n=0 x [ m , n ] e−j2π ( um M + vn N ) . ( 3.3 ) The 2D DFT can be viewed as performing 1D DFT on the two dimensions alternatively . Similar to 1D DFT , 2D DFT of real input x [ m , n ] satisfied the conjugate symmetry property X [ M − u , N − v ] = X∗ [ u , v ] . The FFT algorithms can also be applied to 2D DFT to improve computational efficiency .","Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity trade-offs of our models on both ImageNet and downstream tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet.","In this article, the authors present a new architecture called Global Filter Network (GFNet) that can efficiently capture interactions among spatial locations in vision tasks without introducing inductive biases. GFNet replaces the self-attention layer in transformer models with learnable global filters that are applied to the spectrum of the input features in the frequency domain. The proposed architecture enjoys log-linear complexity and can be easily scaled up to any resolution, making it a competitive alternative to transformer-style models and CNNs. The article also explores the application of Fourier transform in vision tasks and demonstrates the effectiveness of GFNet in image classification, downstream transfer learning, and semantic segmentation tasks.","The article presents a new architecture called Global Filter Network (GFNet) that replaces self-attention layers with learnable filters to capture interactions between spatial locations in images. The filters are applied to the spectrum of input features, allowing GFNet to capture both short-term and long-term interactions. The authors demonstrate that GFNet outperforms recent vision transformer and MLP models and is also compatible with larger feature maps and CNN-style hierarchical architectures without modifications, making it an efficient and flexible alternative for dense prediction tasks. This approach provides a log-linear complexity in computation and eliminates the inductive biases associated with self-attention and MLP layers.","This paper proposes Global Filter Networks (GFNet), which make use of discrete Fourier transform (DFT) and a global filter layer to mix the input tokens. According to the convolution theorem, the combination of the two operations can be regarded as a depthwise global circular convolution, and thus this work can be viewed as a kind of convolutional network with global convolutional layers. By taking advantage of FFT, the proposed method is faster than previous methods such as self-attention and spatial MLP with comparable performance on ImageNet.","2,1,0",2,1,2,0,,"coverage, fluency",factuality,"coherence, coverage",informativeness,0,1,0,2,Summary A has better coverage and is well written (but I am not sure about the inductive bias argument. Introducing a FFT adds inductive bias),Summary B apparently has a factual error (that the methods outperform ViT and MLPs),Summary A is most coherent and best in representing the motivation of the paper. ,Summary C is too general and seems off the main point of the paper.
34,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"1 INTRODUCTION . As machine learning models continue to grow in number of parameters , there is a corresponding effort to try and reduce the ever-increasing memory and computational requirements that these models incur . One method to make models more efficient is to use neural networks with weights and possibly activations restricted to be binary-valued ( Courbariaux et al. , 2015 ; Courbariaux et al. , 2016 ; Rastegari et al. , 2016 ; McDonnell , 2018 ; Gu et al. , 2018 ) . Binary weights and activations require significantly less memory , and also admit faster low-level implementations of key operations such as linear transformations than when using the usual floating-point precision . Although the application of binary neural networks for classification is relatively well-studied , there has been no research that we are aware of that has examined whether binary neural networks can be used effectively in unsupervised learning problems . Indeed , many of the deep generative models that are popular for unsupervised learning do have high parameter counts and are computationally expensive ( Vaswani et al. , 2017 ; Maaløe et al. , 2019 ; Ho et al. , 2019a ) . These models would stand to benefit significantly from converting the weights and activations to binary values , which we call binarization for brevity . In this work we focus on non-autoregressive models with explicit densities . One such class of density model is the variational autoencoder ( VAE ) ( Kingma & Welling , 2014 ; Rezende et al. , 2014 ) , a latent variable model which has been used to model many high-dimensional data domains accurately . The state-of-the-art VAE models tend to have deep hierarchies of latent layers , and have demonstrated good performance relative to comparable modelling approaches ( Ranganath et al. , 2016 ; Kingma et al. , 2016 ; Maaløe et al. , 2019 ) . Whilst this deep hierarchy makes the model powerful , the model size and compute requirements increases with the number of latent layers , making very deep models resource intensive . Another class of density model which has shown promising results are flow-based generative models ( Dinh et al. , 2014 ; Rezende & Mohamed , 2015 ; Dinh et al. , 2017 ) . These models perform a series of invertible transformations to a simple density , with the transformed density approximating the data-generating distribution . Flow models which achieve state-of-the-art performance compose many transformations to give flexibility to the learned density ( Kingma & Dhariwal , 2018 ; Ho et al. , 2019a ) . Again the model computational cost increases as the number of transformations increases . To examine how to binarize hierarchical VAEs and flow models successfully , we take two models which have demonstrated excellent modelling performance - the ResNet VAE ( Kingma et al. , 2016 ) and the Flow++ model ( Ho et al. , 2019a ) - and implement the majority of each model with binary neural networks . Using binary weights and activations reduces the computational cost , but also decreases the representational capability of the model . Therefore our aim is to strike a balance between reducing the computational cost and maintaining good modelling performance . We show that it is possible to decrease the model size drastically , and allow for significant speed ups in run time , with only a minor impact on the achieved loss value . We make the following key contributions : • We propose an efficient binary adaptation of weight normalization , a reparameterization technique often used in deep generative models to accelerate convergence . Binary weight normalization is the generative-modelling alternative to the usual batch normalization used in binary neural networks . • We show that we can binarize the majority of weights and activations in deep hierarchical VAE and flow models , without significantly hurting performance . We demonstrate the corresponding binary architecture designs for both the ResNet VAE and the Flow++ model . • We perform experiments on different levels of binarization , clearly demonstrating the trade-off between binarization and performance . 2 BACKGROUND . In this section we give background on the implementation and training of binary neural networks . We also describe the generative models that we implement with binary neural networks in detail . 2.1 BINARY NEURAL NETWORKS . In order to reduce the memory and computational requirements of neural networks , there has been recent research into how to effectively utilise networks which use binary-valued weights wB and possibly also activations αB rather than the usual real-valued1 weights and activations ( Courbariaux et al. , 2015 ; Courbariaux et al. , 2016 ; Rastegari et al. , 2016 ; McDonnell , 2018 ; Gu et al. , 2018 ) . In this work , we use the convention of binary values being in B : = { −1 , 1 } . Motivation . The primary motivation for using binary neural networks is to decrease the memory and computational requirements of the model . Clearly binary weights require less memory to be stored : 32× less than the usual 32-bit floating-point weights . Binary neural networks also admit significant speed-ups . A reported 2× speed-up can be achieved by a layer with binary weights and real-valued inputs ( Rastegari et al. , 2016 ) . This can be made an additional 29× faster if the inputs to the layer are also constrained to be binary ( Rastegari et al. , 2016 ) . With both binary weights and inputs , linear operators such as convolutions can be implemented using the inexpensive XNOR and bit-count binary operations . A simple way to ensure binary inputs to a layer is to have a binary activation function before the layer ( Courbariaux et al. , 2016 ; Rastegari et al. , 2016 ) . Optimization . Taking a trained model with real-valued weights and binarizing the weights has been shown to be lead to significant worsening of performance ( Alizadeh et al. , 2019 ) . So instead the binary weights are optimized . It is common to not optimize the binary weights directly , but instead optimize a set of underlying real-valued weights wR which can then be binarized in some fashion for inference . In this paper we will adopt the convention of binarizing the underlying weights using the sign function ( see Equation 2 ) . We also use the sign function as the activation function when we use binary activations ( see Equation 5 , where αR are the real-valued pre-activations ) . We define the sign 1We use real-valued throughout the paper to be synonymous with `` implemented with floating-point precision '' . function as : sign ( x ) : = { −1 , if x < 0 1 , if x ≥ 0 ( 1 ) Since the derivative of the sign function is zero almost everywhere2 , the gradients of the underlying weights wR and through binary activations are zero almost everywhere . This makes gradient-based optimization challenging . To overcome this issue , the straight-through estimator ( STE ) ( Bengio et al. , 2013 ) can be used . When computing the gradient of the loss L , the STE replaces the gradient of the sign function ( or other discrete output functions ) with an approximate surrogate . A straightforward and widely used surrogate gradient is the identity function , which we use to calculate the gradients of the real-valued weights wR ( see Equation 3 ) . It has been shown useful to cancel the gradients when their magnitude becomes too large ( Courbariaux et al. , 2015 ; Alizadeh et al. , 2019 ) . Therefore we use a clipped identity function for the gradients of the pre-activations ( see Equation 6 ) . This avoids saturating a binary activation . Lastly , the loss value only depends on the sign of the real-valued weights . Therefore , the values of the weights are generally clipped to be in [ −1 , 1 ] after each gradient update ( see Equation 4 ) . This restricts the magnitude of the weights and thus makes it easier to flip the sign . Forward pass : Backward pass : After update : Weights wB = sign ( wR ) ( 2 ) ∂L ∂wR : = ∂L ∂wB ( 3 ) wR ← max ( −1 , min ( 1 , wR ) ) ( 4 ) Activations αB = sign ( αR ) ( 5 ) ∂L ∂αR : = ∂L ∂αB ∗ 1|αR|≤1 ( 6 ) − 2.2 DEEP GENERATIVE MODELS . Hierarchical VAEs . The variational autoencoder ( Kingma & Welling , 2014 ; Rezende et al. , 2014 ) is a latent variable model for observed data x conditioned on unobserved latent variables z . It consists of a generative model pθ ( x , z ) and an inference model qφ ( z|x ) . The generative model can be decomposed into the prior on the latent variables pθ ( z ) and the likelihood of our data given the latent variables pθ ( x|z ) . The inference model is a variational approximation to the true posterior , since the true posterior is usually intractable in models of interest . Training is generally performed by maximization of the evidence lower bound ( ELBO ) , a lower bound on the log-likelihood of the data : log pθ ( x ) ≥ Eqφ ( z|x ) [ log pθ ( x , z ) − log qφ ( z|x ) ] ( 7 ) To give a more expressive model , the latent space can be structured into a hierarchy of latent variables z1 : L. In the generative model each latent layer is conditioned on deeper latents pθ ( zi|zi+1 : L ) . A common problem with hierarchical VAEs is that the deeper latents can struggle to learn , often `` collapsing '' such that the layer posterior matches the prior : qφ ( zi|zi+1 : L , x ) ≈ pθ ( zi|zi+1 : L ) 3 . One method to help prevent posterior collapse is to use skip connections between latent layers ( Kingma et al. , 2016 ; Maaløe et al. , 2019 ) , turning the layers into residual layers ( He et al. , 2016 ) . We focus on the ResNet VAE ( RVAE ) model ( Kingma et al. , 2016 ) . In this model , both the generative and inference model structure their layers as residual layers . The ResNet VAE uses a bi-directional inference structure with both a bottom-up and top-down residual channel . This is a similar structure to the BIVA model ( Maaløe et al. , 2019 ) , which has demonstrated state-of-the-art results for a latent variable model . We give a more detailed description of the model in Appendix C. Flow models . Flow models consist of a parameterized invertible transformation , z = fθ ( x ) , and a known density pz ( z ) usually taken to be a unit normal distribution . Given observed data x we obtain the objective for θ by applying a change-of-variables to the log-likelihood : log pθ ( x ) = log pz ( fθ ( x ) ) + log ∣∣∣∣det dfθdx ∣∣∣∣ ( 8 ) For training to be possible , it is required that computation of the Jacobian determinant det ( dfθ/dx ) is tractable . We therefore aim to specify of flow model fθ which is sufficiently flexible to model the 2Apart from at 0 , where it is non-differentiable . 3We have assumed here that the inference model is factored `` top-down '' . data distribution well , whilst also being invertible and having a tractable Jacobian determinant . One common approach is to construct fθ as a composition of many simpler functions : fθ = f1 ◦ f2 ◦ ... ◦ fL , with each fi invertible and with tractable Jacobian . So the objective becomes : log pθ ( x ) = log pz ( fθ ( x ) ) + L∑ i=1 log ∣∣∣∣det dfidfi−1 ∣∣∣∣ ( 9 ) There are many approaches to construct the fi layers ( Dinh et al. , 2014 ; Rezende & Mohamed , 2015 ; Dinh et al. , 2017 ; Kingma & Dhariwal , 2018 ; Ho et al. , 2019a ) . In this work we will focus on the Flow++ model ( Ho et al. , 2019a ) , which has state-of-the-art results for flow models . In the Flow++ model , the fi are coupling layers which partition the input into x1 and x2 , then transform only x2 : fi ( x1 ) = x1 , fi ( x2 ) = σ −1 ( MixLogCDF ( x2 ; t ( x1 ) ) ) · exp ( a ( x1 ) ) + b ( x1 ) ( 10 ) Where MixLogCDF is the CDF for a mixture of logistic distributions . This is an iteration on the affine coupling layer ( Dinh et al. , 2014 ; 2017 ) . Note that keeping part of the input fixed ensures that the layer is invertible . To ensure that all dimensions are transformed in the composition , adjacent coupling layers will keep different parts of the input fixed , often using an alternating checkerboard or stripe pattern to choose the fixed dimensions ( Dinh et al. , 2017 ) . The majority of parameters in this flow model come from the functions t , a and b in the coupling layer , and in Flow++ these are parameterized as stacks of convolutional residual layers . In this work we will focus on how to binarize these functions whilst maintaining good modelling performance . We give a more detailed description of the full flow model in Appendix D .","Deep generative models provide a powerful set of tools to understand real-world data. But as these models improve, they increase in size and complexity, so their computational cost in memory and execution time grows. Using binary weights in neural networks is one method which has shown promise in reducing this cost. However, whether binary neural networks can be used in generative models is an open problem. In this work we show, for the first time, that we can successfully train generative models which utilize binary neural networks. This reduces the computational cost of the models massively. We develop a new class of binary weight normalization, and provide insights for architecture designs of these binarized generative models. We demonstrate that two state-of-the-art deep generative models, the ResNet VAE and Flow++ models, can be binarized effectively using these techniques. We train binary models that achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time.","This paper describes a method to binarize weights and activations of variational autoencoders and flow-based networks.  This is an important issue as these methods are valuable to solving unsupervised problems, but are rapidly growing in size, necessitating large and expensive computing systems. And, the literature of low and binary precision hasn’t considered these use-cases to date.","In this article, the authors explore the possibility of reducing the memory and computational requirements of deep neural networks by using binary neural networks, which have binary-valued weights and activations. They propose an efficient binary adaptation of weight normalization, a reparameterization technique often used in deep generative models, and show that binarizing the weights and activations of hierarchical VAEs and flow models can lead to significant speedups without significantly hurting performance.","The article describes a way of reducing the memory and computational requirements of machine learning models, by using binary neural networks with restricted weights and activations. Whilst binary neural networks have been studied for classification problems, in this article the authors examine the use of binary neural networks in deep generative models for unsupervised learning problems, by binarizing the weights and activations of the ResNet Variational Autoencoder (VAE) and the Flow++ model. The authors propose an efficient binary adaptation of weight normalization (a reparameterization technique to accelerate convergence), and demonstrate that it is possible to reduce the model size drastically and allow for significant speed-ups in run time, whilst only moderately impacting the achieved loss value.","0,2,1",,,,,,,,,,,,,,,,,
35,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"1 Introduction . Consider the problem of training deep neural networks on large annotated datasets , such as ImageNet [ 1 ] . This problem can be formalized as finding optimal parameters for a given neural network a , parameterized by w , w.r.t . a loss function L on the dataset D = { ( xi , yi ) } Ni=1 of inputs xi and targets yi : arg min w ∑N i=1 L ( f ( xi ; a , w ) , yi ) , ( 1 ) where f ( xi ; a , w ) represents a forward pass . Equation 1 is usually minimized by iterative optimization algorithms – e.g . SGD [ 2 ] and Adam [ 3 ] – that converge to performant parameters wp of the architecture a . Despite the progress in improving the training speed and convergence [ 4–7 ] , obtaining wp remains a bottleneck in large-scale machine learning pipelines . For example , training a ResNet-50 [ 8 ] on ImageNet can take many GPU hours [ 9 ] . With the ever growing size of networks [ 10 ] and necessity of training the networks repeatedly ( e.g . for hyperparameter or architecture search ) , the classical process of obtaining wp is becoming computationally unsustainable [ 11–13 ] . A new parameter prediction task . When optimizing the parameters for a new architecture a , typical optimizers disregard past experience gained by optimizing other nets . However , leveraging this past experience can be the key to reduce the reliance on iterative optimization and , hence the high computational demands . To progress in that direction , we propose a new task where iterative optimization is replaced with a single forward pass of a hypernetwork [ 14 ] HD . To tackle the task , 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . ∗Part of the work was done while interning at Facebook AI Research . HD is expected to leverage the knowledge of how to optimize other networks F . Formally , the task is to predict the parameters of an unseen architecture a /∈ F using HD , parameterized by θp : ŵp = HD ( a ; θp ) . The task is constrained to a dataset D , so ŵp are the predicted parameters for which the test set performance of f ( x ; a , ŵp ) is similar to the one of f ( x ; a , wp ) . For example , we consider CIFAR-10 [ 15 ] and ImageNet image classification datasets D , where the test set performance is classification accuracy on test images . Approaching our task . A straightforward approach to expose HD to the knowledge of how to optimize other networks is to train it on a large training set of { ( ai , wp , i ) } pairs , however , that is prohibitive2 . Instead , we follow the bi-level optimization paradigm common in meta-learning [ 16–18 ] , but rather than iterating over M tasks , we iterate over M training architectures F = { ai } Mi=1 : arg min θ ∑N j=1 ∑M i=1 L ( f ( xj ; ai , HD ( ai ; θ ) ) , yj ) . ( 2 ) By optimizing Equation 2 , the hypernetwork HD gradually gains knowledge of how to predict performant parameters for training architectures . It can then leverage this knowledge at test time – when predicting parameters for unseen architectures . To approach the problem in Equation 2 , we need to design the network space F and HD . For F , we rely on the previous design spaces for neural architectures [ 19 ] that we extend in two ways : the ability to sample distinct architectures and an expanded design space that includes diverse architectures , such as ResNets and Visual Transformers [ 20 ] . Such architectures can be fully described in the form of computational graphs ( Fig . 1 ) . So , to design the hypernetwork HD , we rely on recent advances in machine learning on graph-structured data [ 21–24 ] . In particular , we build on the Graph HyperNetworks method ( GHNs ) [ 24 ] that also optimizes Equation 2 . However , GHNs do not aim to predict large-scale performant parameters as we do in this work , which motivates us to improve on their approach . By designing our diverse space F and improving on GHNs , we boost the accuracy achieved by the predicted parameters on unseen architectures to 77 % ( top-1 ) and 48 % ( top-5 ) on CIFAR-10 [ 15 ] and ImageNet [ 1 ] , respectively . Surprisingly , our GHN shows good out-of-distribution generalization and predicts good parameters for architectures that are much larger and deeper compared to the ones seen in training . For example , we can predict all 24 million parameters of ResNet-50 in less than a second either on a GPU or CPU achieving ∼60 % on CIFAR-10 without any gradient updates ( Fig 1 , ( b ) ) . Overall , our framework and results pave the road toward a new and significantly more efficient paradigm for training networks . Our contributions are as follows : ( a ) we introduce the novel task of predicting performant parameters for diverse feedforward neural networks with a single hypernetwork forward pass ; ( b ) we introduce DEEPNETS-1M – a standardized benchmark with in-distribution and out-of-distribution architectures to track progress on the task ( § 3 ) ; ( c ) we define several baselines and propose a GHN model ( § 4 ) that performs surprisingly well on CIFAR-10 and ImageNet ( § 5.1 ) ; ( d ) we show that our model learns a strong representation of neural network architectures ( § 5.2 ) , and our model is useful for initializing neural networks ( § 5.3 ) . Our DEEPNETS-1M dataset , trained GHNs and code is available at https : //github.com/facebookresearch/ppuda . D eco d er 2Training a single network ai can take several GPU days and thousands of trained networks may be required . 2 2 Background . We start by providing a brief background about the network design spaces leveraged in the creation of our DEEPNETS-1M dataset of neural architectures described in § 3 . We then cover elements of graph hypernetworks that we leverage when designing our specific GHN HD in § 4 . 2.1 Network Design Space of DARTS . DARTS [ 19 ] is a differentiable NAS framework . For image classification tasks such as those considered in this work , its networks are defined by four types of building blocks : stems , normal cells , reduction cells , and classification heads . Stems are fixed blocks of convolutional operations that process input images . The normal and reduction cells are the main blocks of architectures and are composed of : 3×3 and 5×5 separable convolutions , 3×3 and 5×5 dilated separable convolutions , 3×3 max pooling , 3×3 average pooling , identity and zero ( to indicate the absence of connectivity between two operations ) . Finally , the classification head defines the network output and is built with a global pooling followed by a single fully connected layer . Typically , DARTS networks have one stem block , 14-20 cells , and one classification head , altogether forming a deep computational graph . The reduction cells , placed only at 1/3 and 2/3 of the total depth , decrease the spatial resolution and increase the channel dimensionality by a factor of 2 . Summation and concatenation are used to aggregate outputs from multiple operations within each cell . To make the channel dimensionalities match , 1×1 convolutions are used as needed . All convolutional operations use the ReLU-Conv-Batch Norm ( BN ) [ 7 ] order . Overall , DARTS enables defining strong architectures that combine many principles of manual [ 25 , 8 , 26 , 27 ] and automatic [ 24 , 28–33 ] design of neural architectures . While DARTS learns the optimal task-specific cells , the framework can be modified to permit sampling randomly-structured cells . We leverage this possibility for the DEEPNETS-1M construction in § 3 . Please see § A.1 for further details on DARTS . 2.2 Graph HyperNetwork : GHN-1 . Representation of architectures . GHNs [ 24 ] directly operate on the computational graph of a neural architecture a . Specifically , a is a directed acyclic graph ( DAG ) , where nodes V = { vi } |V |i=1 are operations ( e.g . convolutions , fully-connected layers , summations , etc . ) and their connectivity is described by a binary adjacency matrix A ∈ { 0 , 1 } |V |×|V | . Nodes are further characterized by a matrix of initial node features H0 = [ h01 , h 0 2 , ... , h 0 |V | ] , where each h 0 v is a one-hot vector representing the operation performed by the node . We also use such a one-hot representation for H0 , but in addition encode the shape of parameters associated with nodes as described in detail in § B.1 . Design of the graph hypernetwork . In [ 24 ] , the graph hypernetwork HD consists of three key modules . The first module takes the input node features H0 and transforms them into d-dimensional node features H1 ∈ R|V |×d through an embedding layer . The second module takes H1 together with A and feeds them into a specific variant of the gated graph neural network ( GatedGNN ) [ 34 ] . In particular , their GatedGNN mimics the canonical order π of node execution in the forward ( fw ) and backward ( bw ) passes through a computational graph . To do so , it sequentially traverses the graph and performs iterative message passing operations and node feature updates as follows : ∀t ∈ [ 1 , ... , T ] : [ ∀π ∈ [ fw , bw ] : ( ∀v ∈ π : mtv = ∑ u∈Nπv MLP ( htu ) , h t v = GRU ( h t v , m t v ) ) ] , ( 3 ) where T denotes the total number of forward-backward passes ; htv corresponds to the features of node v in the t-th graph traversal ; MLP ( · ) is a multi-layer perceptron ; and GRU ( · ) is the update function of the Gated Recurrent Unit [ 35 ] . In the forward propagation ( π = fw ) , N πv corresponds to the incoming neighbors of the node defined by A , then in the backward propagation ( π = bw ) it similarly corresponds to the outgoing neighbors of the node . The last module uses the GatedGNN output hidden states hTv to condition a decoder that produces the parameters ŵ v p ( e.g . convolutional weights ) associated with each node . In practice , to handle different parameter dimensionalities per operation type , the output of the hypernetwork is reshaped and sliced according to the shape of parameters in each node . We refer to the model described above as GHN-1 ( Fig . 1 ) . Further subtleties of implementing this model in the context of our task are discussed in § B.1 . 3 DEEPNETS-1M . The network design space of DARTS is limited by the number of unique operations that compose cells , and the low variety of stems and classification heads . Thus , many architectures are not realizable within this design space , including : VGG [ 25 ] , ResNets [ 8 ] , MobileNet [ 33 ] or more recent ones such as Visual Transformer ( ViT ) [ 20 ] and Normalization-free networks [ 36 , 37 ] . Furthermore , DARTS does not define a procedure to sample random architectures . By addressing these two limitations we aim to expose our hypernetwork to diverse training architectures and permit its evaluation on common architectures , such as ResNet-50 . We hypothesize that increased training diversity can improve hypernetworks ’ generalization to unseen architectures making it more competitive to iterative optimizers . Extending the network design space . We extend the set of possible operations with non-separable 2D convolutions3 , Squeeze & Excite4 ( SE ) [ 40 ] and Transformer-based operations [ 41 , 20 ] : multihead self-attention ( MSA ) , positional encoding and layer norm ( LN ) [ 42 ] . Each node ( operation ) in our graphs has two attributes : primitive type ( e.g . convolution ) and shape ( e.g . 3×3×512×512 ) . Overall , our extended set consists of 15 primitive types ( Table 1 ) . We also extend the diversity of the generated architectures by introducing VGG-style classification heads and ViT stems . Finally , to further increase architectural diversity , we allow the operations to not include batch norm ( BN ) [ 7 ] and permit networks without channel width expansion ( e.g . as in [ 20 ] ) . Architecture generation process . We generate different subsets of architectures ( see the description of each subset in the next two paragraphs and in Table 1 ) . For each subset depending on its purpose , we predefine a range of possible model depths ( number of cells ) , widths and number of nodes per cell . Then , we sample a stem , a normal and reduction cell and a classification head . The internal structure of the normal and reduction cells is defined by uniformly sampling from all available operations . Due to a diverse design space it is extremely unlikely to sample the same architecture multiple times , but we ran a sanity check using the Hungarian algorithm [ 43 ] to confirm that ( see Figure 6 in § A.2 for details ) . In-distribution ( ID ) architectures . We generate a training set of |F| = 106 architectures and validation/test sets of 500/500 architectures that follow the same generation rules and are considered to be ID samples . However , training on large architectures can be prohibitive , e.g . in terms of GPU memory . Thus , in the training set we allow the number of channels and , hence the total number of parameters , to be stochastically defined given computational resources . For example , to train 3Non-separable convolutions have weights of e.g . shape 3×3×512×512 as in ResNet-50 . NAS works , such as DARTS and GHN , avoid such convolutions , since the separable ones [ 38 ] are more efficient . Non-separable convolutions are nevertheless common in practice and can often boost the downstream performance . 4The Squeeze & Excite operation is common in many efficient networks [ 39 , 12 ] . our models we upper bound the number of parameters in the training architectures to around 3M by sampling fewer channels if necessary . In the evaluation sets , the number of channels is fixed . Therefore , this pre-processing step prior to training results in some distribution shift between the training and the validation/test sets . However , the shift is not imposed by our dataset . Out-of-distribution ( OOD ) architectures . We generate five OOD test sets that follow different generation rules . In particular , we define WIDE and DEEP sets that are of interest due the stronger downstream performance of such nets in large-scale tasks [ 47 , 48 , 10 ] . These nets are often more challenging to train for fundamental [ 49 , 50 ] or computational [ 51 ] reasons , so predicting their parameters might ease their subsequent optimization . We also define the DENSE set , since networks with many operations per cell and complex connectivity are underexplored in the literature despite their potential [ 27 ] . Next , we define the BN-FREE set that is of interest due to BN ’ s potential negative side-effects [ 52 , 53 ] and the difficulty or unnecessity of using it in some cases [ 54–56 , 36 , 37 ] . We finally add the RESNET/VIT set with two predefined image classification architectures : commonlyused ResNet-50 [ 8 ] and a smaller 12-layer version of the Visual Transformer ( ViT ) [ 20 ] that has recently received a lot of attention in the vision community . Please see § A.1 and § A.2 for further details and statistics of our DEEPNETS-1M dataset .","Deep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefficient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowledge of training other networks. We introduce a large-scale dataset of diverse computational graphs of neural architectures – DEEPNETS-1M– and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis.","In this paper, the authors propose the task of parameter prediction with a new DeepNets-1M dataset. By using a improved Graph HyperNetwork, they can achieve reasonable performance for unseen and diverse networks without iterative optimization. The learned neural architecture representation also performs better than previous works.","The article presents a new task in machine learning called ""predicting performant parameters for diverse feedforward neural networks with a single hypernetwork forward pass."" The task involves optimizing the parameters of a neural network architecture using a single forward pass of a hypernetwork, which is trained to predict the best parameters for a given architecture.","Researchers from Facebook AI Research have developed a method to train deep neural networks that is significantly more efficient than current approaches. Called DEEPNETS-1M, the system involves training a hypernetwork to predict the parameters of an unseen architecture of feedforward neural network using a single forward pass, without having to iterate over multiple architectures.","0,2,1",,,,,,,,,,,,,,,,,
36,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"1 INTRODUCTION . The availability of multiple data types provides a rich source of information and holds promise for learning representations that generalize well across multiple modalities ( Baltrušaitis et al. , 2018 ) . Multimodal data naturally grants additional self-supervision in the form of shared information connecting the different data types . Further , the understanding of different modalities and the interplay between data types are non-trivial research questions and long-standing goals in machine learning research . While fully-supervised approaches have been applied successfully ( Karpathy & Fei-Fei , 2015 ; Tsai et al. , 2019 ; Pham et al. , 2019 ; Schoenauer-Sebag et al. , 2019 ) , the labeling of multiple data types remains time consuming and expensive . Therefore , it requires models that efficiently learn from multiple data types in a self-supervised fashion . Self-supervised , generative models are suitable for learning the joint distribution of multiple data types without supervision . We focus on VAEs ( Kingma & Welling , 2014 ; Rezende et al. , 2014 ) which are able to jointly infer representations and generate new observations . Despite their success on unimodal datasets , there are additional challenges associated with multimodal data ( Suzuki et al. , 2016 ; Vedantam et al. , 2018 ) . In particular , multimodal generative models need to represent both modality-specific and shared factors and generate semantically coherent samples across modalities . Semantically coherent samples are connected by the information which is shared between data types ( Shi et al. , 2019 ) . These requirements are not inherent to the objective—the evidence lower bound ( ELBO ) —of unimodal VAEs . Hence , adaptions to the original formulation are required to cater to and benefit from multiple data types . Furthermore , to handle missing modalities , there is a scalability issue in terms of the number of modalities : naively , it requires 2M different encoders to handle all combinations for M data types . Thus , we restrict our search for an improved multimodal ELBO to the class of scalable multimodal VAEs . Among the class of scalable multimodal VAEs , there are two dominant strains of models , based on either the multimodal variational autoencoder ( MVAE , Wu & Goodman , 2018 ) or the Mixture-ofExperts multimodal variational autoencoder ( MMVAE , Shi et al. , 2019 ) . However , we show that these approaches differ merely in their choice of joint posterior approximation functions . We draw a theoretical connection between these models , showing that they can be subsumed under the class ∗Equal contribution . of abstract mean functions for modeling the joint posterior . This insight has practical implications , because the choice of mean function directly influences the properties of a model ( Nielsen , 2019 ) . The MVAE uses a geometric mean , which enables learning a sharp posterior , resulting in a good approximation of the joint distribution . On the other hand , the MMVAE applies an arithmetic mean which allows better learning of the unimodal and pairwise conditional distributions . We generalize these approaches and introduce the Mixture-of-Products-of-Experts-VAE that combines the benefits of both methods without considerable trade-offs . In summary , we derive a generalized multimodal ELBO formulation that connects and generalizes two previous approaches . The proposed method , termed MoPoE-VAE , models the joint posterior approximation as a Mixture-of-Products-of-Experts , which encompasses the MVAE ( Product-ofExperts ) and MMVAE ( Mixture-of-Experts ) as special cases ( Section 3 ) . In contrast to previous models , the proposed model approximates the joint posterior for all subsets of modalities , an advantage that we validate empirically in Section 4 , where our model achieves state-of-the-art results . 2 RELATED WORK . This work extends and generalizes existing work in self-supervised multimodal generative models that are scalable in the number of modalities . Scalable in the sense that a single model approximates the joint distribution over all modalities ( including all marginal and conditional distributions ) instead of requiring individual models for every subset of modalities ( e.g. , Huang et al. , 2018 ; Tian & Engel , 2019 ; Hsu & Glass , 2018 ) . The latter approach requires a prohibitive number of models , exponential in number of modalities . Multimodal VAEs Among multimodal generative models , multimodal VAEs ( Suzuki et al. , 2016 ; Vedantam et al. , 2018 ; Kurle et al. , 2019 ; Tsai et al. , 2019 ; Wu & Goodman , 2018 ; Shi et al. , 2019 ; 2020 ; Sutter et al. , 2020 ) have recently been the dominant approach . Multimodal VAEs are not only suitable to learn a joint distribution over multiple modalities , but also enable joint inference given a subset of modalities . However , to approximate the joint posterior for all subsets of modalities efficiently , it is required to introduce additional assumptions on the form of the joint posterior . To overcome the issue of scalability , previous work relies on either the product ( Kurle et al. , 2019 ; Wu & Goodman , 2018 ) or the mixture ( Shi et al. , 2019 ; 2020 ) of unimodal posteriors . While both approaches have their merits , there are also disadvantages associated with them . We unite these approaches in a generalized formulation—a mixture of products joint posterior—that encapsulates both approaches and combines their benefits without significant trade-offs . Multimodal posteriors The MVAE ( Wu & Goodman , 2018 ) assumes that the joint posterior is a product of unimodal posteriors—a Product-of-Experts ( PoE , Hinton , 2002 ) . The PoE has the benefit of aggregating information across any subset of unimodal posteriors and therefore provides an efficient way of dealing with missing modalities for specific types of unimodal posteriors ( e.g. , Gaussians ) . However , to handle missing modalities the MVAE relies on an additional sub-sampling of unimodal log-likelihoods , which no longer guarantees a valid lower bound on the joint log-likelihood ( Wu & Goodman , 2019 ) . Previous work provides empirical results that exhibit the shortcomings of the MVAE , attributing them to a precision miscalibration of experts ( Shi et al. , 2019 ) or to the averaging over inseparable individual beliefs ( Kurle et al. , 2019 ) . Our results suggest that the PoE works well in practice , if it is also applied on all subsets of modalities , which naturally leads to the proposed Mixture-of-Products-of-Experts ( MoPoE ) generalization , which yields a valid lower bound on the joint log-likelihood . On the other hand , the MMVAE ( Shi et al. , 2019 ) assumes that the joint posterior is a mixture of unimodal posteriors—a Mixture-of-Experts ( MoE ) . The MMVAE is suitable for the approximation of unimodal posteriors and for translation between pairs of modalities , however , it can not take advantage of multiple modalities being present , because it only takes the unimodal posteriors into account during training . In contrast , the proposed MoPoE-VAE computes the joint posterior for all subsets of modalities and therefore enables efficient many-to-many translations . Extensions of the MVAE and MMVAE ( Kurle et al. , 2019 ; Daunhawer et al. , 2020 ; Shi et al. , 2020 ; Sutter et al. , 2020 ) have introduced additional loss terms , however , these are also applicable to and can be added on top of the proposed model . Table 1 summarizes the properties of previous multimodal VAEs and highlights the benefits of the proposed model : the ability to aggregate multiple modalities , to learn a multi-modal posterior ( in the statistical sense ) , and to efficiently handle missing modalities at test time . 3 METHOD . 3.1 PRELIMINARIES . We consider a dataset { X ( i ) } Ni=1 of N i.i.d . samples , each of which is a set of M modalities X ( i ) = { x ( i ) j } Mj=1 . We assume that the data is generated by some random process involving a joint hidden random variable z such that inter-modality dependencies are unknown . The marginal log-likelihood can be decomposed into a sum over marginal log-likelihoods of individual sets log pθ ( { X ( i ) } Ni=1 ) = ∑N i=1 log pθ ( X ( i ) ) , which can be written as : log pθ ( X ( i ) ) = DKL ( qφ ( z|X ( i ) ) ||pθ ( z|X ( i ) ) ) + L ( θ , φ ; X ( i ) ) , ( 1 ) with L ( θ , φ ; X ( i ) ) : = Eqφ ( z|X ( i ) ) [ log pθ ( X ( i ) |z ) ] −DKL ( qφ ( z|X ( i ) ) ||pθ ( z ) ) . ( 2 ) L ( θ , φ ; X ( i ) ) is called evidence lower bound ( ELBO ) on the marginal log-likelihood of the i-th set . It forms a tractable objective to approximate the joint data distribution log pθ ( X ( i ) ) . qφ ( z|X ( i ) ) is the posterior approximation distribution with learnable parameters φ . From the non-negativity of the KL divergence , it follows that log pθ ( X ( i ) ) ≥ L ( θ , φ ; X ( i ) ) . If the posterior approximation qφ ( z|X ( i ) ) is identical to the true posterior distribution pθ ( z|X ( i ) ) , the bound holds with equality . Hence , maximizing the ELBO in Equation ( 2 ) minimizes the otherwise intractable KL-divergence between approximate and true posterior distribution : argmin φ DKL ( qφ ( z|X ( i ) ) ||pθ ( z|X ( i ) ) ) . ( 3 ) Adaptations to the ELBO formulation in Equation ( 2 ) include an additional hyperparameter β which weights the KL-divergence relative to the log-likelihood ( Higgins et al. , 2017 ) . To improve readability , we will omit the superscript ( i ) in the remaining part of this work . 3.2 APPROXIMATING pθ ( z|X ) IN CASE OF MISSING DATA TYPES For a dataset of M modalities , there are 2M different subsets contained in the powerset P ( X ) . If , for a particular observation , we only have access to a subset of data types Xk ∈ P ( X ) , the approximation of pθ ( Xk ) would result in a different ELBO formulation L ( θ , φk ; Xk ) where the true posterior pθ ( z|Xk ) of subset Xk is approximated . Instead , we are interested in the true posterior pθ ( z|X ) of all data types X , even when only a subset Xk , i.e . q̃φk ( z|Xk ) , is available . The desired ELBO for the available subset Xk is given by Lk ( θ , φk ; X ) : = Eq̃φk ( z|Xk ) [ log ( pθ ( X|z ) ] −DKL ( q̃φk ( z|Xk ) ||pθ ( z ) ) . ( 4 ) The subtle but important difference between Lk ( θ , φk ; X ) and L ( θ , φk ; Xk ) is that the former still yields a valid lower bound on pθ ( X ) , whereas the latter forms a lower bound on log pθ ( Xk ) , which is no longer a valid bound on the desired log pθ ( X ) . Different from previous work , we argue for an optimization of the powerset P ( X ) , i.e. , the joint optimization of all ELBOs Lk ( θ , φk ; X ) defined by the posterior subset approximation q̃φk ( z|Xk ) . Since maximizing the ELBO in Equation ( 2 ) is equivalent to minimizing the KL-divergence in Equation ( 3 ) , the joint optimization of the powerset P ( X ) is equal to the minimization of the following convex combination of KL-divergences of the power set P ( X ) .1 argmin φ ∑ Xk∈P ( X ) DKL ( q̃φ ( z|Xk ) ||pθ ( z|X ) ) ( 5 ) Hence , we propose to optimize Equation ( 4 ) for all subsets Xk . Lemma 1 . The sum of KL-divergences in Equation ( 5 ) describes the joint probability log pθ ( X ) as follows : log pθ ( X ) = 1 2M ∑ Xk∈P ( X ) DKL ( q̃φ ( z|Xk ) ||pθ ( z|X ) ) + 1 2M ∑ Xk∈P ( X ) Eq̃φ ( z|Xk ) [ log pθ ( X|z ) pθ ( z ) q̃φ ( z|Xk ) ] Following Lemma 1 ( see Appendix A.1 for the proof ) and the non-negativity of the KL-divergence , we see that the convex combination of expectations over the powerset P ( X ) is an ELBO on the joint probability log pθ ( X ) . Since this would require 2M different inference networks in a naive implementation , we use a more efficient approach utilizing abstract mean functions .","Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative models approximating an ELBO are not able to fulfill all desired requirements of multimodal models: their posterior approximation functions lead to a trade-off between the semantic coherence and the ability to learn the joint data distribution. We propose a new, generalized ELBO formulation for multimodal data that overcomes these limitations. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. In extensive experiments, we demonstrate the advantage of the proposed method compared to state-of-the-art models in selfsupervised, generative learning tasks.",This paper formulates a multimodal ELBO as a mixture of product of experts. This allows them to use one encoder per mode while still allowing inference over any subset of modes without needing a new encoder for each subset. The idea is simple and appears to improve on baselines derived from a mixture or a product of experts.,"This article proposes a new approach to self-supervised multimodal generative models, which can handle multiple data types jointly and efficiently. The proposed model, called MoPoE-VAE, models the joint posterior approximation as a mixture of products of experts, which combines the benefits of previous approaches without significant trade-offs. The article derives a generalized multimodal ELBO formulation that connects and generalizes two previous approaches, and demonstrates the effectiveness of the proposed model through experimental results.","The article introduces a new approach for multimodal self-supervised generative models, which can efficiently learn from multiple data types in absence of labels. The proposed approach is called Mixture-of-Products-of-Experts-VAE (MoPoE-VAE) which models the joint posterior approximation as a Mixture-of-Products-of-Experts that encompasses the Multimodal Variational Autoencoder (MVAE) and Mixture-of-Experts Multimodal Variational Autoencoder (MMVAE) as special cases and can handle missing modalities efficiently. The MoPoE-VAE can approximate the joint posterior for all subsets of modalities efficiently and achieved the state-of-the-art results on benchmark datasets.","0,2,1",,,,,,,,,,,,,,,,,
37,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"1 INTRODUCTION . With over 5 million applications in Google Play Store and Apple App Store and over a billion webpages , a significant amount of time can be saved by automating even small parts of their development . To achieve this , several tools have been recently developed that help user interface designers explore and quickly prototype different ideas , including Sketch2Code ( Microsoft , 2018 ) and InkToCode ( Corrado et al. , 2018 ) , which generate user interface sketches from hand-drawn images , Swire ( Huang et al. , 2019 ) and Rico ( Deka et al. , 2017 ) , which allow retrieving designs similar to the one supplied by the user and Rewire ( Swearngin et al. , 2018 ) , which transforms images into vector representations consisting of rectangles , circles and lines . At the same time , to help developers implement the design , a number of approaches have been proposed that generate layout code that places the user interface components at the desired position ( e.g. , when resizing the application ) . These include both symbolic synthesis approaches such as InferUI ( Bielik et al. , 2018 ) , which encodes the problem as a satisfiability query of a first-order logic formula , as well as statistical approaches ( Beltramelli , 2018 ; Chen et al. , 2018 ) , which use encoder-decoder neural networks to process the input image and output the corresponding implementation . In this work , we explore a new domain of inferring an implementation of an user interface component from an image which when rendered , looks visually the same as the input image . Going from an image to a concrete implementation is a time consuming , yet necessary task , which is often outsourced to a company for a high fee ( replia , 2019 ; psd2android , 2019 ; psd2mobi , 2019 ) . Compared to prior work , we focus on the pixel-accurate implementation , rather than on producing sketches or the complementary task of synthesizing layouts that place the components at the desired positions . Concretely , given a black box rendering engine that defines a set of categorical and numerical attributes of a component , we design a two step process which predicts the attribute values from an input image – ( i ) first , we train a neural model to predict the most likely initial attribute values , and then ( ii ) we use imitation learning to iteratively refine the attribute values to achieve pixel-level accuracy . Crucially , all our models are trained using synthetic datasets that are obtained by sampling the black box rendering engine , which makes it easy to train models for other attributes in the future . We instantiate our approach to the task of inferring the implementation of Android Button attributes and show that it generalizes well to a real-world dataset consisting of buttons found in existing Google Play Store applications . In particular , our approach successfully infers the correct attribute values in 94.8 % and 92.5 % of the cases for the synthetic and the real-world datasets , respectively . 2 RELATED WORK . As an application , our work is related to a number of recently developed tools in the domain of user interface design and implementation with the goal of making developers more productive , as discussed in Section 1 . Here we give overview of the related research from a technical perspective . Inverting rendering engines to interpret images The most closely related work to ours phrases the task of inferring attributes from images as the more general task of learning to invert the rendering engines used to produce the images . For example , Wu et al . ( 2017 ) use reinforcement learning to train a neural pipeline that given an image of a cartoon scene or a Minecraft screenshot , identifies objects and a small number of high level features ( e.g. , whether the object is oriented left or right ) . Ganin et al . ( 2018 ) also use reinforcement learning , but with an adversarially learned reward signal , to generate a program executed by a graphics engine that draws simple CAD programs or handwritten symbols and digits . Johnson et al . ( 2018 ) and Ellis et al . ( 2018 ) design a neural architecture that generates a program that when rendered , produces that same 2D or 3D shape as in the input image . While Johnson et al . ( 2018 ) train the network using a combination of supervised pretraining and reinforcement learning with a custom reward function ( using Chamfer distance to measure similarity of two objects ) , Ellis et al . ( 2018 ) use a two step process that first uses supervised learning to predict a set of objects in the image and then synthesizes a program ( e.g. , containing loops ) that draws them . In comparison to these prior works , our approach differs in three key aspects . First , the main challenge in prior works is predicting the set of objects contained in the image and how to compose them . Instead , the focus of our work is in predicting a set of object properties after the objects in the image were already identified . Second , instead of using the expensive REINFORCE ( Williams , 1992 ) algorithm ( or its variation ) to train our models , we use a two step process that first pretrains the network to make an initial prediction and then uses imitation learning to refine it . This is possible because , in our setting , there is a fixed set of attributes known in advance for which we can generate a suitable synthetic dataset used by both of these steps . Finally , because our goal is to learn pixel-accurate attribute values , the refinement loop takes as input both the original image , as well as the rendered image of the current attribute predictions . As a result , we do not require our models to predict pixel-accurate rendering of an attribute value but instead , to only predict whether the attribute values in two images are the same or in which direction they should be adjusted . Attribute prediction Optical character recognition ( Jaderberg et al. , 2016 ; Lyu et al. , 2018 ; Jaderberg et al. , 2014 ; Gupta et al. , 2016 ) is a well studied example of predicting an attribute from an image with a large number of real-world applications . Other examples include predicting text fonts ( Zhao et al. , 2018 ; Wang et al. , 2015 ; Chen et al. , 2014 ) , predicting eye gaze ( Shrivastava et al. , 2017 ) , face pose and lighting ( Kulkarni et al. , 2015 ) , chair pose and content ( Wu et al. , 2018 ) or 3D object shapes and pose ( Kundu et al. , 2018 ) , to name just a few . The attribute prediction network used in our work to predict the initial attribute value is similar to these existing approaches , except that it is applied to a new domain of inferring user interface attributes . As a result , while some of the challenges remain the same ( e.g. , how to effectively generate synthetic datasets ) , our main challenge is designing a pipeline , together with a network architecture capable of achieving pixel-level accuracies on a range of diverse attributes . 3 BACKGROUND : USER INTERFACE ATTRIBUTES . Visual design of user interface components can be specified in many different ways – by defining a program that draws on a canvas , by defining a program that instantiates components at runtime and manipulates their properties , declaratively by defining attribute values in a configuration file ( e.g. , using CSS ) , or by using a bitmap image that is rendered in place of the components . In our work , we follow the best practices and consider the setting where the visual design is defined declaratively , thus allowing separating the design from the logic that controls the application behaviour . Formally , let C denote a component with a set of attributes A . The domain of possible values of each attribute ai ∈A is denoted as Θi . As all the attributes are rendered on a physical device , their domains are finite sets containing measurements in pixels or a set of categorical values . For example , the domain for the text color attribute is three RGB channels N3× [ 0,255 ] , the domain for text gravity is { top , left , center , right , bottom } and the domain for border width is N [ 0,20 ] . We distinguish two domain types : ( i ) comparable ( e.g. , colors , shadows or sizes ) for which a valid distance metric d : Θ×Θ→N [ 0 , ∞ ) exists , and ( ii ) uncomparable ( e.g. , font types or text gravity ) for which the distance between any two attribute values is equal to one . We use Θ ⊆ Θ1×· · ·×Θn to denote the space of all possible attribute configurations , and use the function render : Θ→R3×h×w to denote an image with width w , height h and three color channels obtained by rendering the attribute configuration y∈Θ . Furthermore , we use the notation y∼Θ to denote a random sample of attribute values from the space of all valid attribute configurations . Finally , we note that attributes often affect the same parts of the rendered image ( e.g. , the shadow is overlayed on top of the background ) and they are in general not independent of each other ( e.g , changing the border width affects the border radius ) . 4 LEARNING TO INFER USER INTERFACE ATTRIBUTES FROM IMAGES . We now present our approach for learning to infer user interface component attributes from images . Problem statement Given an input image I ∈ R3×h×w , our goal is to find an attribute configuration y ∈ Θ which when rendered , produces an image most visually similar to I : arg miny∈Θ cost ( I , render ( y ) ) where cost : I × I → R [ 0 , ∞ ) is a function that computes the visual similarity of a given user interface component in two images . It returns zero if the component looks visually the same in both images or a positive real value denoting the degree to which the attributes are dissimilar . The first challenge that arises from the problem statement above is how to define the cost function . Pixel based metrics , such as mean squared error of pixel differences , are not suitable and instead of producing images with similar attribute values , produce images that have on average similar colors . Training a discriminator also does not work , as all the generated images are produced by rendering a set of attributes and are true images by definition . Finally , the cost can be computed not over the rendered image but by comparing the predicted attributes y with the ground-truth labels . Unfortunately , even if we would spend the effort and annotated a large number of images with their ground-truth attributes , using a manually annotated dataset restricts the space of models that can be used to infer y to only those that do supervised learning . In what follows we address this challenge by showing how to define the cost function over attributes ( used for supervise learning ) as well as over images ( used for reinforcement learning ) , both by using a synthetically generated dataset . Our approach To address the task of inferring user interface attributes from images , we propose a two step process that – ( i ) first selects the most likely initial attribute values arg maxy∈Θ p ( y | I ) by learning a probability distribution of attribute values conditioned on the input image , and then ( ii ) iteratively refines the attribute values by learning a policy π ( ∆y ( i ) |I , render ( y ( i ) ) ) that represents the probability distribution of how each attribute should be changed , conditioned on both the original image , as well as the rendered image of the attribute configuration y ( i ) at iteration i . We use the policy π to define the cost between two images as cost ( I , I ′ ) : = 1−π ( ∆y=0 | I , I ′ ) . That is , the cost is defined as the probability that the two images are not equal in the attribute space . We illustrate both steps in Figure 1 with an example that predicts attributes of a Button component . In Figure 1 ( a ) , the input image is passed to a set of convolutional neural networks , each of which is trained to predict a single attribute value . In our example , the most likely value predicted for the border width is 2dp while the most likely color of the border is # 4a4a4a . Then , instead of returning the most likely attribute configuration y , we take advantage of the fact that it can be rendered and compared to the original input image . This give us additional information that is used to refine the predictions as shown in Figure 1 ( b ) . Here , we use a pair of siamese networks ( pretrained on the prediction task ) to learn the probability distribution over changes required to make the component attributes in both images the same . In our example , the network predicts that the border color and the text gravity attributes have already the correct values but the border width should be decreased by 2dp and the shadow should be increased by 4dp . Then , due to the large number of different attributes that affect each other , instead of applying all the changes at once , we select and apply a single attribute change . In our example , the ∆y corresponds to adjusting the value of border width by −2dp . Since the change is supposed to correct a mispredicted attribute value , we accept it only if it indeed makes the model more confident that the prediction is correct . ( a ) Attribute Prediction ( Section 4.1 ) CNNCNN y border width 2dp border color shadow 0dp text gravity center . . . p ( y | I ) 0.77 0.99 0.91 0.96 D = { ( render ( y ( i ) ) , y ( i ) ∼ Θ ) } N i=1 Training Input Image I p ( y | I ) Predicted Attributes render ( y ) Synthetic datasets We instantiate our approach by training it purely using synthetic datasets , while ensuring that it generalizes well to real-world images . This allows us to avoid the expensive task of collecting and annotating real-world datasets and more importantly , makes our approach easily applicable to new domains and attributes . In particular , given a space of possible attribute configurations Θ and a rendering function render , we generate two different datasets D and D∆ used to train the attribute prediction network and the policy π , respectively . The dataset D = { ( render ( y ( i ) ) , y ( i ) ∼ Θ ) } Ni=1 is constructed by sampling a valid attribute configuration y ( i ) ∼ Θ and rendering it to produce the input image . To generateD∆ , we sample two attribute configurations y ( i ) , y ( j ) ∼Θ that are used to render two input images and train the network to predict the difference of their attributes , that is , D∆ = { ( 〈render ( y ( i ) ∼ Θ ) , render ( y ( j ) ∼ Θ ) 〉 , ∆ ( y ( i ) −y ( j ) ) ) } Mi , j=1 . For both datasets , to avoid overfitting when training models for attributes with large domain of possible values , we sample only a subset of attributes , while setting the remaining attributes to values from the previous example y ( i−1 ) . As a result , every two consecutive samples are similar to each other , since a subset of their attributes is the same . Further , because the real-world images do not contain components in isolation but together with other components that fill the rest of the screen , we introduce three additional attributes xpos∈N , ypos∈N and background . We use xpos and ypos to denote the horizontal and vertical position of the component in the image , respectively . This allows the network to learn robust predictions regardless of the component position in the image . We use background to select the background on which the component is rendered . We experimented with three different choices of backgrounds – only while color , random solid color and overlaying the component on top of an existing application , all of which are evaluated in Section 5 .","We explore a new domain of learning to infer user interface attributes that helps developers automate the process of user interface implementation. Concretely, given an input image created by a designer, we learn to infer its implementation which when rendered, looks visually the same as the input image. To achieve this, we take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, we also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values. We instantiate our approach to the task of inferring Android Button attribute values and achieve 92.5% accuracy on a dataset consisting of real-world Google Play Store applications.","The article discusses the problem of inferring attributes of a user interface component from an image, with the goal of achieving pixel-accurate implementation. The authors propose a two-step approach that first predicts the most likely attribute values and then refines them using a policy learned from a synthetic dataset. The approach is instantiated by training a neural network on a synthetic dataset, ensuring generalization to real-world images, and avoiding the expensive task of collecting and annotating real-world datasets.","This paper proposes an approach for reverse-engineering webpages using Siamese networks and imitation learning. While the idea of using synthetic data (which can be easily procedurally generated) to do this reverse-engineer training is very clever, prior work has exploited it also. Novel elements include the attribute refinement using imitation learning, and the authors show the effect of this step, but the improvement is small. Thus, the limited novelty and not very convincing results make the question the potential impact of this paper.","The article discusses how several recently developed tools have been helping user interface designers in exploring and quickly prototyping different ideas. To help developers implement designs, approaches have been proposed for generating layout code that places user interface components at desired positions. The article presents a two-step process for predicting attribute values from an input image as a new domain of inferring implementation of a user interface component. The approach uses synthetic datasets to train models and was instantiated to infer the implementation of Android Button attributes and was shown to generalize well to real-world datasets.","2,0,1",,,,,,,,,,,,,,,,,
38,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"1 INTRODUCTION . Machine learning systems have shown exceptional performance on numerous tasks in computer vision and beyond . However performance drops rapidly when the standard assumption of i.i.d . training and testing data is violated . This domain-shift phenomenon occurs widely in many applications of machine learning ( Csurka , 2017 ; Zhou et al. , 2021 ; Koh et al. , 2021 ) , and often leads to disappointing results in practical machine learning deployments , since data ‘ in the wild ’ is almost inevitably different from reference training sets . Given the practical significance of this issue , a large number of methods have been proposed that aim to improve models ’ robustness to deployment under a different distribution than used for training ( Zhou et al. , 2021 ) , a problem setting known as domain generalisation ( DG ) . These methods span diverse approaches such as specialised neural architectures , data augmentation strategies , and regularisers . Nevertheless , the DG problem setting is difficult to model formally for principled derivation and theoretical analysis of algorithms , the target domain of interest is unobservable during training , and can not be directly approximated by the training domains due to unknown distribution shift . Therefore the majority of these existing approaches are based on poorly understood empirical heuristics . To make matters worse , a recent study by Gulrajani & Lopez-Paz ( 2021 ) assessed the state of DG research with a carefully conducted comparative evaluation of algorithms on a large benchmark suite under a common platform . They found that published methods were not as effective as claimed , and in particular reported that ‘ no existing method reliably beats a well tuned empirical risk minimization ( ERM ) baseline ’ . We argue that this negative result highlights the need for better theory in this area in order to understand why existing algorithms have such erratic performance , and to guide the development of principled algorithms that are more effective and reliable . To this end , our first contribution is to present an intuitive learning-theoretic bound for DG performance . Intuitively , while the held-out domain of interest is indeed unobservable during training , we can bound its performance using learning theoretic tools similar to the standard ones used to bound the performance on ( unobserved ) testing data given ( observed ) training data . In particular we show that the performance on a held out target domain is bounded by the performance on known source domains , plus two additional model complexity terms , that describe how much a model can possibly have overfitted to the training domains . This theoretical contribution leads to several insights . Firstly , our theory suggests that DG performance is governed by a trade-off between empirical risk and model complexity that is analogous to the corresponding and widely understood trade-off that explains generalisation in standard i.i.d . learning as an overfitting-underfitting trade-off ( Geman et al. , 1992 ) . Based on this , we hypothesise that performance variability is determined by implicit or explicit regularisation . That is , the plethora of different strategies available ( Zhou et al. , 2021 ) – from data-augmentation to specialised optimisers – actually affect DG performance by explicitly or implicitly choosing different fit-complexity trade-offs . We corroborate this hypothesis by evaluating a number of models in the DomainBed suite in terms of complexity , and showing that their apparently erratic performance in Gulrajani & Lopez-Paz ( 2021 ) is actually consistent with an explanation in terms of implied complexity . Practically , our analysis suggests that the model selection strategy ( Hastie et al. , 2009 ) is a factor in DG performance that is at least as important as the actual mechanism of model complexity control ( i.e. , Tuning of regularisation strength vs specific parametric design of regulariser ) . In particular , regularisation should be stronger when optimizing for future DG performance than when optimizing for performance on seen domains . Unfortunately , model complexity is hard to carefully control in deep learning due to the large number of relevant factors ( architecture , regularisers , implicit regularisation from optimiser , etc ) . Gulrajani & Lopez-Paz ( 2021 ) attempted to address this by hyper-parameter search in the DomainBed benchmark , but are hampered by the computational infeasibility of accurate hyper-parameter search . In this paper , we use linear models and off-the-shelf self-supervised features to demonstrate much more clearly how cross-domain performance depends on complexity . Specifically , our theoretical and empirical results show that , contrary to the conclusion of Gulrajani & Lopez-Paz ( 2021 ) , simple domain-wise cross-validation is a better objective to drive DG model selection . In summary , based on our new generalisation bound , and associated empirical analysis , our takehome messages are : ( i ) Model fit vs complexity trade-off is a key determinant of DG performance , that explains existing DG algorithm performance variability . ( ii ) The complexity control strategy used to determine bias-variance trade-off is crucial in practice , with peak DG performance achieved when optimizing model complexity based on domain-wise validation . ( iii ) Regularisation required for optimal DG is greater than for conventional optimization for within-domain performance . 2 RELATED WORK . Theoretical Analysis of the DG Setting and Algorithms The DG problem setting was first analysed in Blanchard et al . ( 2011 ) . Since then there have been some attempts to analyse DG algorithms from a generalisation bound perspective ( Muandet et al. , 2013 ; Blanchard et al. , 2021 ; Hu et al. , 2020 ; Albuquerque et al. , 2020 ; Rosenfeld et al. , 2021 ) . However these studies have theoretical results that are either restricted to specific model classes , such as kernel machines , or make strong assumptions about how the domains seen during training will resemble those seen at test time—e.g. , that all domains are convex combinations of a finite pre-determined set of prototypical domains . In contrast , our Rademacher complexity approach can be applied to a broad range of model classes ( including neural networks ) , and makes comparatively milder assumptions about the relationship between domains—i.e. , they are i.i.d . samples from another arbitrary distribution over domains . The majority of the existing work investigating the theoretical foundations of DG follow the initial formalisation of the domain generalisation problem put forth by Blanchard et al . ( 2011 ) , where the goal is to minimise the expected error over unseen domains . However , several recent works have also explored the idea of bounding the error on a single unseen domain with the most pathological distribution shift ( Janzing , 2019 ) . This type of analysis is typically rooted in methods from causal inference , rather than statistical learning theory . As a consequence , they are able to make stronger claims for the problems they address , but the scope of their analysis is necessarily limited to the scenarios where their assumptions about the underlying causal structures are valid . For example , Janzing ( 2019 ) provides bounds that assume problems conform to a specific class of structural equation models , and the analysis is performed under the assumption that infinite training data is available within each of the observed training domains . Throughout the work we address the stan- dard DG formalisation given by Blanchard et al . ( 2011 ) , where one is concerned with the expected performance of a model on domains sampled from some distribution over domains . Others rely on trying to link between domain adaptation objectives ( where target domains are observable for alignment to source domains ) and domain generalisation ( where target domains are not observable and thus can not correctly be used in a learning objective ) . Albuquerque et al . ( 2020 ) proceed by making assumptions on the structure of the distribution over possible domains ( i.e. , that it has support determined by the convex hull of a finite set of prototypical domains ) , which allows them to upper bound the domain alignment metric . Ye et al . ( 2021 ) provide a bound that depends on an unobservable domain distance quantity , which they then approximate in experiments using kernel density estimates . Rosenfeld et al . ( 2021 ) is another piece of work that theoretically investigates the generalisation of ERM in a DG setting . They deal with online DG , where each time-step corresponds to observing a new domain , and the learner must produce a new model capable of generalising to novel domains . Another point of difference between their work and the standard DG problem setting of Blanchard et al . ( 2011 ) is that the domain at each time-step is chosen by an adversary . They analyse this game for a finite number of time-steps , but they assume each domain has an infinite amount of data . They also put some limitations on the adversary : e.g. , it must choose a domain that is a convex combination of a finite number of pre-determined domains . In contrast , our theoretical analysis is in the more realistic setting where one has a finite amount of data per domain , and the domains we consider are not limited to convex combinations of a set of prototypical domains . Possibly the most similar work to our theoretical contributions is due to Ahuja et al . ( 2021 ) , who also provide learningtheoretic generalisation bounds for DG . However , their analysis only applies to finite hypothesis classes ( which does not include , e.g. , linear models or neural networks ) , whereas ours can be applied to any class amenable to analysis with Rademacher complexity . Empirical Analysis The main existing empirical analysis on DG is Gulrajani & Lopez-Paz ( 2021 ) , who compared several state of the art DG methods under a common evaluation and hyper-parameter tuning protocol called DomainBed . They ultimately defend Empirical Risk Minimization ( ERM ) over more sophisticated alternatives on the grounds that no competitor consistently beats it across the benchmark suite . We also broadly defend ERM , and build on the same benchmark , but differently we provide a much deeper analysis into when and why ERM works . More specifically : ( i ) We provide a new theoretical analysis of ERM ’ s generalisation quality unlike the prior purely empirical evaluation , ( ii ) We re-use the DomainBed benchmark to directly corroborate this theory under controlled conditions using linear models where model complexity can be tractably and accurately tuned . ( iii ) We use our complexity-based analysis to explain the previously erratic results of prior DomainBed competitors in terms of model complexity . ( iv ) We identify , and empirically validate , the preferred model selection criterion for DG , a point which was inconclusive in Gulrajani & Lopez-Paz ( 2021 ) . 3 BOUNDING RISK FOR DOMAIN GENERALIZATION . I.i.d . learning is concerned with learning a mapping from some input space X , to a label space Y , given data drawn from a distribution on X × Y . One aims to find a model , f∗ ∈ F , that minimises the expected loss ( also called risk ) on unseen data , f∗ = arg min f∈F Lp ( f ) , Lp ( f ) = E ( ~x , y ) ∼p [ ` ( f ( ~x ) , y ) ] , ( 1 ) where p is the data distribution and ` ( · , · ) is the loss function . In practice , we only have access to a finite set of data , S = { ( ~xi , yi ) } mi=1 , sampled i.i.d . from this distribution , so must minimise an empirical risk estimate , f̂ = arg min f∈F L̂p ( f ) , L̂p ( f ) = 1 m m∑ i=1 ` ( f ( ~xi ) , yi ) . ( 2 ) wherem is the number of training examples . One of the central focuses of statistical learning theory is to bound the difference between these two types of risk . For example , in the standard single domain setting this can be done via Lp ( f ) ≤ L̂p ( f ) + 2Rm ( F ) +O ( √ ln ( 1/δ ) m ) , ( 3 ) which holds with probability at least 1− δ , andR ( F ) is known as the empirical Rademacher complexity of the hypothesis class , F . This complexity term is defined as Rm ( F ) = E~σ [ sup f∈F 1 m m∑ i=1 σi ` ( f ( ~xi ) , yi ) ] . ( 4 ) For a hypothesis class consisting of norm-constrained linear classifiers , one can achieve the following upper bound on the empirical Rademacher complexity ( Shalev-Shwartz & Ben-David , 2014 ) , Rm ( F ) ≤ XB√ m , F = { ~x 7→ ~x · ~w : ‖~w‖2 ≤ B } , where we assume that each input , ~x , has a Euclidean norm of at mostX . There are a variety of ways to define hypothesis classes for neural networks , but most recent approaches take the view of fixing a particular architecture and specifying constraints on the norms of weights or distances they can move from their initialisations ( Bartlett et al. , 2017 ; Neyshabur et al. , 2019 ; Gouk et al. , 2021 ) . Domain generalisation . While standard i.i.d . learning assumes all data come from the same distribution , the DG problem setting assumes the existence of an environment E , of distributions p ( Blanchard et al. , 2011 ) . Note that we do not restrict what types of differences one could see between different domains sampled from E : for any two distributions in p , qsupp ( E ) it could be the case that either p ( ~x ) 6= q ( ~x ) , or p ( y|~x ) 6= q ( y|~x ) , or even that both types of distribution shift have occurred . The conceptually simplest—and often implicitly assumed—goal of DG methods is to minimise the expected risk across different distributions that could be sampled from the environment , LE ( f ) = Ep∼E [ Lp ( f ) ] . ( 5 ) This object is also the most commonly analysed idealised objective in the learning theory literature ( Blanchard et al. , 2011 ; Muandet et al. , 2013 ; Blanchard et al. , 2021 ; Rosenfeld et al. , 2021 ) , but other formulations also exist ( Arjovsky et al. , 2019 ) . As with the single domain learning problem , we only have access to an empirical estimate of the risk , L̂E ( f ) = 1 n n∑ j=1 L̂pj ( f ) , ( 6 ) where we assume for ease of exposition that all n domains have the same number of examples .","The domain generalization (DG) problem setting challenges a model trained on multiple known data distributions to generalise well on unseen data distributions. Due to its practical importance, a large number of methods have been proposed to address this challenge. However most of the work in general purpose DG is heuristically motivated, as the DG problem is hard to model formally; and recent evaluations have cast doubt on existing methods’ practical efficacy – in particular compared to a well chosen empirical risk minimisation baseline. We present a novel learning-theoretic generalisation bound for DG that bounds novel domain generalisation performance in terms of the model’s Rademacher complexity. Based on this, we conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. Algorithmically, this analysis suggests that domain generalisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective. Empirical results on the DomainBed benchmark corroborate this.","Domain generalization (DG) is the problem of improving machine learning models' ability to perform well on new unseen data from outside of the training data distribution. While a large number of empirical methods have been proposed to approach this problem, there is a lack of formal theory behind this field. This paper presents an intuitive learning-theoretic bound for DG performance, highlighting that implicit or explicit regularization plays a significant role in performance, and that model selection based on domain-wise validation performs better than other techniques.","The article discusses the problem of domain generalization in machine learning, where the goal is to train a model on a single dataset and generalize well to unseen domains. The authors present a new learning-theoretic bound for the performance of domain generalization models, which shows that the performance on a held-out target domain is bounded by the performance on known source domains plus two additional terms that depend on the complexity of the model.","In domain generalization, recent work has shown that ERM has comparable out-of-domain accuracy to state-of-the-art DG methods. The paper aims to explain this result through model complexity. The main argument is that ood generalization requires a smaller model complexity. The upshot is that DG methods can utilize cross-domain validation to obtain better generalizing models.","1,2,0",1,2,1,0,,"coverage, fluency",relevance (+ detail),coverage,relevance,0,1,0,2,Summary A has good coverage and fluency.,Summary B delivers too much details.,Summary A best represent the motivation of the paper. ,Summary C seems to be off target for the main motivation of the paper. 
39,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"1 INTRODUCTION . Traditional optimization methods for machine learning are usually designed to optimize a single objective . However , in many real-world machine learning applications , we are often required to optimize multiple correlated objectives simultaneously . For example , in autonomous driving ( Huang et al. , 2019 ; Lu et al. , 2019b ) , the self-driving vehicle needs to learn to solve multiple tasks such as self-localization and object identification at the same time . In online advertising ( Ma et al. , 2018a ; b ) , the advertiser aims to choose the exposure of items to different users so that both the Click-Through Rate ( CTR ) and the Post-Click Conversion Rate ( CVR ) are maximized simultaneously . In many multi-objective scenarios , the objectives may conflict with each other ( Kendall et al. , 2018 ) . Hence , there may not exist any single solution that optimizes all the objectives simultaneously . For example , in the online advertising scenario , merely optimizing either CVR or CTR will incur the degradation of performance of the other ( Ma et al. , 2018a ; b ) . Multi-objective optimization ( MOO ) ( Marler and Arora , 2004 ; Deb , 2014 ) is concerned with optimizing multiple conflicting objectives simultaneously . Many different approaches for MOO have been proposed , which include evolutionary methods ( Murata et al. , 1995 ; Zitzler and Thiele , 1999 ) , scalarization methods ( Fliege and Svaiter , 2000 ) and gradient-based iterative methods ( Désidéri , 2012 ) . Recently , due to the great success of training multi-task deep neural networks , the first-order gradient-based iterative methods , i.e. , Multiple Gradient Descent Algorithm ( MGDA ) and its variants , have regained a significant amount of research interest ( Sener and Koltun , 2018 ; Lin et al. , 2019 ; Yu et al. , 2020 ) . These methods compute a composite gradient based on the gradient information of all the individual objectives and then apply the composite gradient to model update . The determination of the composite gradient is based on a min-norm solver ( Désidéri , 2012 ) which yields the common descent direction of all the objectives . However , compared to the increasingly wide application prospect , the first-order gradient-based iterative algorithms are relatively understudied , especially for the online learning setting . Multiobjective online learning is very important due to reasons in two main aspects . First , due to the data explosion in many real-world scenarios such as various web applications , making in-time prediction requires to perform online learning . Second , the theoretical investigation will lay a solid foundation for the design of new optimizers of multi-task deep neural networks , such as multi-objective Adam . In this paper , we conduct a systematic study of multi-objective online learning . To begin with , we first formulate the framework of Multi-Objective Online Convex Optimization ( MO-OCO ) . The biggest challenge in the design of MO-OCO is the derivation of an appropriate regret definition in the multi-objective setting . Since the multiple objectives form a vector space , we need a discrepancy metric to scalarize the loss vector . Specifically , we adopt the Pareto suboptimality gap ( PSG ) , which is a distance-based discrepancy metric extensively used in multi-objective bandits ( Turgay et al. , 2018 ; Lu et al. , 2019a ) . Then analogously to the single-objective online setting , we define the multi-objective static regret and the multi-objective dynamic regret . However , since PSG is a metric motivated purely from the geometric view , it is intrinsically difficult to directly optimize using firstorder gradient-based iterative methods . To remedy this problem , for the multi-objective dynamic regret , via a highly non-trivial transformation , we derive its equivalent regret in the unconstrained max-min form , which is much easier to optimize . Unfortunately , we further show that such an equivalence does not hold for the multi-objective static regret since PSG always yields non-negative measurements . Hence , in this paper , we mainly focus on studying the multi-objective dynamic regret and leave the complete treatment of its static counterpart as an open problem . Based on the proposed MO-OCO framework , we develop the Online Mirror Multiple Descent ( OMMD ) algorithm . The key module of OMMD is the gradient composition scheme , which utilizes the information of all the individual gradients to compute a composite gradient that tends to descend all the losses simultaneously . By directly applying the min-norm solver ( Désidéri , 2012 ) in the offline setting to determine the composition weights , we give the first variant of OMMD termed as OMMD-I . However , the min-norm solver only uses the instantaneous gradients and ignores the historical information , which can be very unstable in the online setting where the losses and the gradients at different rounds can vary wildly . To make the learning process more stable , we introduce a carefully designed L1-regularizer to the vanilla min-norm solver , which results in the second variant of OMMD , namely OMMD-II . We then give the theoretical analysis of the proposed OMMD algorithm . Specifically , we derive a non-trivial dynamic regret bound of OMMD-II , which includes that of OMMD-I as a special case . The dynamic regret bound is in the same order O ( V 1/3T T 2/3 ) , where T is the time horizon and VT is the temporal variability at T , as that of its single-objective counterpart ( Zhang et al. , 2018 ) . We further show that the regret bound of OMMD-II is lower than that of OMMD-I . To evaluate the effectiveness of our proposed algorithm , we conduct extensive experiments using both simulation datasets and real-world datasets . Specifically , we first design simulation experiments to verify the capability of OMMD-II to track the dynamic online data streams . Then we successfully realize adaptive regularization for online learning using our MO-OCO formalism , which demonstrates the effectiveness of OMMD-II in the convex setting . We further conduct online multitask learning experiments with deep neural networks , the results of which show that both OMMD-I and OMMD-II are effective in the non-convex setting . Moreover , in both simulation and multi-task deep experiments , OMMD-II yields better performance than OMMD-I , which verifies the theoretical superiority of the regularized min-norm method over the vanilla min-norm method . In summary , in this paper , we give the first systematic study of multi-objective online learning , which includes novel framework , algorithm design and theoretical analysis . We believe that our paper paves the way for future research on multiple-objective optimization and multi-task learning . 2 PRELIMINARIES . In this section , we briefly review the necessary background knowledge of online convex optimization and multi-objective optimization . 2.1 ONLINE CONVEX OPTIMIZATION . Online Convex Optimization ( OCO ) ( Zinkevich , 2003 ; Hazan , 2019 ) is the most commonly adopted framework for designing online learning algorithms . It can be viewed as a structured repeated game between a learner and an adversary . At each round t ∈ { 1 , . . . , T } , the learner is required to generate a decision xt from a convex compact set X ⊂ Rn . Then the adversary replies the learner with a convex function ft : X → R and the learner suffers the loss ft ( xt ) . The goal of the learner is to minimize the regret with respect to the best fixed decision in hindsight , i.e. , RS ( T ) = T∑ t=1 ft ( xt ) − min x∗∈X T∑ t=1 ft ( x ∗ ) . Note that the above regret is the static regret ( Hall and Willett , 2013 ) , which compares the learner ’ s cumulative loss with that of a fixed decision . There is another version of regret , namely the dynamic regret ( Hall and Willett , 2013 ; Zhang et al. , 2018 ) , which compares the learner ’ s cumulative loss with that of a sequence of changing decisions , i.e. , RD ( T ) = T∑ t=1 ft ( xt ) − T∑ t=1 min x∗t∈X ft ( x ∗ t ) . Any meaningful regret is required to be sublinear in T , i.e. , limT→∞ RS/D ( T ) /T = 0 , which implies that when T is large enough , the learner can perform as well as the best fixed decision in hindsight ( for static regret ) or the changing optimal decisions at each round ( for dynamic regret ) . Online Mirror Descent ( OMD ) ( Hazan , 2019 ) is a classic first-order online learning algorithm . At each round t ∈ { 1 , . . . , T } , OMD yields its decision using the following formula xt+1 = argmin x∈X η⟨∇ft ( xt ) , x⟩+BR ( x , xt ) , where η is the step size , R : X → R is the regularization function , and BR ( x , x′ ) = R ( x ) − R ( x′ ) − ⟨∇R ( x′ ) , x − x′⟩ is the Bregman divergence induced from R. As a generic algorithm , by instantiating different regularization functions , OMD can induce two important algorithms , i.e. , Online Gradient Descent ( Zinkevich , 2003 ) and Online Exponentiated Gradient ( Hazan , 2019 ) . Several papers work on the dynamic regret of online mirror descent ( Jadbabaie et al. , 2015 ; Shahrampour and Jadbabaie , 2017 ) . 2.2 MULTI-OBJECTIVE OPTIMIZATION . Multiple-objective optimization ( MOO ) is concerned with solving the problems of optimizing multiple objective functions simultaneously ( Zitzler and Thiele , 1999 ; Sener and Koltun , 2018 ) . In general , since different objectives may conflict with each other , there is no single solution that can optimize all the objectives at the same time . Instead , MOO seeks to find solutions that achieve Pareto optimality . In the following , we exposit Pareto optimality and related definitions more formally using a vector-valued loss H = ( h1 , . . . , hm ) ⊤ as objectives , where m ≥ 2 and hi : K → R , i ∈ { 1 , . . . , m } , K ⊂ R , is the i-th loss function . Definition 1 ( Pareto optimality ) . ( a ) For any two solutions x , x′ ∈ K , we say that x dominates x′ , denoted as x ≺ x′ or x′ ≻ x , if hi ( x ) ≤ hi ( x′ ) for all i , and there exists one i such that hi ( x ) < hi ( x′ ) ; otherwise , we say that x does not dominate x′ , denoted as x ⊀ x′ or x′ ⊁ x . ( b ) A solution x∗ ∈ K is called Pareto optimal if it is not dominated by any other solution in K. There may exist multiple Pareto optimal solutions . For example , it is easy to show that the optimizer of any single objective , say , x∗1 ∈ argminx∈K h1 ( x ) , is Pareto optimal . Different Pareto optimal solutions reflect different trade-offs among the objectives ( Sener and Koltun , 2018 ; Lin et al. , 2019 ) . Definition 2 ( Pareto front ) . ( a ) All Pareto optimal solutions form the Pareto set , denoted as PK ( H ) . ( b ) The image of PK ( H ) constitutes the Pareto front , denoted as P ( H ) = { H ( x ) | x ∈ PK ( H ) } . Now that we ’ ve established the notion of optimality in MOO , we proceed to introduce the metrics that measure the discrepancy of an arbitrary solution x ∈ K from being optimal . Recall that , in the single-objective setting with merely one loss function h : Q → R , where Q ⊂ R , for any z ∈ Q , the loss gap h ( z ) − minz′′∈Q h ( z′′ ) is directly the discrepancy measure . However , in MOO with more than one loss , for any x ∈ K , the loss gap H ( x ) − H ( x′′ ) , where x′′ ∈ PK ( H ) , is a vector . Intuitionally , the desired discrepancy metric shall scalarize the vector-valued loss gap and yield the value 0 for any Pareto optimal solution . In general , there are two commonly used discrepancy metrics in MOO , namely Pareto suboptimality gap ( PSG ) ( Turgay et al. , 2018 ) and Hypervolume ( HV ) ( Bradstreet , 2011 ) . As HV is a volume-based metric , it is very difficult to optimize or analyze via iterative algorithms ( Zhang and Golovin , 2020 ) . Hence in this paper , we adopt PSG , which has been extensively used in multi-objective bandits ( Turgay et al. , 2018 ; Lu et al. , 2019a ) . Definition 3 ( Pareto suboptimality gap ) . For any x ∈ K , the Pareto suboptimality gap to a given comparator set K∗ ⊂ K , denoted as ∆ ( x ; K∗ , H ) , is defined as the minimal scalar ϵ ≥ 0 that needs to be subtracted from all entries of H ( x ) , such that H ( x ) − ϵ1 is not dominated by any point in K∗ , where 1 denotes the all-one vector in Rm , i.e. , ∆ ( x ; K∗ , H ) = inf ϵ≥0 ϵ s.t . ∀x′′ ∈ K∗ , ∃i ∈ { 1 , . . . , m } , hi ( x ) − ϵ < hi ( x′′ ) . Clearly , PSG is a distance-based discrepancy metric that motivated from a purely geometric viewpoint . In practice , the comparator set K∗ is often set to be the Pareto set PK ( H ) ( Turgay et al. , 2018 ) . Then for any x ∈ K , its PSG is always non-negative and equals to zero if and only if x ∈ PK ( H ) . Multiple Gradient Descent Algorithm ( MGDA ) is an offline first-order algorithm for MOO ( Fliege and Svaiter , 2000 ; Désidéri , 2012 ) . At each iteration l ∈ { 1 , . . . , L } , where L is the maximum number of iterations , it first computes the gradient ∇hi ( xl ) for each objective i ∈ { 1 , . . . , m } , and then derive the composite gradient gl = ∑m i=1 λ i l∇hi ( xl ) as the convex combination of these multiple gradients ; it applies the composite gradient to execute the gradient descent step to update the decision , i.e. , xl+1 = xl − ηgl , where η is the step size . The core module of MGDA is the determination of the weights λl = ( λ1l , . . . , λ m l ) for the gradient composition , which is given as λl = argmin λl∈∆m ∥ m∑ i=1 λil∇hi ( xl ) ∥22 , where ∆m = { λ ∈ Rm | λi ≥ 0 , i ∈ { 1 , . . . , m } , and ∑m i=1 λ i = 1 } denotes the probabilistic simplex in Rm . This is a min-norm solver which finds the weights in the simplex that yields the minimum L2 norm of the composite gradient . Thus MGDA is also called the min-norm method . Existing works ( Désidéri , 2012 ; Sener and Koltun , 2018 ) have shown that MGDA is guaranteed to decrease all the objectives simultaneously until it reaches a Pareto optimal decision ( under the convex setting where all hi are convex functions ) .","This paper presents a systematic study of multi-objective online learning. We first formulate the framework of Multi-Objective Online Convex Optimization, which encompasses a novel multi-objective dynamic regret in the unconstrained max-min form. We show that it is equivalent to the regret commonly used in the zero-order multi-objective bandit setting and overcomes the problem that the latter is hard to optimize via first-order gradient-based methods. Then we propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed L1-regularized min-norm solver. We further derive regret bounds of both variants and show that the L1-regularized variant enjoys a lower bound. Extensive experiments demonstrate the effectiveness of the proposed algorithm and verify the theoretical advantage of the L1-regularized variant.","In this paper, we address the problem of multi-objective online learning, which involves optimizing multiple conflicting objectives simultaneously in a dynamic environment. We propose a novel framework for multi-objective online convex optimization (MOCO) and develop an algorithm called Online Mirror Multiple Descent (OMMD). OMMD uses a gradient composition scheme to compute a composite gradient that descends all losses simultaneously, and it is designed to handle the issue of multiple objectives by using a Pareto suboptimality gap as a discrepancy metric. We prove the dynamic regret bound of OMMD and show that it is lower than that of a baseline algorithm in some cases. We also conduct extensive experiments using both simulation datasets and real-world datasets to evaluate the effectiveness of OMMD. The results show that OMMD outperforms a baseline algorithm in most cases and is able to adapt to the dynamic online data streams. Finally, we believe that our work paves the way for future research on multiple-objective optimization and multi-task learning.","The paper formulates a novel framework for multi-objective online convex optimization. The novel framework, similarly to the single-objective online convex optimization framework, can be viewed as a two players repeated game where at each round the online learner selects a point $x_t$ and the (possibly adversarial) environment selects a vector valued loss function $F_t (\cdot )$. To extend the notion of regret to the multi-objective setting at each round the suboptimality of $x_t$ is measured using the Pareto suboptimality gap (PSG). Two algorithms (OMMD-I and OMMD-II) which upper bound the extension of the dynamic regret in the multi-objective setting are designed. The two algorithms can be viewed as extensions of the min-norm method for offline multi-objective optimization in the online setting. In both algorithms a composite gradient, which is a convex combination of the descent directions for every objective, is calculated. This composite gradient is used to select, together with a Bregman reguralization, the descent direction and consequently the point $x_{t+1}$. The main difference between the two algorithms OMMD-I and OMMD-II is that in the latter the composition of the descent directions in two consecutive rounds will possibly be ""more similar"" (due to the regularization term $|| \lambda - \lambda_{t-1}||_1$). ","Machine learning often requires simultaneous optimization of multiple correlated objectives, leading to multi-objective optimization (MOO). MOO has many approaches, including first-order gradient-based iterative methods. In MOO, Pareto optimality is often used, and the Pareto suboptimality gap (PSG) is used to scalarize the loss vector. Online Convex Optimization (OCO) is used to create online learning algorithms where an adversary replies to a learner with a convex function to test the proficiency of the learner in generating decisions. The formulation of multi-objective online convex optimization (MO-OCO) is challenging since PSG is a distance-based discrepancy metric. OMMD is an algorithm designed for MO-OCO, and its key module is gradient composition. The authors conduct experiments using both simulation datasets and real-world datasets to reveal the effectiveness of the proposed algorithm. The paper paves the way for future research on multiple-objective optimization and multi-task learning.","2,0,1",,,,,,,,,,,,,,,,,
40,SP:64f2744e938bd62cd47c1066dc404a42134953da,"1 INTRODUCTION . Many methods have been developed to estimate the causal effect of an intervention , such as the administration of a treatment , on an outcome such as survival , from observational data , i.e. , data that is potentially confounded by selection bias due to the absence of randomization . Classical ones include matching ( Iacus et al. , 2012 ) , inverse propensity weighting ( IPW , Horvitz & Thompson , 1952 ; Rosenbaum & Rubin , 1983 ) and doubly robust methods ( Robins et al. , 1994 ; Chernozhukov et al. , 2018 ; Wager & Athey , 2018 ; Athey et al. , 2019 ) . More recent proposals use deep learning methods that ensure balance of the population at the level of representation ( Johansson et al. , 2016 ; Shalit et al. , 2017 ) , infer the joint distribution of latent and observed confounders , the treatment and the outcome ( Louizos et al. , 2017 ) or predict the counterfactuals with GANs ( Yoon et al. , 2018 ) . For a detailed review of existing literature on treatment effect estimation we refer to Imbens ( 2004 ) , Lunceford & Davidian ( 2004 ) and Guo et al . ( 2019 ) . However , state-of-the-art methods still suffer from important shortcomings . In particular , they seldom consider the possibility that covariates have missing values , which is ubiquitous in many realworld situations ( Josse & Reiter , 2018 ) and has been widely discussed in different contexts ( Mayer et al. , 2019a ; van Buuren , 2018 ; Little & Rubin , 2002 ) . Although this question of missing attributes in the context of treatment effect estimation has been raised early in the development of causal inference ( Rosenbaum & Rubin , 1984 ) , there is still a lack of effective and consistent solutions addressing this problem , with a few notable exceptions such as Mattei & Mealli ( 2009 ) ; Seaman & White ( 2014 ) ; Yang et al . ( 2019 ) ; Kallus et al . ( 2018 ) which mainly focus on inverse propensity weighting ( IPW ) methods and Kuroki & Pearl ( 2014 ) who discuss identifiability of causal effects under measurement error or unobserved confounders . Recently , Mayer et al . ( 2019b ) , in addition to suggesting doubly robust estimators with missing data , classified the existing approaches into two families : the ones that adapt the causal inference assumptions to the missing values setting ( D ’ Agostino Jr & Rubin , 2000 ; Blake et al. , 2019 ) and the ones ( Mattei & Mealli , 2009 ; Seaman & White , 2014 ; Kallus et al. , 2018 ) that consider the classical machinery and missingness mechanisms assumptions ( Little & Rubin , 2002 ) . While the former are based on the assumption of unconfoundedness with missing values , which can be difficult to assess in practice , the latter have been developed under strong para- metric assumptions about the outcome , treatment and covariates models , in addition to relying on missing values hypotheses that can also be difficult to meet in practice ( Yang et al. , 2019 ) . To avoid relying on the hypothesis of unconfoundedness with missing values or being in the very parametric ( and linear ) framework of multiple imputation ( Mattei & Mealli , 2009 ; Seaman & White , 2014 ) and matrix factorization ( Kallus et al. , 2018 ) , we propose a new method for causal inference with missing data , which we call MissDeepCausal . MissDeepCausal is inspired by the work of Kallus et al . ( 2018 ) in the sense that we consider a model with latent confounders , and assume that we only have access to covariates with missing values that are noisy proxies of the true latent confounders . However , our approach generalizes and extends the work of Kallus et al . ( 2018 ) in different aspects : ( i ) instead of linear factor analysis models with missing values , we consider nonlinear versions using deep latent variable models ( Kingma & Welling , 2014 ; Rezende et al. , 2014 ) ; ( ii ) we rely on the missing at random ( MAR ) ( Rubin , 1976 ) assumption for the missing data mechanisms , and not on the stronger missing completely at random ( MCAR ) one ; ( iii ) we take into account the posterior distribution of the latent variables given observed data and not only their conditional expectation . This latter point allows us to define a multiple imputation strategy adapted to the latent confounders model , and to couple it with doubly robust treatment effect estimation ( Chernozhukov et al. , 2018 ) . In the remainder of this article we first introduce the problem framework and recall existing work for handling missing values in causal inference in Section 2 . We then introduce two variants of our MissDeepCausal approach in Section 3 . Finally we compare MissDeepCausal empirically with several state-of-the-art methods on simulated data in Section 4 . 2 SETTING , NOTATIONS AND RELATED WORKS . In this section we start by quickly reviewing the problem of causal inference from observational data without missing data . We consider the potential outcomes framework ( Rubin , 1974 ; Imbens & Rubin , 2015 ) where we have a sample of n independent and identically distributed ( i.i.d . ) observations ( Yi ( 0 ) , Yi ( 1 ) , Wi , Xi ) i=1 , ... , n with Wi ∈ { 0 , 1 } a binary treatment , Xi = ( Xi1 , . . . , Xip ) > ∈ Rp a vector of covariates , and ( Yi ( 0 ) , Yi ( 1 ) ) ∈ R2 the outcomes we would have observed had we assigned control or treatment to the i-th sample , respectively . The observed outcome for unit i , Yi ∈ R is defined as Yi , WiYi ( 1 ) + ( 1 − Wi ) Yi ( 0 ) . The individual causal effect of the treatment is τi , Yi ( 1 ) − Yi ( 0 ) and the average treatment effect ( ATE ) is defined as τ , E [ Yi ( 1 ) − Yi ( 0 ) ] = E [ τi ] . The ATE τ , i.e. , the link between W and Y , can be estimated from observational data by taking into account the confounding factors X , i.e. , the common causes of W and Y . A popular estimator of τ from observational data is the so-called doubly robust estimator : τ̂DR , 1 n n∑ i=1 µ̂1 ( Xi ) − µ̂0 ( Xi ) +Wi Yi − µ̂1 ( Xi ) ê ( Xi ) − ( 1−Wi ) Yi − µ̂0 ( Xi ) 1− ê ( Xi ) , ( 1 ) where µ̂w ( x ) are regression estimates of the conditional response surfaces µw ( x ) , E [ Y ( w ) |X = x ] , w ∈ { 0 , 1 } , and ê ( x ) is an estimate of the propensity score e ( x ) , P ( Wi = 1 |Xi = x ) ( Rosenbaum & Rubin , 1983 ; Imbens & Rubin , 2015 ) . Standard results state that if either ( µ̂0 , µ̂1 ) or ê is correctly specified , then τ̂DR is an unbiased estimator of τ ( Robins et al. , 1994 ; Chernozhukov et al. , 2018 ; Wager & Athey , 2018 ) under the following assumptions ( Rosenbaum & Rubin , 1983 ) : the ignorability or unconfoundedness assumption that states that all confounding factors are measured , i.e. , conditionally on X , the treatment assignment is independent of the potential outcomes : { Yi ( 1 ) , Yi ( 0 ) } ⊥ Wi |Xi , for all i ; ( 2 ) and the overlap assumption assuming the existence of some η > 0 such that η < e ( x ) < 1 − η , for all x ∈ X . We now consider an extension to account for possible missing entries in the covariates . For that purpose , we denote the missingness pattern of the i-th sample as Mi ∈ { 0 , 1 } p such that Mij = 0 if Xij is observed and Mij = 1 otherwise . The matrix of observed covariates can be written as X ? , X ( 1 −M ) + NA M , with the elementwise multiplication and 1 the matrix filled with 1 , so that X ? takes its value in the half discrete space X ? , ( R ∪ { NA } ) p. We model Mi as a random vector , and the possibility to infer causal effects with missing data now depends on additional assumptions on the joint law of ( Yi ( 0 ) , Yi ( 1 ) , Wi , Xi , Mi ) i=1 , ... , n. Methods for causal inference with missing covariates can be classified into two categories . Unconfoundedness with missing values . Rosenbaum & Rubin ( 1984 ) extend the unconfoundedness hypothesis ( 2 ) to missing values as { Yi ( 1 ) , Yi ( 0 ) } ⊥ Wi |X ? i , for all i . ( 3 ) This implies the assumption , illustrated in Figure 1 , that if a covariate is not observed , it is not a confounder . In particular , observations can have different confounders depending on their pattern of missing data . They define the generalized propensity score as : ∀x ? ∈ X ? , e ? ( x ? ) , P ( Wi = 1 |X ? i = x ? ) , ( 4 ) which is a balancing score under ( 3 ) . Consequently , an IPW estimator formed with estimators of e ? can be an unbiased estimator of the ATE with missing values . Nevertheless , this method relies both on the fact that the covariates X are the appropriate set of confounders , which can be questioned without missing data ( Kallus et al. , 2018 ) , and requires certain expert input and reasoning to verify that for each observation , treatment assignment and/or outcome values depend only on observed values of the confounders ( Blake et al. , 2019 ; Mayer et al. , 2019b ) . Note in particular , that it is not because the missing data in the covariates are completely at random ( MCAR ) , i.e. , M ⊥ X , that ( 3 ) is met . In practice , in addition , a difficulty with this approach is that estimating ( 4 ) requires fitting one model per pattern of missing values , which is unrealistic with classical tools ( Miettinen , 1985 ; D ’ Agostino Jr & Rubin , 2000 ; D ’ Agostino Jr et al. , 2001 ; Blake et al. , 2019 ) ; Mayer et al . ( 2019b ) address this problem using random forests adapted to covariates with missing values . Missingness mechanisms assumptions . Multiple imputation is one of the most powerful approaches to estimate parameters and their variance from an incomplete data ( Little & Rubin , 2002 ; van Buuren , 2018 ) . Seaman & White ( 2014 ) show that when assuming ( i ) identifiability of the ATE in the complete case , ( ii ) missing at random ( MAR ) values givenW and Y , ( iii ) correct specification of the propensity score with logistic regression and of the Gaussian distribution of covariates , then multiple imputation gives a consistent estimate for the ATE estimated with IPW . An extension to doubly robust estimation has been proposed by Mayer et al . ( 2019b ) . Instead of assuming that confounders are observed directly , Kallus et al . ( 2018 ) consider a more general model where observed covariates X are noisy and/or incomplete proxies of the true latent confounders Z . More specifically , they assume a low-rank model for the covariates and estimate the latent variables from the incomplete confounders using matrix completion methods ( Hastie et al. , 2015 ; Josse et al. , 2016 ) . Then , under the linear regression model Yi = Z T i α+ τWi + εi , ( 5 ) with random latent variables Z , missing values completely at random ( MCAR ) in X , unconfoundedness given Z , and some additional assumptions , they prove that regressing Y on Ẑ and W leads to a consistent ATE estimator . Both techniques , multiple imputation and matrix factorization , rely on parametric ( and linear ) frameworks .","Inferring causal effects of a treatment, intervention or policy from observational data is central to many applications. However, state-of-the-art methods for causal inference seldom consider the possibility that covariates have missing values, which is ubiquitous in many real-world analyses. Missing data greatly complicate causal inference procedures as they require an adapted unconfoundedness hypothesis which can be difficult to justify in practice. We circumvent this issue by considering latent confounders whose distribution is learned through variational autoencoders adapted to missing values. They can be used either as a pre-processing step prior to causal inference but we also suggest to embed them in a multiple imputation strategy to take into account the variability due to missing values. Numerical experiments demonstrate the effectiveness of the proposed methodology especially for non-linear models compared to competitors.","The article discusses the problem of causal inference from observational data with missing covariates and proposes a new method called MissDeepCausal. The method considers a nonlinear latent variable model and relies on the missing at random ( MAR ) assumption for the missing data mechanisms. The article also introduces two variants of MissDeepCausal, one of which uses a multiple imputation strategy adapted to the latent confounders model, and the other uses doubly robust treatment effect estimation. The article compares MissDeepCausal with several state-of-the-art methods on simulated data and shows that it outperforms them in terms of bias reduction. The article concludes by highlighting the advantages of MissDeepCausal over existing methods, including its ability to handle complex missing data mechanisms and its robustness to misspecification of the latent variable model.","This article proposes a new method for causal inference with missing data, called MissDeepCausal. The method addresses the shortcomings of existing causal inference methods that seldom consider the possibility of covariate missing values. MissDeepCausal is inspired by the work of Kallus et al. and considers a model with latent confounders. It assumes that the covariates with missing values are noisy proxies of the true latent confounders and models them using deep latent variable models. The missing data mechanism is assumed to be missing at random (MAR), and the posterior distribution of the latent variables is taken into account. It defines a multiple imputation strategy adapted to the latent confounders model and couples it with doubly robust treatment effect estimation. The method is compared with several state-of-the-art methods on simulated data. Existing methods for causal inference with missing data can be classified into two categories: unconfoundedness with missing values and missingness mechanisms assumptions.","This contribution considers deep latent-factor models for causal inference -inferring the effect of a treatment- in the presence of missing values. The core challenge is that of confounders not directly observed, and accessible only via noisy proxys, in particular in the missingness. The contributed method relies on using the latent-factor model for multiple imputation in doubly-robust causal treatment effect estimators. As a consequence, it requires the missing at random assumption to control for the impact of imputation. Given that the confounders are not directly assumed, a first approach estimates their effect via an estimate of P(Z|X*)  (probability of confounder given observed data), which is then plugged in the doubly robust estimator in a multiple imputation strategy. A second approach uses heuristically the estimated latent confounders as regressors of non interest in a linear-regression model. The approaches are empirically compared to other imputation strategies used as plugins in the doubly-robust estimator. The contributed approach show marked benefits when the problem is highly non-linear.","2,1,0",2,1,1,0,,coverage,coherence,"coverage, coherence",relevance (+ detail),0,1,1,2,Summary A has good coverage without excessive details.,Summary B has a coherence issue in the last sentence.,"Summary B best show the motivation of the paper, although there are facts that cannot be verified by the abstract. It is also the most coherent one. ",Summary C is a bit too detailed (including a formula in the summary seems unnecessary) for a summary. 
41,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"Sampling from a complex distribution π and approximating its intractable normalizing constant Z are challenging problems . In this paper , a novel family of importance samplers ( IS ) and Markov chain Monte Carlo ( MCMC ) samplers is derived . Given an invertible map T , these schemes combine ( with weights ) elements from the forward and backward Orbits through points sampled from a proposal distribution ρ . The map T does not leave the target π invariant , hence the name NEO , standing for Non-Equilibrium Orbits . NEO-IS provides unbiased estimators of the normalizing constant and self-normalized IS estimators of expectations under π while NEO-MCMC combines multiple NEO-IS estimates of the normalizing constant and an iterated sampling-importance resampling mechanism to sample from π . For T chosen as a discrete-time integrator of a conformal Hamiltonian system , NEO-IS achieves state-of-the art performance on difficult benchmarks and NEO-MCMC is able to explore highly multimodal targets . Additionally , we provide detailed theoretical results for both methods . In particular , we show that NEO-MCMC is uniformly geometrically ergodic and establish explicit mixing time estimates under mild conditions . 35th Conference on Neural Information Processing Systems ( NeurIPS 2021 ) . 1 Introduction . Consider a target distribution of the form π ( x ) ∝ ρ ( x ) L ( x ) where ρ is a probability density function ( pdf ) on Rd and L is a nonnegative function . Typically , in a Bayesian setting , π is a posterior distribution associated with a prior distribution ρ and a likelihood function L. Another situation of interest is generative modeling where π is the distribution implicitly defined by a Generative Adversarial Networks ( GAN ) discriminator-generator pair where ρ is the distribution of the generator and L is derived from the discriminator [ 31 , 6 ] . In a Variational Auto Encoder ( VAE ) context [ 14 , 5 ] , π could be the true posterior distribution , ρ the approximate posterior distribution output by the encoder and L an importance weight between the true posterior and approximate posterior distributions . We are interested in this paper in sampling from π and approximating its intractable normalizing constant Z = ∫ ρ ( x ) L ( x ) dx . These problems arise in many applications in statistics , molecular dynamics or machine learning , and remain challenging . Many approaches to compute normalizing constants are based on Importance Sampling ( IS ) - see [ 1 , 2 ] and the references therein - and its variations , among others , Annealed Importance Sampling ( AIS ) [ 19 , 34 , 10 ] and Sequential Monte Carlo ( SMC ) [ 9 ] . More recently , Neural IS has also become very popular in machine learning ; see e.g . [ 11 , 18 , 21 , 22 , 32 , 33 ] . Neural IS is an adaptive IS which relies on an importance function obtained by applying a normalizing flow to a reference distribution . The parameters of this normalizing flow are chosen by minimizing a divergence between the proposal and the target ( such as the Kullback–Leibler [ 18 ] or the χ2-divergence [ 1 ] ) . Recent work on the subject proposes to add stochastic moves in order to enhance the performance of the normalizing flows [ 33 ] . More recently , the Non-Equilibrium IS ( NEIS ) method has been introduced by [ 23 ] as an alternative to these approaches . Similar to Neural IS , NEIS consists in transporting samples { Xi } Ni=1 from a reference distribution using a family of deterministic mappings . For NEIS , this family is chosen to be an homogeneous differential flow ( φt ) t∈R . In contrast to Neural IS , for any i ∈ [ N ] , the sample Xi is propagated both forward and backward in time along the orbits associated with ( φt ) t∈R until stopping conditions are met . Moreover , the resulting estimator of the normalizing constant is obtained by computing weighted averages of the whole orbit ( φt ( Xi ) ) t∈ [ τ+ , i , τ− , i ] , where τ+ , i , τ− , i are the resulting stopping times , and not only the endpoints φτ+ , i ( X i ) , φτ− , i ( X i ) . In [ 23 ] , the authors provide an application of NEIS with ( φt ) t∈R associated with a conformal Hamiltonian dynamics , and reports impressive numerical results on difficult normalizing constants estimation problems , in particular for high-dimensional multimodal distributions . We propose in this work NEO-IS which alleviates the shortcomings of NEIS . Similar to NEIS , samples are drawn from a reference distribution , typically set to ρ , and are propagated under the forward and backward orbits of a discrete-time dynamical system associated with an invertible transform T. An estimator of the normalizing constant is obtained by reweighting all the points on the whole orbits using the IS rule . Contrary to NEIS , the NEO-IS estimator of Z is unbiased under assumptions that are mild and easy to verify . It is more flexible than NEIS because it does not rely on the accuracy of the discretization of a continuous-time dynamical system . We then show how it is possible to leverage the unbiased estimator of Z defined by NEO-IS to obtain NEO-MCMC , a novel massively parallel MCMC algorithm to sample from π . In a nutshell , NEOMCMC relies on parallel walkers which each estimates the normalizing constant but are allowed to interact through a resampling mechanism . Our contributions can be summarized as follows . ( i ) We present a novel class of IS estimators of the normalizing constant Z referred to as NEO-IS . More broadly , a small modification of this algorithm also allows us to estimate integrals with respect to π . Both finite sample and asymptotic guarantees are provided for these two methodologies . ( ii ) We develop a new massively parallel MCMC method , NEO-MCMC . NEO-MCMC combines NEO-IS unbiased estimator of the normalizing constant with iterated samplingimportance resampling methods . We prove that it is π-reversible and ergodic under very general conditions . We derive also conditions which imply that NEO-MCMC is uniformly geometrically ergodic ( with an explicit expression of the mixing time ) . ( iii ) We illustrate our findings using numerical benchmarks which show that both NEO-IS and NEO-MCMC outperform state-of-the-art ( SOTA ) methods in difficult settings . 2 NEO-IS algorithm . In this section , we derive the NEO-IS algorithm . The two key ingredients for this algorithm are ( 1 ) the reference distribution ρ and ( 2 ) a transformation T assumed to be a C1-diffeomorphism with inverse T−1 . Write , for k ∈ N∗ = N\ { 0 } , Tk = T ◦Tk−1 , T0 = Idd and similarly T−k = T−1 ◦T− ( k−1 ) . For any k ∈ Z , denote by ρk : Rd → R+ the pushforward of ρ by Tk , defined for x ∈ Rd by ρk ( x ) = ρ ( T −k ( x ) ) JT−k ( x ) , where JΦ ( x ) ∈ R+ is the absolute value of the Jacobian determinant of Φ : Rd → Rd evaluated at x . In line with multiple importance sampling à la Owen and Zhou [ 20 ] , we introduce the proposal density ρT ( x ) = Ω −1 ∑ k∈Z $ kρk ( x ) , ( 1 ) where { $ k } k∈Z is a nonnegative sequence and Ω = ∑ k∈Z $ k . Note that we assume in the sequel that the support of the weight sequence defined as { k ∈ Z : $ k 6= 0 } is finite . Thus , the mixture distribution in ( 1 ) is a finite mixture . Given x ∈ Rd , ρT ( x ) is a function of the forward and backward orbit of T through x . For any nonnegative function f , the definition of ρT implies that ∫ f ( y ) ρT ( y ) dy = Ω−1 ∫ ∑ k∈Z $ kf ( T k ( x ) ) ρ ( x ) dx . Assuming that $ 0 > 0 , the ratio ρ ( x ) /ρT ( x ) ≤ $ −10 Ω < ∞ is bounded . We can therefore apply the IS principle which allows to write the identity∫ f ( x ) ρ ( x ) dx = ∫ ( f ( y ) ρ ( y ) ρT ( y ) ) ρT ( y ) dy = ∫ ∑ k∈Z f ( Tk ( x ) ) wk ( x ) ρ ( x ) dx , ( 2 ) where the weights are given by ( see Appendix A.2 for a detailed derivation ) , wk ( x ) = $ kρ ( T k ( x ) ) / { ΩρT ( Tk ( x ) ) } = $ kρ−k ( x ) /∑ i∈Z $ k+iρi ( x ) . ( 3 ) We assume in the sequel that $ 0 > 0 . In particular , note that under this condition , the weights wk are also upper bounded uniformly in x : for any x ∈ Rd , wk ( x ) ≤ $ k/ $ 0 . Equations ( 2 ) and ( 3 ) suggest to estimate the integral ∫ f ( x ) ρ ( x ) dx by INEO $ , N ( f ) = N −1∑N i=1 ∑ k∈Z wk ( X i ) f ( Tk ( Xi ) ) where { Xi } Ni=1 are i.i.d . samples from the proposal ρ , which is denoted by X1 : N iid∼ ρ. Algorithm 1 NEO-IS Sampler 1 . Sample X1 : N iid∼ ρ for i ∈ [ N ] . 2 . For i ∈ [ N ] , compute the path ( Tj ( Xi ) ) j∈Z and weights ( wj ( Xi ) ) j∈Z . 3 . INEO $ , N ( f ) = N −1∑N i=1 ∑ k∈Z wk ( X i ) f ( Tk ( Xi ) ) . This estimator is obtained by a weighted combination of the elements of the independent forward and backward orbits { Tk ( Xi ) } k∈Z with X1 : N iid∼ ρ . This estimator is referred to as NEO-IS . Choosing f ≡ L provides the NEO-IS estimator of the normalizing constant of π : Ẑ $ Xi = ∑ k∈Z L ( T k ( Xi ) ) wk ( X i ) , Ẑ $ X1 : N = N −1∑N i=1 Ẑ $ Xi . ( 4 ) We now study the performance of the NEO-IS estimator . The following two quantities play a fundamental role in the analysis : E $ T = EX∼ρ [ ( ∑ k∈Z wk ( X ) L ( T k ( X ) ) /Z ) 2 ] , M $ T = supx∈Rd ∑ k∈Z wk ( x ) L ( T k ( x ) ) /Z . ( 5 ) Theorem 1 . Ẑ $ X1 : N is an unbiased estimator of Z . If E $ T < ∞ , then , E [ |Ẑ $ X1 : N /Z−1|2 ] = N−1 ( E $ T − 1 ) . If M $ T < ∞ , then , for any δ ∈ ( 0 , 1 ) , with probability 1− δ , √ N ∣∣∣Ẑ $ X1 : N /Z−1∣∣∣ ≤ M $ T √ log ( 2/δ ) /2 . The ( elementary ) proof is postponed to Appendix A.3 . E $ T plays the role of the second-order moment of the importance weights EX∼ρ [ L2 ( X ) ] which is key to the performance of IS algorithms [ 1 , 2 ] . In addition , since the NEO-IS estimator Ẑ $ X1 : N is unbiased , the Cauchy–Schwarz inequality implies that EX∼ρ [ ( ∑ k∈Z wk ( X ) L ( T k ( X ) ) ) 2 ] ≥ Z2 and hence that E $ T ≥ 1 . Note that if ‖L‖∞ = supx∈Rd L ( x ) < ∞ , then since the weights are uniformly bounded by Ω $ −10 , we have M $ T ≤ ‖L‖∞Ω $ −1 0 /Z . Using the NEO-IS estimate Ẑ $ X1 : N of the normalizing constant , we can construct a self-normalized IS estimate of ∫ f ( x ) π ( x ) dx : JNEO $ , N ( f ) = N −1 N∑ i=1 Ẑ $ Xi Ẑ $ X1 : N ∑ k∈Z L ( Tk ( Xi ) ) wk ( X i ) Ẑ $ Xi f ( Tk ( Xi ) ) , ( 6 ) referred to as NEO-SNIS estimator . This expression may seem unnecessarily complicated but highlights the hierarchical structure of the estimator . We combine estimators ( Ẑ $ Xi ) −1∑ k∈Z L ( T k ( Xi ) ) wk ( X i ) f ( Tk ( Xi ) ) evaluated on the forward and backward orbits through the points { Xi } Ni=1 using the normalized weights { Ẑ $ Xi/Ẑ $ X1 : N } Ni=1 . Although the NEO-IS estimator is unbiased , the NEO-SNIS is in general biased . However , for bounded functions , both the bias and the variance of the NEO-SNIS estimator are O ( N−1 ) , with constants proportional to E $ T . For g a π-integrable function , we set π ( g ) = ∫ g ( x ) π ( x ) dx . Theorem 2 . Assume that E $ T < ∞ . Then , for any function g satisfying supx∈Rd |g ( x ) | ≤ 1 on Rd , and N ∈ N E X1 : N iid∼ρ [ |JNEO $ , N ( g ) − π ( g ) |2 ] ≤ 4 ·N−1E $ T , ( 7 ) ∣∣∣E X1 : N iid∼ρ [ JNEO $ , N ( g ) − π ( g ) ] ∣∣∣ ≤ 2 ·N−1E $ T . ( 8 ) If M $ T < ∞ , then for δ ∈ ( 0 , 1 ] , with probability at least 1− δ , √ N |JNEO $ , N ( g ) − π ( g ) | ≤ ‖g‖∞M $ T √ 32 log ( 4/δ ) . ( 9 ) The proof is postponed to Appendix A.4 . These results extend to NEO-SNIS estimators the results known for self-normalized IS estimators ; see e.g. , [ 1 , 2 ] and the references therein . The upper bounds stated in this result suggest it is good practice to keep E $ T /N small in order to obtain sensible approximations . For two pdfs p and q on Rd , denote by Dχ2 ( p , q ) = ∫ { p ( x ) /q ( x ) − 1 } 2q ( x ) dx the χ2-divergence between p and q. Lemma 3 . For any nonnegative sequence ( $ k ) k∈Z , we have E $ T ≤ Dχ2 ( π‖ρT ) + 1 . The proof is postponed to Appendix A.5 . Lemma 3 suggests that accurate sampling requires N to scale linearly with the χ2-divergence between the target π and the extended proposal ρT . Remark 1 . We can extend NEO to non homogeneous flows , replacing the family { Tk : k ∈ Z } with a collection of mappings { Tk : k ∈ Z } . This would allow us to consider further flexible classes of transformations such as normalizing flows ; see e.g . [ 21 ] . The χ2-divergence Dχ2 ( π‖ρT ) provides a natural criterion for learning the transformation . We leave this extension to future work . Conformal Hamiltonian transform The efficiency of NEO relies heavily on the choice of T. Intuitively , a sensible choice of T requires that ( i ) E $ T is small , i.e . ρT should be close to π by Lemma 3 ( see ( 5 ) ) , ( ii ) the inverse T−1 and the Jacobian of T are easy to compute . Following [ 23 ] , we use for T a discretization of a conformal Hamiltonian dynamics . Assume that U ( · ) = − log π ( · ) is continuously differentiable . We consider the augmented distribution π̃ ( q , p ) ∝ exp { −U ( q ) −K ( p ) } on R2d , where q is the position , p is the momentum , and K ( p ) = pTM−1p/2 is the kinetic energy , with M a positive definite mass matrix . By construction , the marginal distribution of the momentum under π̃ is the target pdf π ( q ) = ∫ π̃ ( q , p ) dp . The conformal Hamiltonian ODE associated with π̃ is defined by dqt/dt = ∇pH ( qt , pt ) = M−1pt , ( 10 ) dpt/dt = −∇qH ( qt , pt ) − γpt = −∇U ( qt ) − γpt , where H ( q , p ) = U ( q ) +K ( p ) , and γ > 0 is a damping constant . Any solution ( qt , pt ) t≥0 of ( 10 ) satisfies setting Ht = H ( qt , pt ) , dHt/dt = −γpTt M−1pt ≤ 0 . Hence , all orbits converge to fixed points that satisfy∇U ( q ) = 0 and p = 0 ; see e.g . [ 12 , 17 ] . In the applications below , we consider the conformal version of the symplectic Euler ( SE ) method of ( 10 ) , see [ 12 ] . This integrator can be constructed as a splitting of the two conformal and conservative parts of the system ( 10 ) . When composing a dissipative with a symplectic operator , we set for all ( q , p ) ∈ R2d , Th ( q , p ) = ( q + hM−1 { e−hγp − h∇U ( q ) } , e−hγp − h∇U ( q ) ) , where h > 0 is a discretization stepsize . This transformation can be connected with classical momentum optimization schemes , see [ 12 , Section 4 ] . By [ 12 , Section 3 ] , for any h > 0 Th is a C1-diffeomorphism on R2d with Jacobian given by JTh ( q , p ) = e−γhd . In addition , its inverse is T −1 h ( q , p ) = ( q − hM−1p , eγh { p+ h∇U ( q − hM−1p ) } ) . Therefore , the weight ( 3 ) of the NEO estimator is given by wk ( q , p ) = $ kρ̃ ( T k h ( q , p ) ) e −γkhd∑ j∈Z $ k+j ρ̃ ( T −j h ( q , p ) ) e γjhd , where ρ̃ ( q , p ) ∝ ρ ( q ) e−K ( p ) . Figure 1 displays for different values of γ on a log-scale the bound E 1 [ 0 : K ] Th − 1 appearing in Theorem 1 as a function of K , here we use the sequence of weights ( $ k ) k∈Z = ( 1 [ 0 : K ] ( k ) ) k∈Z ( i.e . only the K + 1 first elements of the forward orbits are used and are equally weighted ) . For comparison , we also present on the same plot the bounds achieved by averaging K + 1 independent IS estimates , EIS ( K ) − 1 = ( K + 1 ) −1EX∼ρ [ L ( X ) 2 ] . Interestingly , Figure 1 shows that there is a trade-off in the choice of γ which controls the exploration of the state space by the Hamiltonian dynamics since the higher γ , the faster the orbits converge towards the modes . This fast convergence prevents a “ good ” exploration of the space ; e.g . E 1 [ 0 : K ] Th is smaller for γ = 1.0 than for γ = 2.0 when K > 7 . 3 NEO-MCMC algorithm We now derive an MCMC method to sample from π based on the NEO-IS estimator . A natural idea consists in adapting the Sampling Importance Resampling procedure ( SIR ) ( see for example [ 24 , 27 ] ) to the NEO framework . The SIR method to sample JNEO $ , N ( see ( 6 ) ) consists of 4 steps . ( SIR-1 ) Draw independently X1 : N iid∼ ρ and compute the associated forward and backward orbits { Tk ( Xi ) } k∈Z of the point . ( SIR-2 ) Compute the normalizing constants associated with each orbit { Ẑ $ Xi } Ni=1 . ( SIR-3 ) Sample an orbit index IN ∈ [ N ] with probability { Ẑ $ Xi/ ∑N j=1 Ẑ $ Xj } Ni=1 . ( SIR-4 ) Draw the iteration index KN on the IN -th orbit with probability { L ( Tk ( XIN ) ) wk ( XI N ) /Ẑ $ XIN } k∈Z . The resulting draw is denoted by UN = TK N ( XI N ) . By construction , for any bounded function f , we get that E [ f ( UN ) ∣∣X1 : N , IN ] = { Ẑ $ XIN } −1∑k∈Z wk ( XIN ) L ( Tk ( XIN ) ) which implies E [ f ( UN ) ∣∣X1 : N ] = JNEO $ , N ( f ) ( see ( 6 ) ) . Using Theorem 2 , we therefore obtain |E [ f ( UN ) ] −∫ f ( z ) π ( z ) dz| ≤ 101/2‖f‖∞E $ T N−1 , showing that the law of the random variable µN = Law ( UN ) converges in total variation to π as N →∞ , ‖µN − π‖TV = sup ‖f‖∞≤1 |µN ( f ) − π ( f ) | ≤ 101/2E $ T N−1 . ( 11 ) Algorithm 2 NEO-MCMC Sampler At step n ∈ N∗ , given the conditioning orbit point Yn−1 . Step 1 : Update the conditioning point 1 . Set X1n = Yn−1 and for any i ∈ { 2 , . . . , N } , sample Xin iid∼ ρ . 2 . Sample the orbit index In with probability proportional to ( Ẑ $ Xin ) i∈ [ N ] , ( 4 ) . 3 . Set Yn = XInn . Step 2 : Output a sample 4 . Sample index Kn with probability proportional to { wk ( Yn ) L ( Tk ( Yn ) ) /Ẑ $ Yn } k∈Z 5 . Output Un = TKn ( Yn ) . Based on [ 3 ] , we now derive the NEOMCMC procedure , which in a nutshell consists in iterating the SIR procedure while keeping a conditioning point ( or equivalently , orbit ) ; see Appendix C. The convergence of NEO-MCMC does not rely on letting N →∞ : the NEO-MCMC works as soon as N ≥ 2 , although as we will see below the mixing time decreases as N increases . This procedure is summarized in Algorithm 2 . The NEO-MCMC procedure is an iterated algorithm which produces a sequence { ( Yn , Un ) } n∈N of points in Rd . The n-th iteration of the NEO-MCMC algorithm consists in two main steps : 1 ) updating the conditioning point Yn−1 → Yn 2 ) sampling Un by selecting a point in the orbit { Tk ( Yn ) } k∈Z of the conditioning point . Compared to SIR , only the generation of the points ( step ( SIR-1 ) ) is modified : we set X1n = Yn−1 ( the conditioning point ) , and then draw X2 : Nn iid∼ ρ . The sequence { Yn } n∈N defined by Algorithm 2 is a Markov chain : P ( Yn ∈ A |Y0 : n−1 ) = P ( Yn ∈ A |Yn−1 ) = P ( Yn , A ) where P ( y , A ) = ∫ δy ( dx 1 ) N∏ j=2 ρ ( xj ) dxj N∑ i=1 Ẑ $ xi∑N j=1 Ẑ $ xj 1A ( x i ) , y ∈ Rd , A ∈ B ( Rd ) . ( 12 ) Note that this Markov kernel describes the way , at stage n + 1 , the conditioning point Yn+1 is selected given Yn , which depends only on the estimator of the normalizing constants associated with each orbit , but not on the sample Un selected on the conditioning orbit . In addition , given the conditioning point Yn at the n-th iteration , the conditional distribution of the output sample Un is P ( Un ∈ B | In , X1 : Nn ) = P ( Un ∈ B |Yn ) = Q ( Yn , B ) where Q ( y , B ) = ∑ k∈Z wk ( y ) L ( T k ( y ) ) Ẑ $ y 1B ( T k ( y ) ) , y ∈ Rd , B ∈ B ( Rd ) . ( 13 ) With these notations , if the Markov chain is started at Y0 = y , then for any n ∈ N , the law of the n-th conditioning point is P ( Yn ∈ A |Y0 = y ) = Pn ( y , A ) and the law of the n-th sample is P ( Un ∈ B |Y0 ) = PnQ ( y , B ) . Define π̃ the pdf given for y ∈ Rd by π̃ ( y ) = ρ ( y ) Z ∑ k∈Z wk ( y ) L ( T k ( y ) ) = ρ ( y ) Ẑ $ y Z . ( 14 ) The following theorem shows that , for any initial condition y ∈ Rd , the distribution of the variable Yn converges in total variation to π̃ and that the distribution of Un converges to π. Theorem 4 . The Markov kernel P is reversible with respect to the distribution π̃ , ergodic and Harris positive , i.e. , for all y ∈ Rd , limn→∞ ‖Pn ( y , · ) − π̃‖TV = 0 . In addition , π = π̃Q and limn→∞ ‖PnQ ( y , · ) − π‖TV = 0 . Moreover , for any bounded function g and any y ∈ Rd , limn→∞ n −1∑n−1 i=0 g ( Ui ) = π ( g ) , P-almost surely , where { Ui } i∈N is defined in Algorithm 2 with Y0 = y . The proof is postponed to Appendix A.6 . Remark 2 . We may provide another sampling procedure of { Yn } n∈N . Define the pdf on the extended space [ N ] × RdN by π̌ ( i , x1 : N ) = N−1π̃ ( xi ) ∏N j=1 , j 6=i ρ ( x j ) . Consider a Gibbs sampler targeting π̌ consisting in ( a ) sampling X1 : N\ { In−1 } n | ( In−1 , Xn−1 ) ∼ ∏ j 6=In−1 ρ ( x j ) , ( b ) sampling In|X1 : Nn ∼ Cat ( { Ẑ $ Xin / ∑N j=1 Ẑ $ Xjn } Ni=1 and ( c ) set Yn = XInn . This algorithm is a Gibbs sampler on π̌ and we easily verify that the distribution of { Yn } n∈N is the same as Algorithm 2 . The next theorem provides non asymptotic quantitative bounds on the convergence in total variation . The main interest of NEO-MCMC algorithm is motivated empirically from observed behaviour : the mixing time of the corresponding Markov chain improves as N increases . This behaviour is quantified theoretically in the next theorem . Moreover , this improvement is obtained with little extra computational overhead , since sampling N points from the proposal distribution ρ , computing the forward and backward orbits of the points and evaluating the normalizing constants { Ẑ $ Xin } Ni=1 can be performed in parallel . Theorem 5 . Assume thatM $ T < ∞ , see ( 5 ) . Set N = ( N − 1 ) / ( 2M $ T +N − 2 ) and κN = 1− N . Then , for any y ∈ Rd and k ∈ N , ‖P k ( y , · ) − π̃‖TV ≤ κkN and ‖P kQ ( y , · ) − π‖TV ≤ κkN . Instead of sampling the new points X2 : Nn independently from ρ ( Step 1 in Algorithm 2 ) , it is possible to draw the proposals X1 : Nn conditional to the current point Yn−1 ; see [ 29 , 8 , 26 , 25 ] for related works . Following [ 25 ] , we use a reversible Markov kernel with respect to the proposal ρ , i.e. , such that ρ ( x ) m ( x , x′ ) = ρ ( x′ ) m ( x′ , x ) , assuming for simplicity that this kernel has density m ( x , x′ ) . If ρ = N ( 0 , σ2 Idd ) , an appropriate choice is an autoregressive kernel m ( x , x′ ) = N ( x′ ; αx , σ2 ( 1− α2 ) Idd ) . More generally , we can use a Metropolis–Hastings kernel with invariant distribution ρ . In this case , r1 ( x1 , x1 : N\ { 1 } ) = ∏N j=2m ( x j−1 , xj ) and for each i ∈ [ 2 : N ] , ri ( x i , x1 : N\ { i } ) = i−1∏ j=1 m ( xj+1 , xj ) N∏ j=i+1 m ( xj−1 , xj ) . ( 15 ) Since m is reversible with respect to ρ , for all i , j ∈ [ N ] , ρ ( xi ) ri ( xi , x1 : N\ { i } ) = ρ ( xj ) rj ( x j , x1 : N\ { j } ) where ri ( xi ; x1 : N\ { i } ) defines the the conditional distribution of X1 : N\ { i } given Xi = xi . The only modification in Algorithm 2 is Step 1 , which is replaced by : Draw Un ∈ [ N ] uniformly , set XUnn = Yn−1 and sample X 1 : N\ { Un } n ∼ rUn ( XUnn , · ) . The validity of this procedure is established in Appendix A.6 . 4 Continuous-time version of NEO and NEIS The NEO framework can be thought of as an extension of NEIS introduced in [ 23 ] . NEIS focuses on normalizing constant estimation and should be therefore compared with NEO-IS . In [ 23 ] , the authors do not consider possible extensions of these ideas to sampling problems . We consider here how NEO could be adapted to continuous-time dynamical system . Proofs of the statements and detailed technical conditions are postponed to Appendix B . Consider the Ordinary Differential Equation ( ODE ) ẋt = b ( xt ) , where b : Rd → Rd is a smooth vector field . Denote by ( φt ) t∈R the flow of this ODE ( assumed to be well-behaved ) . Under appropriate regularity condition Jφt ( x ) = exp ( ∫ t 0 ∇ · b ( φs ( x ) ) ds ) ; see Lemma S5 . Let $ : R → R+ be a nonnegative smooth function with finite support , with Ωc = ∫∞ −∞ $ ( t ) dt . The continuous-time counterpart of the proposal distribution ( 1 ) is ρcT ( x ) = ( Ω c ) −1 ∫∞ −∞ $ ( t ) ρ ( φ−t ( x ) ) Jφ−t ( x ) dt , which is a continuous mixture of the pushforward of the proposal ρ by the flow of ( φs ) s∈R . Assuming for simplicity that ρ ( x ) > 0 for all x ∈ Rd , then ρcT ( x ) > 0 for all x ∈ Rd , and using again the IS formula , for any nonnegative function f , ∫ f ( x ) ρ ( x ) dx = ∫ f ( x ) ρ ( x ) ρcT ( x ) ρcT ( x ) dx = ∫ [ ∫ ∞ −∞ wct ( x ) f ( φt ( x ) ) dt ] ρ ( x ) dx , ( 16 ) wct ( x ) = $ ( t ) ρ ( φt ( x ) ) Jφt ( x ) /∫ ∞ −∞ $ ( s+ t ) ρ ( φs ( x ) ) Jφs ( x ) ds . ( 17 ) These relations are the continuous-time counterparts of ( 2 ) . Eqs . ( 16 ) - ( 17 ) define a version of NEIS [ 23 ] , with a finite support weight function $ ; see Appendices B.2 and B.3 for weight functions with infinite support . This identity is of theoretical interest but must be discretized to obtain a computationally tractable estimator . For h > 0 , denote by Th an integrator with stepsize h > 0 of the ODE ẋ = b ( x ) . We may construct NEO-IS and NEO-SNIS estimators based on the transform T ← Th and weights $ k ← $ ( kh ) . We might show that for any bounded function f and for any x ∈ Rd , limh↓0 ∑ k∈Z wk ( x ) f ( T k h ( x ) ) = ∫∞ −∞ w c t ( x ) f ( φt ( x ) ) dt , where we omitted here the dependency in h of wk . Therefore , taking h ↓ 0+ , the NEO-IS converges to the continuous-version ( 16 ) - ( 17 ) . There is however an important difference between NEO and the NEIS method in [ 23 ] which stems from the way ( 16 ) - ( 17 ) are discretized . Compared to NEIS , NEO-IS using T← Th and weights $ k ← $ ( kh ) is unbiased for any stepsize h > 0 . NEIS uses an approach inspired by the nested-sampling approach , which amounts to discretizing the integral in ( 16 ) also in the state-variable x ; see [ 28 , 7 ] . This discretization is biased which prevents the use of this approach to develop MCMC sampling algorithm ; see Appendix B .","Sampling from a complex distribution π and approximating its intractable normalizing constant Z are challenging problems. In this paper, a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) samplers is derived. Given an invertible map T, these schemes combine (with weights) elements from the forward and backward Orbits through points sampled from a proposal distribution ρ. The map T does not leave the target π invariant, hence the name NEO, standing for Non-Equilibrium Orbits. NEO-IS provides unbiased estimators of the normalizing constant and self-normalized IS estimators of expectations under π while NEO-MCMC combines multiple NEO-IS estimates of the normalizing constant and an iterated sampling-importance resampling mechanism to sample from π. For T chosen as a discrete-time integrator of a conformal Hamiltonian system, NEO-IS achieves state-of-the art performance on difficult benchmarks and NEO-MCMC is able to explore highly multimodal targets. Additionally, we provide detailed theoretical results for both methods. In particular, we show that NEO-MCMC is uniformly geometrically ergodic and establish explicit mixing time estimates under mild conditions. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).","The paper introduce two sampling approaches named NEO-IS and NEO-MCMC respectively. The former method aims to estimate normalizing constant for an unnormalized target distribution, and the latter method can be executed to do sampling from unnormalized distribution. Empirical experiments reveal impressive performance for both the approaches on Normalizing Constant Estimation and Sampling tasks.  Exact theoretical analysis, especially convergence guarantees are also given in this paper. ","The paper proposes a new family of importance samplers and Markov chain Monte Carlo (MCMC) samplers, referred to as NEO-IS and NEO-MCMC, respectively, for approximating intractable normalizing constants and sampling from complex distributions. The schemes combine forward and backward orbits from a proposal distribution using an invertible map T, which is chosen as a discrete-time integrator of a conformal Hamiltonian system. The new methods achieve state-of-the-art performance on difficult benchmarks and provide detailed theoretical results.","In this paper, the authors propose a novel class of importance sampling (IS) estimators for computing normalizing constants, which are crucial in Bayesian inference and other applications. The proposed NEO-IS algorithm leverages a sequence of invertible transformations to generate samples from a reference distribution and estimate the normalizing constant. The authors show that under mild conditions, the NEO-IS estimator is unbiased and achieves a logarithmic convergence rate for the mean squared error, making it a promising method for estimating normalizing constants in challenging situations.","0,1,2",1,2,0,2,,coverage,coverage,"concise, coherence",coverage,1,2,0,2,Summary B has better coverage of topics (high-level description of the method).,Summary B does not include one of the methods (NEO-MCMC).,Summary A is concise and coherent. ,Summary C missed one of the method: NEO-MCMC. 
42,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"In this paper , we argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization . We find that the optimization process imposes a bias toward non-compositional solutions . This is caused by gradient descent , trying to use all available and redundant information from input , violating the conditional independence property of compositionality . Based on this finding , we suggest that compositionality learning approaches considering only model architecture design are unlikely to achieve complete compositionality . This is the first work to investigate the relation between compositional learning and gradient descent . We hope this study provides novel insights into compositional generalization , and forms a basis for new research directions to equip machine learning models with such skills for human-level intelligence . The source code is included in supplementary material . 1 INTRODUCTION . Compositional generalization is the algebraic capacity to understand and produce many novel combinations from known components ( Chomsky , 1957 ; Montague , 1970 ) , and it is a key element of human intelligence ( Minsky , 1986 ; Lake et al. , 2017 ) to recognize the world efficiently and create imagination . Broadly speaking , compositional generalization is a class of out-of-distribution generalization ( Bengio , 2017 ) , where the training and test distributions are different . A sample in such a setting is a combination of several components , and the generalization is enabled by recombining the seen components of the unseen combination during inference . For example , in the image domain , an object is a combination of multiple parts or properties . In the language domain , a sentence is a combination of syntax and semantics . Each component of an output depends only on the corresponding input component , but not on other variables . We call this the conditional independence property , and will formally introduce in Section 3 . People hope to design machine learning algorithms with compositional generalization skills . However , conventional neural network models generally lack such ability . There have been many attempts to equip models with compositionality ( Fodor & Pylyshyn , 1988 ; Bahdanau et al. , 2019 ) , and most efforts focus on designing neural network architectures ( Graves et al. , 2014 ; Andreas et al. , 2016 ; Henaff et al. , 2016 ; Shazeer et al. , 2017 ; Li et al. , 2018 ; Santoro et al. , 2018 ; Kirsch et al. , 2018 ; Rosenbaum et al. , 2019 ; Goyal et al. , 2019 ) . Recently , multiple approaches showed progress in specific tasks ( Li et al. , 2019 ; 2020 ; Lake , 2019 ; Russin et al. , 2019 ) , but we still do not know why standard approaches seldom achieve good compositionality in general . In this paper , we argue that there is a bias to prevent parameters from reaching compositional solutions , when we use gradient descent in optimization ( please see Figure 1 for illustrations ) . This is because gradient seeks the steepest direction , so that it uses all possible and redundant input information , which contradicts to the conditional independence property of compositionality . This problem is not due to how gradient is computed , such as back propagation , but caused by the essential property of gradient . We derive theoretical relation between gradient descent and compositionality with information theory . We also provide examples and visualization to show the detailed process of how gradient resists compositionality . Based on the finding , we propose that compositionality learning approaches with model structure design ( manual or searching ) alone are not likely to achieve complete compositionality . We hope this research provides new insights and forms a basis for new research directions in compositional generalization , and helps to improve machine intelligence towards human-level . The contributions of this paper can be summarized as follows . • The novelty of this work is to find the relation between compositional learning and gradient descent in optimization process , i.e. , gradient descent resists compositionality . • We theoretically derive the result and explain why standard approaches with architecture design alone do not address compositionality . 2 RELATED WORK . Compositionality Humans learn language and recognize the world in a flexible way by leveraging systematic compositionality . The compositional generalization is critical in human cognition ( Minsky , 1986 ; Lake et al. , 2017 ) , and it helps humans to connect limited amount of learned concepts for unseen combinations . Though deep learning has many achievements in recent years ( LeCun et al. , 2015 ; Krizhevsky et al. , 2012 ; Yu & Deng , 2012 ; He et al. , 2016 ; Wu & et al , 2016 ) , compositional generalization has not been well addressed ( Fodor & Pylyshyn , 1988 ; Marcus , 1998 ; Fodor & Lepore , 2002 ; Marcus , 2003 ; Calvo & Symons , 2014 ) . There are observations that current neural network models do not learn compositionality ( Bahdanau et al. , 2019 ) . Most recently , multiple approaches are proposed to address compositionality in neural networks ( Li et al. , 2019 ; 2020 ; Lake , 2019 ; Russin et al. , 2019 ) for specific tasks . However , we are still not sure why compositionality is hard to achieve in general cases , and this work discusses about this problem from optimization perspective . Another line of related work is independent disentangled representation learning ( Higgins et al. , 2017 ; Locatello et al. , 2019 ) . Its main assumption is that the expected components are statistically independent in training data . This setting does not have transferring problem in test , because all combinations have positive joint probabilities in training ( please refer to Section 3 ) . Compositionality is applied in different areas such as continual learning ( Jin et al. , 2020 ; Li et al. , 2020 ) , question answering ( Andreas et al. , 2016 ; Hudson & Manning , 2019 ; Keysers et al. , 2020 ) , and reasoning ( Talmor et al. , 2020 ) . Gradient Descent Gradient descent is a powerful and general purpose optimization tool for solving large scale problems in deep neural networks . It is usually used in a stochastic way ( Stochastic Gradient Descent ) with mini-batches , and has many variations such as Momentum ( Rumelhart et al. , 1986 ) , averaging ( Polyak & Juditsky , 1992 ) , AdaGrad ( Duchi et al. , 2011 ) , AdaDelta ( Zeiler , 2012 ) , RMSProp ( Tieleman & Hinton , 2012 ) , Adam ( Kingma & Ba , 2014 ) . Most of the previous work focus on faster reduction of loss and theoretical convergence analysis of SGD ( Bottou et al. , 2018 ; Luo , 1991 ; Reddi et al. , 2018 ; Chen et al. , 2018 ; Zhou et al. , 2018 ; Zou & Shen , 2018 ; De et al. , 2018 ; Zou et al. , 2018 ; Ward et al. , 2018 ; Barakat & Bianchi , 2019 ) . In particular , this work focuses on investigating why standard neural network training only achieves limited level of compositionality by studying the relationship between gradient descent and compositionality . 3 CONCEPTS FOR COMPOSITIONALITY AND GRADIENT DESCENT . We first formulate compositionality using the conditional independence property , and define compositional generalization . We then review properties of gradient for the derivation in the next section . Conditional Independence Property for Compositionality When multiple hidden variables live in the same representation , and can not be separated by simply splitting the representation , then these variables are entangled in the representation . For example , color and shape are two hidden variables and they share the same representation of image . Also , syntax and semantics are two hidden variables and they share the same representation of sentence . When we extract the hidden variables from their shared representation , we disentangle them . We then consider a prediction problem , where input X and output Y have multiple entangled components that are not labeled in data , and they are aligned . For example in machine translation , X1 is input syntax , and X2 is input semantics . Y1 is output syntax , and Y2 is output semantics . The syntax of output Y1 depends only on the syntax of input X1 , and the semantics of output Y2 depends only on the semantics of X2 . We can formalize the alignments as conditional independence property : Yi depends only on Xi . ∀i : P ( Yi|X1 , . . . , XK , Y1 , . . . , Yi−1 , Yi+1 , . . . , YK ) = P ( Yi|Xi ) . When a model fits this property , we say it has compositionality . Note that this can be understood as a kind of sparseness property ( Bengio , 2017 ) , because it restricts effective connection between input and output components . Compositional Generalization In compositional generalization , each sample in either training or test is a combination of several components . A test sample has a combination that does not appear in training , but each component of the test sample appears in training . We need to recombine the seen components to generalize to the test sample . We can define compositional generalization probabilistically as follows . In train , In test , ∀i : P ( Xi ) > 0 , P ( X1 , . . . , XK ) = 0 , P ( X1 , . . . , XK ) > 0 , ∀i : P ( Yi|Xi ) is high . P ( Y1 , . . . , YK |X1 , . . . , XK ) is predicted high . Compositionality bridges the gap between training and test distributions to achieve compositional generalization . We first apply chain rule , and then use compositionality as follows . P ( Y1 , . . . , YK |X1 , . . . , XK ) = K∏ i=1 P ( Yi|X1 , . . . , XK , Y1 , . . . , Yi−1 ) = K∏ i=1 P ( Yi|Xi ) . When P ( Yi|Xi ) are all high , their product should also be high . Therefore , a model with compositionality—satisfying this conditional independence property—addresses compositional generalization . Property of Gradient For a function f ( x1 , . . . , xK ) , the gradient∇f is the steepest direction to change the function ’ s value . Generally , gradient descent methods estimate ∇f using low-order local estimation . By definition , it is the vector of partial derivatives with respective to the inputs . ∇f = ∂f ∂x1 , . . . , ∂f ∂xK We will use the following definition in later arguments . Definition 1 ( Partial derivative ) . Partial derivative for an input is the derivative assuming other inputs are constant . ∀i = 1 , . . . , K : ∂f ( x1 , . . . , xK ) ∂xi = df ( c1 , . . . , xi , . . . , cK ) dxi 4 GRADIENT DESCENT RESISTS COMPOSITIONALITY . We focus on the early phase of training to show that gradient descent causes a model to use the redundant information to compute output when it has information to reduce the loss . We develop the arguments step by step . We first analyze the influence of the input on an output variable . We then consider the case of entangled inputs and one output . Finally , we discuss the case with entangled inputs and entangled outputs . The gradient is used to reduce loss , so we aim to relate loss reduction and the influence from input to output . To do that , we use mutual information to describe the influence , and use knowledge from information theory ( Definition 2 , Theorem 1 and Theorem 2 ) . We also study the impact of gradient descent to the influence , so we relate mutual information with gradient with Proposition 1 . Definition 2 ( Markov chain ( Cover , 1999 ) p.34 ) . Random variables X , Y , Z are said to form a Markov chain in that order ( denoted by X → Y → Z ) if the conditional distribution of Z depends only on Y and is conditionally independent of X . Theorem 1 ( Data-processing inequality ( Cover , 1999 ) p.34 ) . If X → Y → Z , then I ( X ; Y ) ≥ I ( X ; Z ) . Theorem 2 ( Chain rule for information ( Cover , 1999 ) p.24 ) . For random variables X , Y , Z , I ( X , Y ; Z ) = I ( Y ; Z|X ) + I ( X ; Z ) . Proposition 1 . When ∂Y∂X is defined , I ( X ; Y ) > 0 ⇐⇒ ∂Y ∂X 6= 0 Proof . I ( X ; Y ) > 0 means X and Y are not independent , which means Y is not invariant to X .","In this paper, we argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization. We find that the optimization process imposes a bias toward non-compositional solutions. This is caused by gradient descent, trying to use all available and redundant information from input, violating the conditional independence property of compositionality. Based on this finding, we suggest that compositionality learning approaches considering only model architecture design are unlikely to achieve complete compositionality. This is the first work to investigate the relation between compositional learning and gradient descent. We hope this study provides novel insights into compositional generalization, and forms a basis for new research directions to equip machine learning models with such skills for human-level intelligence. The source code is included in supplementary material.","This paper addresses the effects of gradient descent methods onto compositionality and compositional generalization of models. The authors claim that the optimization process imposes the models to deviate compositionality, which is defined with conditional independence among random variables of input, predicted output and the ground-truth. Since compositionality is one of important features of human intelligence, it has been interested widely in the field of AI/ML such as vision, language, neuro-symbolic approaches, common sense reasoning, disentangled representation, and the emergence conditions of compositionality. As it has been not much focused on the relationship with optimizers, it is fresh and interesting. However, it is not easy to figure out the position of this paper from two reasons: (1) the definitions on compositionality in this paper are not so compatible with recent related works, which mostly consider certain structures in models [ICLR19, JAIR20] or representative problems such as visual reasoning [CVPR17] and Raven progressive matrices [PNAS17]. (2) The authors do not consider quantitative approaches such as compositionality [ICLR19] or compositional generalization [ICLR20]. ","The article explores the relationship between gradient descent and compositional learning in neural network optimization. The authors argue that the optimization process creates a bias towards non-compositional solutions due to gradient descent. Gradient descent seeks to use all available and redundant information from input, violating the conditional independence property of compositionality. The article provides examples and visualizations to show the detailed process of how gradient descent resists compositionality. The authors suggest that compositionality learning approaches considering only model architecture design are unlikely to achieve complete compositionality. They derive a theoretical relation between gradient descent and compositionality using information theory and propose new research directions in compositional generalization. Overall, the article provides novel insights into compositional generalization and forms a basis for equipping machine learning models with such skills for human-level intelligence.","This paper investigates the relationship between gradient descent and compositionality in deep learning models. The authors find that gradient descent imposes a bias towards non-compositional solutions, which contradicts the conditional independence property of compositionality. They theoretically derive the result and explain why standard approaches with architecture design alone do not address compositionality. The paper proposes that compositionality learning approaches with model structure design alone are unlikely to achieve complete compositionality. The authors provide examples and visualization to show the detailed process of how gradient resists compositionality and discuss the limitations of current neural network models in addressing compositionality. The study provides novel insights into improving machine intelligence towards human-level intelligence by equipping models with compositional generalization skills.","0,1,2",,,,,,,,,,,,,,,,,
43,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"1 INTRODUCTION . Deep Neural Networks have achieved state-of-the-art performances in many machine learning tasks such as such as speech recognition ( Collobert et al. , 2011 ) , machine translation ( Bahdanau et al. , 2014 ) , object detection ( Ren et al. , 2015 ) and image classification ( Krizhevsky et al. , 2012 ) . However , excellent performance comes at the cost of significantly high computational and memory complexity , typically requiring teraops of computation during inference and Gigabytes of storage . To overcome these complexities , compression methods have been utilized , aiming to exploit the inherent resilience of DNNs to noise . These engender representations which maintain algorithm performance but significantly improve latency , throughput and power consumption of hardware implementations . In particular , exploiting reduced numerical precision for data representations through quantization has been emphatically promising , whereby on customizable hardware , efficiency scales quadratically with each bit of precision . Quantization of fixed-point arithmetic ( Q-FX ) for DNN inference has been extensively studied , and more recently there has been increasing interest in quantized floating point ( Q-FP ) arithmetic for both DNN inference and training ( Wang et al. , 2018 ) . Q-FP has the advantage of higher dynamic range compared to equivalent Q-FX representations and reduced hardware cost over single-precision floating point ( FP ) . This has influenced application specific integrated circuits ( ASICs ) such as Google ’ s tensor processing unit ( TPU ) , which supports 16-bit floating point ( 16-FP ) and soft processors such as Microsoft ’ s Project Brainwave which utilizes 8-FP . To illustrate these hardware benefits , we synthesized arithmetic logic units ( ALUs ) in different formats and different precision on an FPGA and present performance estimates in operations per second ( OPs ) and area estimates in Look-up Tables ( LUTs ) per operation ( LUTs/Op ) in Figure 1 . As shown , 8-bit fixed point ( 8-FX ) achieves improved performance and area over 8-FP . However , 7-FP is a significant improvement over 8 or 12-FX and 8 or 9-FP . These examples demonstrate substantial performance and area benefits from reducing FP precision by only 1 to 2-bits . Thus , if we 1Source code will be available if the paper is accepted can design networks which not only achieve high accuracy but are robust to quantization , higher performing hardware solutions are possible . Since in Q-FP , we are trying to represent the infinite set of real numbers using a finite number of bits , quantization and rounding artefacts will be introduced , with inaccuracies being cascaded along the computation graph ( IEEE , 1985 ; Goldberg , 1991 ; Higham , 2002 ) . This paper proposes Monte Carlo Deep Neural Network Arithmetic ( MCDA ) , a novel way to apply Monte Carlo Arithmetic ( MCA ) ( Parker et al. , 2000 ) for determining the sensitivity of Deep Neural Networks to Q-FP representations . It allows hardware-software designers to quantify the impact of quantization , enabling more efficient systems to be discovered . We do this by exploiting Monte Carlo simulations under which rounding effects are randomized . This , in turn , allows one to infer the sensitivity of executing a computation graph to quantization effects . Our MCDA technique is highly sensitive , allowing very small differences in quantization behaviour to be detected . The technique makes no assumptions regarding data distributions and directly measures the effects of quantization on the problem under study . This allows us to provide insights into the precision requirements of any inference network for any given dataset . Additionally we can use the technique to select weight parameters which are more robust to floating point rounding . The theoretical and practical contributions of this work can be summarized as follows : • We introduce a novel and rigorous analysis technique , Monte Carlo Deep Neural Network Arithmetic ( MCDA ) , which measures the sensitivity of Deep Neural Network inference computation to floating point rounding error . • When applied to neural network inference , we show MCDA can determine the precision requirements of different networks , rank them , and detect small differences between different neural network topologies and weight sets . • We demonstrate that while a network with the same topology but different weights may have the same loss and validation accuracy , their sensitivity to quantization can be vastly different . Using the CIFAR-10 and ImageNet datasets , we introduce a method to choose weights which are more robust to rounding error , resulting in a greatly improved accuracyarea tradeoff over state-of-the-art methods . It is worth noting that although we consider convolutional neural networks for image classification , this method could be applied for any neural network model architectures and applications . Moreover , while the experiments in this paper are limited to inference , it may be possible to apply the same idea to analyze training algorithms . 2 RELATED WORK . Low-precision representations of deep learning have been extensively studied . Many training methods have been developed to design representations for fixed point inference ( Jacob et al. , 2018 ; Faraone et al. , 2018 ; Zhou et al. , 2016 ) and training ( Wu et al. , 2018b ; Yang et al. , 2019 ; Gupta et al. , 2015 ; Sakr & Shanbhag , 2019 ) . Other methods have also utilized Q-FP arithmetic for inference and training whilst maintaining single-precision accuracy . ( Micikevicius et al. , 2018 ) implemented 16- FP arithmetic training whilst storing a 32-FP master copy for the weight updates . Additionally , ( Wang et al. , 2018 ; Mellempudi et al. , 2019 ) with 8-FP arithmetic whilst using a 16-FP copy of the weights and 16/32-bits for the accumulator . Techniques for determining per-layer sensitivity to quantization have also been studied ( Choi et al. , 2016 ; Sakr & Shanbhag , 2018 ) . Further , other studies have successfully determined the minimum fixed point precision requirements for a given DNN accuracy threshold ( Sakr et al. , 2017 ) . The accuracy and stability of various numerical algorithms in finite precision arithmetic has been studied in ( Higham , 2002 ; Wilkinson , 1994 ) . This has led to techniques for tracking information lost from finite precision arithmetic using random perturbation such as Monte Carlo Arithmetic ( Parker et al. , 2000 ; Frechtling & Leong , 2015 ) . Monte Carlo methods have also been used in Bayesian Neural Networks ( Buchholz et al. , 2018 ; Blier & Ollivier , 2018 ) . In particular , ( Achterhold et al. , 2018 ) introduced a quantizing prior to learn weights which are either close to a quantized representation or have high variance . ( Louizos et al. , 2017 ) used hierarchical priors to prune nodes and posterior uncertainties to determine the optimal fixed point precision . ( Blundell et al. , 2015 ) use a Monte Carlo approach to learn a probability distribution on the weights of a neural network . To the best of our knowledge , our work is the first to present a technique for directly determining the sensitivity of DNNs to floating point rounding and to explicitly compute precision bounds of a trained network . These ideas can be very usefully applied to extending the limits of low-precision representations in deep learning applications . 3 BACKGROUND . In this section , we describe background theory upon which our technique for determining the sensitivity of DNNs to floating point rounding is based . 3.1 FLOATING POINT ARITHMETIC . The IEEE-754 binary floating point format ( IEEE , 1985 ) represents most real numbers x by a subset in normal form as : x̂ = ( −1 ) sx ( 1 +mx ) 2ex ( 1 ) where sx ∈ { 0 , 1 } is the sign bit , ex is an integer representing the exponent of x̂ and mx is the mantissa of x̂ . Such number formats can be described as a ( sx , ex , mx ) tuple . In binary form the representation is ( bs , be1 , b e 2 , ... , b e Bex , bm1 , b m 2 ... , b m Bmx ) ∈ { 0 , 1 } B , with Bex and Bmx being the number of exponent and mantissa bits , respectively . The infinite set of real numbers R is represented in a computer withB = 1+Bex+Bmx bits , and we define the finite set of real numbers representable in floating point format as exact values , F ⊂ R. Real numbers which aren ’ t representable are rounded to their nearest exact value . We call this set of numbers inexact values , I , where I∪F = R. The approximation x̂ = F ( x ) = x ( 1 + δ ) , given x ∈ I , introduces rounding error into the computation . The value of δ = ∥∥x−x̂ x ∥∥ , represents the relative error which is a function of the machine hardware precision , p , as δ ≤ , where = 2−p ( IEEE , 1985 ; Goldberg , 1991 ; Higham , 2002 ) . In general , inexactness can be caused by finite representations or errors propagating from earlier parts of the computation . Often the primary cause of error in floating point arithmetic is catastrophic cancellation which causes numerical inaccuracy . Catastrophic cancellation occurs when for example , two near equal FP numbers , sharing k significant digits , are subtracted from one another as shown in ( 2 ) ( Higham , 2002 ) . 0. f1 f2 ... fk f ( k+1 ) ... ft − 0. f1 f2 ... fk g ( k+1 ) ... gt = 0 . 0 0 .... 0 h ( k+1 ) ... ht ( 2 ) 0. f1 f2 ... fk f ( k+1 ) ... ft r ( t+1 ) ... rp − 0. f1 f2 ... fk g ( k+1 ) ... gt r̂ ( t+1 ) ... r̂p = 0 . 0 0 .... 0 h ( k+1 ) ... ht i ( t+1 ) ... ip ( 3 ) In normalized form , the leading zeros are removed by shifting the result to the left and adjusting the exponent accordingly . The result is 0.hk+1 ... hti1 ... ik which has only ( t − k ) accurate digits and digits i which are unknown . Additionally , the remaining accurate digits h are most likely affected by rounding error in previous computations . This can significantly magnify errors , especially in computing large computational graphs such as that of state-of-the-art DNNs . If either operand in ( 2 ) is inexact , then the digits h are no more significant than any other sequence of digits . Yet , FP arithmetic has no mechanism of recording this loss of significance . By padding both our operands with random digits r and r̂ in ( 3 ) , the resulting digits i are randomized . If k digits are lost in the result , then k random digits will be in the normalized result and when computed over many random trials , the results will disagree on the trailing k digits . In this case , we are able to detect catastrophic cancellation because the randomization over many trials provides a statistical simulation of round-off errors . We can use techniques from numerical analysis such as Monte Carlo methods to appropriately insert precision-dependant randomization in this way . 3.2 MONTE CARLO ARITHMETIC . Monte Carlo methods can be used to analyze rounding by representing inexact values as random variables ( Parker et al. , 2000 ; Frechtling & Leong , 2015 ) . The real value x , as represented in ( 1 ) , can be modelled to t digits , using : inexact ( x̂ , t , δ ) = x̂+ 2ex−tδ = ( −1 ) sx ( 1 +mx + 2−tδ ) 2ex ( 4 ) where δ ∈ U ( − 12 , 1 2 ) is a uniformly distributed random variable and t is a positive integer representing the virtual precision of concern . For the same input x̂ in ( 4 ) , we can run many Monte Carlo trials which will yield different values on each trial , where 0 < t < p so that the MCA can be run accurately on a computer with machine precision p. The ability to vary t is useful because it allows us to then evaluate the hardware precision requirements of a given system or computational graph for a given DNN . MCA is a method to model the effect of rounding on a computational graph by randomizing all arithmetic operations . The randomization is applied for both generating inexact operands and also in rounding . In each operation using MCA , ideally both catastrophic cancellation and rounding error can be detected . An operation using MCA is defined as : x ◦ y = round ( inexact ( inexact ( x ) ◦ inexact ( y ) ) ) ( 5 ) where ◦ ∈ ( + , − , × , ÷ ) . By applying the inexact function to both operators we make it possible to detect catastrophic cancellation . Furthermore , applying the inexact function to the operation output and then rounding this value implements random rounding and hence is used to detect rounding error ( Parker & Langley , 1997 ) . Hence , for the same input into the system , each trial will yield different operands and output . After modifying the inexact and rounding operations as described , we use random sampling to simulate Monte Carlo trials . For each trial , we collect data on the resulting output of the system and compute summary statistics to quantify its behaviour ( Parker et al. , 2000 ) . With sufficiently large number of Monte Carlo trials and virtual precision t , the expected value of the output from these trials will equal the value from using real arithmetic . As explained in the next section , we can determine the total number of digits lost to rounding error and the minimum precision required to avoid a total loss of significance .","Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy. We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCDA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic. We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss. The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR-10 and ImageNet datasets. For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials. Additionally, we can apply MCDA to compare the sensitivity of different network topologies to quantization effects.1","Deep neural networks have been widely used in various applications, but their high computational complexity and memory requirements limit their deployment on resource-constrained devices. To address this challenge, researchers have proposed various low-precision representations of deep neural networks, including fixed-point arithmetic and quantized floating-point arithmetic. However, these representations can introduce rounding errors that significantly affect the accuracy of the network, and it is crucial to quantify the sensitivity of the network to these errors to achieve optimal hardware design.","The article discusses the high computational and memory complexity required by deep neural networks (DNNs) for tasks such as speech recognition and image classification, and suggests compression methods to address these complexities. The article then proposes a novel method called Monte Carlo Deep Neural Network Arithmetic (MCDA) which uses Monte Carlo simulations to determine the sensitivity of DNNs to floating point rounding errors, allowing for more efficient systems to be discovered. The authors demonstrate the effectiveness of their technique on image classification tasks using CIFAR-10 and ImageNet datasets.",The paper studies the sensitivity of a neural network with respect to quantizing its weights and activations. The idea is to use Monte Carlo Arithmetic (MCA) in order to calculate the number of significant bits in the training loss (e.g. cross entropy) that are lost due to floating-point arithmetic. The results show that the number of significant bits lost correlates with the reduction in classification accuracy when quantizing the weights and activations of the neural network.,"2,1,0",1,2,1,2,,coverage,coverage,"coverage, factuality",coverage,1,0,1,0,Summary B has better coverage.,Summary A includes only background and not the paper contributions.,Summary B best present the motivation of the paper and includes the main algorithm correctly. ,Summary A miss the main part of the work. It only included some information about the background. 
44,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"1 INTRODUCTION . Time series data in the real world is high dimensional , unstructured , and complex with unique properties , leading to challenges for data modeling ( Yang & Wu , 2006 ) . In addition , without human recognizable patterns , it is much harder to label time series data than images and languages in realworld applications . These labeling limitations hinder deep learning methods , which typically require a huge amount of labeled data for training , been applied on time series data ( Eldele et al. , 2021 ) . Representation learning learns a fixed-dimension embedding from the original time series that keeps their inherent features . Comparing to the raw time series data , these representations are with better transferability and generalization capacity . To deal with labeling limitations , contrastive learning methods have been widely adopted in various domains for their soaring performance on representation learning , including vision , language , and graph-structured data ( Chen et al. , 2020 ; Xie et al. , 2019 ; You et al. , 2020 ) . In a nutshell , contrastive learning methods typically train an encoder to map instances to an embedding space where dissimilar ( negative ) instances are easily distinguishable from similar ( positive ) ones and model predictions to be invariant to small noise applied to either input examples or hidden states . Despite being effective and prevalent , contrastive learning has been less explored in the time series domain ( Eldele et al. , 2021 ; Franceschi et al. , 2019 ; Fan et al. , 2020 ; Tonekaboni et al. , 2021 ) . Existing contrastive learning approaches often involve a specific data augmentation strategy that creates novel and realistic-looking training data without changing its label to construct positive alternatives for any input sample . Their success relies on carefully designed rules of thumb guided by domain expertise . Routinely used data augmentations for contrastive learning are mainly designed for image and language data , such as color distortion , flip , word replacement , and back-translation ( Chen et al. , 2020 ; Luo et al. , 2021 ) . These augmentation techniques generally do not apply to time series data . Recently , some researchers propose augmentations for time series to enhance the size and quality of the training data ( Wen et al. , 2021 ) . For example , Eldele et al . ( 2021 ) and Fan et al . ( 2020 ) propose to adopt jittering , scaling , and permutation strategies to generate augmented instances . Franceschi et al . ( 2019 ) extracts subsequences for data augmentation . In spite of the current progress , existing methods have two main limitations . First , unlike images with human recognizable features , time series data are often associated with inexplicable underlying patterns . Strong augmentation such as permutation may ruin such patterns and consequently , the model will mistake the negative handcrafts for positive ones . While weak augmentation methods such as jittering may generate augmented instances that are too similar to the raw inputs to be informative enough for contrastive learning . On the other hand , time series datasets from different domains may have diverse nature . Adapting a universal data augmentation method , such as subsequence in Xie et al . ( 2019 ) , for all datasets and tasks leads to sub-optimal performances . Other works follow empirical rules to select suitable augmentations from expensive trial-and-error . Akin to hand-crafting features , hand-picking choices of data augmentations are undesirable from the learning perspective . The diversity and heterogeneity of real-life time series data further hinder these methods away from wide applicability . To address the challenges , we first introduce the criteria for selecting good data augmentations in contrastive learning . Data augmentation benefits generalizable , transferable , and robust representation learning by correctly extrapolating the input training space to a larger region ( Wilk et al. , 2018 ) . The positive instances enclose a discriminative zone in which all the data points should be similar to the original instance . The desired data augmentations for contrastive representation learning should have both high fidelity and high variety . High fidelity encourages the augmented data to maintain the semantic identity that is invariant to transformations ( Wilk et al. , 2018 ) . For example , if the downstream task is classification , then the generated augmentations of inputs should be class-preserving . Meanwhile , generating augmented samples with high variety benefits representation learning by increasing the generalization capacity ( Chen et al. , 2020 ) . From the motivation , we theoretically analyze the information flows in data augmentations based upon information theory and derive the criteria for selecting desired time series augmentations . Due to the inexplicability in practical time series data , we assume that the semantic identity is presented by the target in the downstream task . Thus , high fidelity can be achieved by maximizing the mutual information between the downstream label and the augmented data . A one-hot pseudo label is assigned to each instance in the unsupervised setting when downstream labels are unavailable . These pseudo labels encourage augmentations of different instances to be distinguishable from each other . We show that data augmentations preserving these pseudo labels can add new information without decreasing the fidelity . Concurrently , we maximize the entropy of augmented data conditional on the original instances to increase the variety of data augmentations . Based on the derived criteria , we propose an adaptive data augmentation method , InfoTS ( as shown in Figure 1 ) , by employing a meta-learning mechanism to avoid ad-hoc choices or painstakingly trial-and-error tuning . Specifically , we utilize a meta-network to learn the augmentation prior in tandem with contrastive learning . The meta-learner automatically selects optimal augmentations from candidate augmentations to generate feasible positive samples . Along with random sampled negative instances , augmented instances are then fed into a time series encoder to learn representations in a contrastive manner . With a reparameterization trick , the meta-network can be efficiently optimized with back-propagation based upon the proposed criteria . Therefore , the meta-network can automatically select data augmentations in a per dataset and per learning task manner without resorting to expert knowledge or tedious downstream validation . Our main contributions include : • We propose criteria to guide the selection of data augmentations for contrastive time series representation learning without prefabricated knowledge . • We propose a meta-learning based method to automatically select feasible data augmentations for different time series datasets , which can be efficiently optimized with backpropagation . • We empirically verify the effectiveness of the proposed criteria to find optimal data augmentations . Extensive experiments demonstrate that InfoTS can achieve highly competitive performance with up to 11.4 % reduction in MSE on forecasting task and up to 2.8 % relative improvement in accuracy on classification task over the leading baselines . 2 METHODOLOGY . 2.1 NOTATIONS AND PROBLEM DEFINITION . A time series instance x has dimension T × F , where T is the length of sequence and F is the dimension of features . Given a set of time series instances X , we aim to learn an encoder fθ ( x ) that maps each instance x to a fixed-length vector z ∈ RD , where θ is the learnable parameters of the encoder network and D is the dimension of representation vectors . In semi-supervised settings , each instance x in the labelled set XL ⊆ X is associated with a label y for the downstream task . Specially , XL = X holds in the fully supervised setting . In the work , we use the Sans-serif style lowercase letters , such as x , to denote random time series variables and italic lowercase letters , such as x , for sampled instances . 2.2 WHAT ARE GOOD AUGMENTATIONS FOR CONTRASTIVE LEARNING ? . The goal of data augmentation for contrastive learning is to create realistically rational instances that maintain semantics through different transformation approaches . Unlike instances in vision and language domains , the underlying semantics of time series data is not recognizable to human , making it hard , if not impossible , to include human knowledge to data augmentation for time series data . For example , rotating an image will not change its content or the label . While permuting a time series instance may ruin its signal patterns and generates a meaningless time series instance . In addition , the tremendous heterogeneity of real-life time series datasets further makes selections based on trial-and-errors impractical . Although multiple data augmentation methods have been proposed for time series data , there is less discussion on what is a good augmentation that is meaningful for a given learning task and dataset without prefabricated human priors . From our perspective , ideal data augmentations for contrastive representation should keep high fidelity , high variety , and adaptive to different datasets . The illustration and examples are shown in Figure 2 . High Fidelity . Augmentations with high fidelity maintain the semantic identity that is invariant to transformations . Considering the inexplicability in practical time series data , it is challenging to visually check the fidelity of augmentations . Thus , we assume that the semantic identity of a time series instance is presented by its label in the downstream task , which might be either available or unavailable during the training period . Here , we start our analysis from the supervised case and will extend it to the unsupervised case later . Based on the information theory ( Tishby et al. , 2000 ) , we define the objective that keeps high fidelity as the large mutual information ( MI ) between augmentation v and the label y , i.e. , MI ( v ; y ) . We consider augmentation v as a probabilistic function of x and a random variable , that v = g ( x ; ) . From the definition of mutual information , we have MI ( v ; y ) = H ( y ) −H ( y|v ) , where H ( y ) is the ( Shannon ) entropy of y and H ( y|v ) is the entropy of y conditioned on augmentation v. Since H ( y ) is irrelevant to data augmentations , the objective is equivalent to minimizing the conditional entropy H ( y|v ) . Considering the efficient optimization , we follow Ying et al . ( 2019 ) and Luo et al . ( 2020 ) to approximate it with cross-entropy between y and ŷ , where ŷ is the prediction with augmentation v as the input and calculated via v = g ( x ; ) z = fθ ( v ) ŷ = hw ( z ) , ( 1 ) where z is the representation and hw ( · ) is a prediction projector parameterized by w. The prediction projector is optimized by the classification objective . Then , the objective of high fidelity for supervised or semi-supervised cases is to minimize CE ( y ; ŷ ) = − C∑ c=1 P ( y = c ) logP ( ŷ = c ) , ( 2 ) where C is the number of labels . In the unsupervised settings where y is unavailable , one-hot encoding ys ∈ R|X| is utilized as the pseudo label to replace y in Eq . ( 2 ) . The motivation is that augmented instances are still distinguishable from other instances with the classifier . We theoretically show that augmentations that preserving pseudo labels have the following properties . Property 1 ( Preserving Fidelity ) . If augmentation v preserves the one-hot encoding pseudo label , the mutual information between v and the downstream task label y ( although not visible to training ) is equivalent to that between raw input x and y , i.e. , MI ( v ; y ) = MI ( x ; y ) . Property 2 ( Adding New Information ) . By preserving the one-hot encoding pseudo label , augmentation v contains new information comparing to the raw input x , i.e. , H ( v ) ≥ H ( x ) . Detailed proofs are shown in the Appendix A . These properties show that in the unsupervised setting , preserving the one-hot encoding pseudo label guarantees that the generated augmentations will not decrease the fidelity , regardless of the downstream tasks and variances inherent in the augmentations . Concurrently , it may introduce new information for contrastive learning . Since the number of labels is equal to the number of instances in dataset X in an unsupervised case , direct optimization of Eq . ( 2 ) is inefficient and unscalable . Thus , we further relax it by approximating y with the batch-wise one-hot encoding yB , which decreases the number of labels C from the dataset size to the batch size . High Variety . Sufficient variances in augmentations improve the generalization capacity of contrastive learning models . In the information theory , the uncertainty inherent in the random variable ’ s possible outcomes is described by its entropy . Considering that augmented instances are generated based on the raw input x , we maximize the entropy of v conditioned on x , H ( v|x ) , to maintain a high variety of augmentations . From the definition of conditional entropy , we have H ( v|x ) = H ( v ) −MI ( v ; x ) . ( 3 ) We dismiss the first part since the unconstrained entropy of v can be dominated by meaningless noise . Considering the continuity of both v and x , we adopt a mutual information neural estimator , InfoNCE ( Oord et al. , 2018 ; Tian et al. , 2020 ) to approximately compute the mutual information for its practical effectiveness . Other MI estimators , such as Jensen-Shannon ( JSD ) estimator ( Nowozin et al. , 2016 ) , normalized temperature-scaled cross-entropy ( NT-Xent ) ( Chen et al. , 2020 ) , and leaveone-out ( Poole et al. , 2019 ) can also conveniently be the plug-and-play component in our framework . Then , the objective to encourage high variety is to maximize the InfoNCE between v and x : InfoNCE ( v ; x ) = −Ex [ log exp ( sim ( zx , zv ) ) ∑ x′∈X exp ( sim ( zx , zv′ ) ) ] , ( 4 ) where v′ is an augmented instance of input instance x′ . zx , zv , and zv′ are representations of instance x , v , and v′ respectively . sim ( z1 , z2 ) = zT1 z2 is the inner product of vectors z1 and z2 . Criteria . Combining the information aware definition of both high fidelity and variety , we propose the criteria for selecting good augmentations without prior knowledge , min v −InfoNCE ( v ; x ) + βCE ( y ; hw ( fθ ( v ) ) ) , ( 5 ) where β is a hyper-parameter to achieve the trade-off between fidelity and variety . Note that in the unsupervised settings , y is replaced by one-hot encoding pseudo label .. Relation to Information Bottleneck . Although the formation is similar to information bottleneck in data compression , minp ( e|x ) MI ( x ; e ) − βMI ( e ; y ) , our criteria are different in the following aspects . First , e in the information bottleneck is a representation of input x , while v in Eq . ( 5 ) represents the augmented instances . Second , information bottleneck aims to keep minimal and sufficient information for data compression , while our criteria are designed for data augmentations in contrastive learning . Third , in information bottleneck , the compressed representation e is a deterministic function of input x with no variances . MI ( e ; y ) and MI ( e ; x ) are constraint by MI ( x ; y ) and H ( x ) that MI ( e ; y ) ≤ MI ( x ; y ) and MI ( e ; x ) = H ( e ) , where H ( e ) is the entropy of e. In our criteria , v is a probabilistic function of input x . As a result , the variances of v makes the augmentation space much larger than the compression representation space in information bottleneck . Relation to InfoMin . InfoMin is designed based on the information bottleneck that good views should keep minimal and sufficient information from the original input ( Tian et al. , 2020 ) . Similar to the information bottleneck , InfoMin assumes that augmented views are functions of the input , which heavily constrains the variance of data augmentations . Besides , high fidelity property is dismissed in the unsupervised setting . It works for image datasets due to the availability of human knowledge . However , it may fail to generate reasonable augmentations for time series data .","Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where “desired” augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high fidelity and variety based upon information theory. A theoretical analysis leads to the criteria for selecting feasible data augmentations. On top of that, we employ the meta-learning mechanism and propose an information-aware approach, InfoTS, that adaptively selects optimal time series augmentations for contrastive representation learning. The meta-learner and the encoder are jointly optimized in an endto-end manner to avoid sub-optimal solutions. Experiments on various datasets show highly competitive performance with up to 11.4% reduction in MSE on forecasting task and up to 2.8% relative improvement in accuracy on classification task over the leading baselines.","Researchers have been facing challenges in applying contrastive learning to time series data due to the inherent inexplicability and diversity of real-life time series data. To address these challenges, the article proposes criteria for selecting good data augmentations in contrastive learning for time series data, which can be efficiently optimized using a meta-learning mechanism.","This paper proposes InfoTS, a method for learning augmentations that improve contrastive learning of time series data. The core contribution is a learnable augmentation strategy that uses Concrete/Gumbel-Softmax distributions. The paper shows empirical results on several time-series forecasting and classification benchmarks.",The article proposes a meta-learning method called InfoTS for data augmentation in contrastive time series representation learning and discusses the criteria for selecting good data augmentations. The proposed method achieves highly competitive performance with up to 11.4% reduction in MSE on forecasting task and up to 2.8% relative improvement in accuracy on classification task over the leading baselines.,"2,0,1",1,0,2,1,,coverage,coverage,coverage,coverage,2,1,0,2,Summary C has better coverage of information.,Summary B is too short and do not explicitly mention the meta-learning component.,"Summary A shows background, motivation, and what the paper introduces, as well as the results. It is the most comprehensive summary among all three. ","Summary C includes info that cannot be verified, and miss the background/motivation of the work. "
45,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"1 INTRODUCTION . Federated learning ( Konečný et al. , 2015 ) is a distributed learning paradigm that leverages data from remote participants and aggregates their knowledge without requiring their raw data to be transferred to a central server , thereby largely reducing the concerns from data security and privacy . FedAvg ( McMahan et al. , 2017 ) is among the most popular federated instantiations , which aggregates knowledge by averaging homogeneous model parameters . In many practical federated learning scenarios , the participants usually exhibit a variety of heterogeneity . The training datasets from participants can be non-i.i.d . and therefore lead to data heterogeneity , which has received extensive attention in the community ( Li et al. , 2020b ; Fallah et al. , 2020 ; Hong et al. , 2021 ; Li et al. , 2021 ) . In addition , the participants can also have resource heterogeneity , i.e. , participants have different hardware resources available , such as memory , computing power , and network bandwidth ( Ignatov et al. , 2018 ) . For example , in a learning task for face recognition , clients may use different types of devices ( e.g. , desktop computers , different generations of tablets or smartphones ) to participate in learning , which thereby leads to drastic differences in computational resources for training models , and in sizes of models that can be loaded to the memory and transferred through the network . The disparity of hardware resources results in varying budgets in both training and inference stages ( Xu et al. , 2021 ) . To accommodate different model sizes , one can turn to more flexible architectures trained by distillation from ensemble ( Lin et al. , 2020 ) , partial model averaging ( Diao et al. , 2021 ) , or directly combining predictions ( Shi et al. , 2021 ) . Yet besides heterogeneity , the deployment of models usually has a great amount of dynamics . One type of dynamics is directly related to the resources of participants ’ devices . For example , memory consumptions and computational capabilities may vary drastically between training and inference stages , and the same dynamics in hardware can also happen during inference ( Yu et al. , 2018 ; Xu et al. , 2021 ) , depending on how the resource allocation of the running programs is prioritized on a participant ’ s device . Another example of dynamics is the requirements of properties of the model , such as adversarial robustness . In autonomous driving , for instance , cloudy and rainy weather can ( a ) Illustration of FedAvg ( McMahan et al. , 2017 ) with a device-incompatible model and a heterogenousarchitecture variant ( HeteroFL ) ( Diao et al. , 2021 ) with under-trained wide models . ( b ) The proposed Split-Mix framework provides in-situ customization of widths and adversarial robustness to address heterogeneity and dynamics , enabling efficient training and inference . In this example , we use a subnet with 1/4 channels ( or widths ) per layer as a base model for model-width customization . For simplicity , we denote it ×0.25 net , and a ×1 net can be split into 4 ×0.25 base models . Figure 1 : Comparison of traditional and proposed methods . lower the quality of camera and other sensors , and therefore one would like to quickly switch to a more robust prediction model , as compared to the one used on sunny days . The dynamics along with the low latency requirements for the inference , altogether demand a model that can be customized in situ , meaning that we can instantly tailor our predictive model to a proper size that fits our available budget , and promptly adjust its robustness . Most existing federate learning models used a rather static model architecture that prevents such customization . A naive solution is to simultaneously train multiple models with wanted architectures and properties , which however may lead to prohibitive training cost and latency of switching models in a budget-limited device . Moreover , the trade-off between model adversarial robustness and generalization performance can hardly be reconcilable ( Tsipras et al. , 2019 ; Merchant et al. , 2020 ) , which further complicates the customization . To address the aforementioned challenges from heterogeneity and dynamics , we study a novel SplitMix federated learning approach for in-situ customization , which follows a communication-efficient paradigm and yields a model whose size and robustness can be efficiently customized , without additional training . Specifically , we first split the complete knowledge in a large model into several small base sub-networks ( shards ) according to model widths and robustness levels . To complete the knowledge , we let the base models be fully trained on all clients . To provide customized models , we mix selected base models to construct desired widths and required robustness . We illustrate the main idea in Fig . 1b . Overall , our contributions can be summarized in three folds : • Within the domain of heterogeneous federated learning , we are the first to study training a model with the capability of in-situ customization with heterogeneous local computation budgets . Illustrated in Fig . 1a , we show that conventional static-model solutions , such as FedAvg , can not provide customized models for dynamic client environments , and that its heterogeneous-architecture variant ( HeteroFL ) suffers from under-training in its large models due to local budget constraints . • To address the challenge , we propose a novel Split-Mix framework that aggregates knowledge from heterogeneous clients into a width- and robustness-adjustable model structure . Remarkably , due to fewer parameters and modular nature , our framework is not only efficient in federated communication and flexibly adaptable to various client budgets during training , but also efficient and flexible in storage , model loading and execution during inference . • Empirically , we demonstrate that the performance of the proposed method is better than other FL baselines under heterogeneous budget constraints . Moreover , we show its effectiveness when facing the challenge of data heterogeneity , i.e. , clients ’ training datasets suffer from distributional shifts . 2 RELATED WORK . Heterogeneous Federated Learning . Federated learning with budget-insufficient devices has attracted a great amount of attention . For example , FedDistill ( Lin et al. , 2020 ) used logit averages to distill a network of the same size or several prototypes , which will be communicated with users . FedDistill made an assumption that the central server has access to a public dataset of the same learning task , which is impractical for many applications . He et al . ( 2020 ) introduced a distillation-based method after aggregating private representations from all participants . The method closely resembles centralized learning because all encoded samples are gathered , and however it is less efficient when local clients have large data dimensions or sample sizes . Importantly , the method may not transfer adversarial robustness knowledge through the intermediate representations due to the decoupling of the input and prediction . On the other hand , HeteroFL ( Diao et al. , 2021 ) avoids distillation , allowing participants to train different prototypes and sharing parameters among prototypes to reduce redundant parameters . However , HeteroFL also reduces the samples available for training each prototype , which leads to degraded performance . FedEnsemble ( Shi et al. , 2021 ) is technically related to the proposed approach , which uses ensemble of diverse models to accommodate non-i.i.d . heterogeneity . The authors showed that combining multiple base models trained simultaneously in FL can outperform a single base model in testing . A critical difference between the proposed approach and FedEnsemble comes from the challenging problem setting of constrained heterogeneous computation budgets . For the first time , we show that base models can be trained adaptively under budget constraints and used to customize efficient inference networks that can outperform a single model of the same width but trained in a heterogeneous-architecture way . Customizable Models . To our best knowledge , this is the first paper discussing in-situ customization in federated learning and here we review similar concepts in central learning . First , customization of robustness and accuracy was discussed by Wang et al . ( 2020 ) , where an adjustable-conditional layer together with decoupled batch-normalization were used . The conditional layer enables continuous trade-off but also brings in over 50 % more parameters . In comparison , a simple weighted combination without additional parameters is used in our method and is very efficient in both communication and inference . In terms of model complexity , a series of work on dynamic neural networks were proposed to provide instant , adaptive and efficient trade-off between accuracy and latency ( mainly related to model complexity ) of neural networks at the inference stage . Typically , sub-path networks of different complexity ( Liu & Deng , 2018 ) or sub-depth networks ( Huang et al. , 2018 ; Wu et al. , 2019 ; Wang et al. , 2018 ; Zhang et al. , 2021 ) are trained together . However , due to the large memory footprint brought by a constant number of channels per layer , the memory footprint at inference is barely reduced . To address the challenge , slimmable neural network ( SNN ) ( Yu et al. , 2018 ) was proposed to train networks with adaptive layer widths . Distinct from SNN , we consider a more challenging scenario with distributed and non-sharable data and heterogeneous client capabilities . 3 PROBLEM SETTING . The goal of this work is to develop a federated learning ( FL ) strategy , that yields a global model , which can be instantly customized for dynamic needs . Formally , FL minimizes the objective 1∑ k |Dk| ∑K k=1 ∑ ( x , y ) ∈Dk L ( f ( x ; W ) , y ) with distributed data { Dk } K k=1 across K participants/clients , where L is loss function , e.g. , cross-entropy loss , f is a model parameterized by W , and x , y are a sample and a label , respectively . In this paper , we consider two kinds of customization : adjustable model sizes to trade off between execution complexity and accuracy , adjustable robustness for dynamic security sensitivity . Though other types of customization could be considered , for example , neural-architecture search or fairness , we believe the two types considered are quite representative for both architecture and non-architecture knowledge ( Wang et al. , 2020 ) . When training customizable/adjustable models , significant challengs arise from heterogeneity among clients . In this paper , we consider the following : 1 ) Heterogeneous computational budgets during training of clients constrain the maximal complexity of local models . The complexity of deep neural networks can be defined in multiple dimensions , like depths or widths ( Zagoruyko & Komodakis , 2017 ; Tan & Le , 2019 ) . In this paper , we consider the width of hidden channels , because it can significantly reduce not only the number of model parameters but also the layer output cache . Thus , we assume clients have confined width capabilities { Rk ∈ ( 0 , 1 ] } Kk=1 , defined by width ratios w.r.t . the original model , i.e. , ×Rk net as presented in Fig . 1b . Similar to FedAvg and HeteroFL , the same architecture is used by clients and therefore the model width can be tailored according to local budgets . 2 ) Imbalance computation budgets . In many applications , there are usually a significant number of devices with insufficient computational budgets ( Enge , 2021 ) . Without loss of generality , we assume exponentially distributed budgets in uniformly divided client groups : Rk = ( 1/2 ) ⌈4k/K⌉ , i.e. , the first group with 1/4 clients is capable for a 1-width and the rest are for 0.5 , 0.25 , 0.125-widths respectively . Such a budget distribution could result in additional challenges in the presence of data heterogeneity , e.g. , non-i.i.d . features , Dk ∼ Di for ordered domain i , since training a single model on multiple domains ( Li et al. , 2020b ) or models in a single domain ( due to budget constraints ) ( Hong et al. , 2021 ) are known to be suboptimal on average performance . Finally , following the standard FL setting ( McMahan et al. , 2017 ) , only model parameters can be shared to protect privacy . In summary , the problem setting has challenges from 1 ) resource heterogeneity , where disparity in computation budgets will significantly affect the training of large models because they can only be trained scarce budget-sufficient clients ; 2 ) data heterogeneity , where the under-trained large models may perform poorly on unseen in-domain or out-of-domain samples .","Federated learning (FL) provides a distributed learning framework for multiple participants to collaborate learning without sharing raw data. In many practical FL scenarios, participants have heterogeneous resources due to disparities in hardware and inference dynamics that require quickly loading models of different sizes and levels of robustness. The heterogeneity and dynamics together impose significant challenges to existing FL approaches and thus greatly limit FL’s applicability. In this paper, we propose a novel Split-Mix FL strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness. Specifically, we achieve customization by learning a set of base sub-networks of different sizes and robustness levels, which are later aggregated on-demand according to inference requirements. This split-mix strategy achieves customization with high efficiency in communication, storage, and inference. Extensive experiments demonstrate that our method provides better in-situ customization than the existing heterogeneous-architecture FL methods. Codes and pre-trained models are available: https://github.com/illidanlab/SplitMix.","In this article, the authors propose a novel federated learning approach called Split-Mix to address the challenges of heterogeneity and dynamics in federated learning. The proposed method enables in-situ customization of model widths and robustness, which can efficiently adapt to dynamic and non-i.i.d. data distributions across clients. Unlike traditional federated learning methods that suffer from under-training due to resource constraints, Split-Mix uses a modular architecture and adaptive training to ensure efficient training and inference. The authors demonstrate the effectiveness of Split-Mix through extensive experiments on several benchmark datasets, showing improved performance and adaptability compared to existing federated learning methods. The proposed approach has important implications for real-world applications where data and computational resources are often heterogeneous and subject to change.","This paper presents a new federated learning approach named Split-Mix FL that allows clients to train customized models efficiently while considering heterogeneity in data and computation resources. The key idea is threefold: 1) the global model is first split into several sub-networks called base models that each have different sizes, thus requiring a different amount of computational resources; 2) Each selected client trains a random subset of base models, under its computational resource constraints; 3) updated base models are aggregated at the server-side and distributed again. Furthermore, the proposed approach can train both accurate and robust models in a joint fashion, where all but batch-norm layers are shared for efficiency. Experimental results on multiple datasets (CIFAR10, Digits, DomainNet) demonstrate the effectiveness of the proposed approach compared to FedAvg and HeteroFL.","The article discusses the challenges of federated learning when participants exhibit heterogeneity and when models need to be dynamically customized to fit specific needs. The authors introduce a novel Split-Mix framework that allows for in-situ customization of model widths and robustness, while remaining efficient in both communication and inference. The approach involves splitting knowledge into small base sub-networks, fully training them on all clients, and then mixing them to construct custom models. The proposed method outperforms traditional federated learning baselines under heterogeneous budget constraints and is effective in addressing data heterogeneity. This work is the first study of training a model with the capability of in-situ customization in the context of federated learning with heterogeneous local computation budgets.","2,0,1",2,1,1,0,,"coverage, relevance (+ detail)",fluency,"coherence, coverage",coherence,0,2,2,1,"Summaries A and B are good, but A matches the abstract better in terms of level of detail (summary B has a lot of details).",Summary C seems slightly less fluent.,Summary C is more coherent and shows the motivation of the paper well. ,"Summary B includes some information that cannot be verified by the abstract, plus, the whole coherence of the summary is not as good as others. "
46,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"1 INTRODUCTION . Generative Adversarial Network ( GAN ) ( Goodfellow et al. , 2014 ) has attracted considerable attention and achieved great success in image generation . Conditional GAN ( cGAN ) ( Mirza & Osindero , 2014 ) is a type of GAN using class information to guide the training of the discriminator and generator so that it usually obtains a better generation effect . Most cGANs incorporate class information into the generator through Conditional Batch Normalization ( CBN ) ( de Vries et al. , 2017 ) , or into the discriminator through projection discriminator ( Miyato & Koyama , 2018 ) , multi-hinge loss ( Kavalerov et al. , 2019 ) , auxiliary loss ( Odena et al. , 2017 ) , etc . In this paper , we investigate the possibility of designing class-aware generators for cGAN ( i.e. , using a distinct generator network architecture for each class ) . To automatically design class-aware generators , we propose a neural architecture search ( NAS ) algorithm on top of reinforcement learning so that the generator architecture of each class is automatically designed . However , as the number of classes increases , there are three main issues we have to consider . First , the search space will grow exponentially as the number of categories grows ( i.e. , combinatorial explosion ) . Second , training the generator separately for each class is prone to insufficient data ( Karras et al. , 2020 ) . Furthermore , searching and re-training each generator one by one may be impractical when the number of generators is large . We propose solutions for these challenges . First , we present a carefully designed search space that is both flexible and safe . We refer to flexibility as the ability to assign a distinct generator architecture to each class , which makes the search space exponentially large while its size is still controllable . To guarantee the safety ( i.e. , enable the limited amount of training data to be shared among a large number of generators ) , we introduce a new operator named Class-Modulated convolution ( CMconv ) . CMconv shares the same set of convolutional weights with a regular convolution but is equipped with a standalone set of weights to modulate the convolutional weights , allowing the training data to be shared among different architectures and thus alleviating the inefficiency on training data . Second , to make the procedure of search and re-training as simple as possible , we develop mixed-architecture optimization , such that the training procedure of multiple class-aware generators is as simple as that of training only one generator . Integrating these modules produces the proposed Multi-Net NAS ( MN-NAS ) . To the best of our knowledge , this is the first method that can produce a number of generator architectures , one for each class , through one search procedure . Figure 1 shows the overall framework of MN-NAS . It applies a Markov decision process equipped with moving average as the top-level logic for sampling and evaluating candidate architectures . After the search procedure , the optimal architecture for each class is determined and they get re-trained and calibrated for better image generation performance . We perform experiments on some popular benchmarks , including the CIFAR10 and CIFAR100 datasets that have different numbers of classes . We achieve FID scores of 5.85 and 12.28 on CIFAR10 and CIFAR100 respectively , which are comparable to state-of-the-art results . In addition to achieving good performance , our method has given us some inspiration . For example , we find the phenomenon that the coordination between the discriminator and generator is very important ( i.e. , to derive distinct class-aware generators , the discriminator must also be class-aware ) . More interestingly , by analyzing the best model found by NAS , we find that the class-modulated convolution is more likely to appear in the early stage ( close to the input noise ) of the generator . We think this phenomenon is related to the semantic hierarchy of GANs ( Bau et al. , 2018 ; Yang et al. , 2020 ) . We apply this finding as an empirical rule to BigGAN ( Brock et al. , 2018 ) , and also observe performance gain . This implies that our algorithm delivers useful and generalized insights to the design of cGAN models . We will release code and pre-trained models to facilitate future research . 2 RELATED WORK . Generative Adversarial Network ( GAN ) ( Goodfellow et al. , 2014 ) have demonstrated impressive generation capabilities ( Karras et al. , 2017 ; Brock et al. , 2018 ; Karras et al. , 2019a ) . Nevertheless , it has notorious issues like vanishing gradient , training instability , and mode collapse . There are a number of improvements for the original GAN , e.g. , changing the objective function ( Arjovsky et al. , 2017 ; Gulrajani et al. , 2017 ; Mao et al. , 2016 ; Jolicoeur-Martineau , 2019 ; Qi , 2017 ) , improving network architecture ( Radford et al. , 2015 ; Brock et al. , 2018 ; Karras et al. , 2019a ; Denton et al. , 2015 ; Zhang et al. , 2018 ; Karnewar & Wang , 2019 ) , using multiple generators or discriminators ( Tolstikhin et al. , 2017 ; Hoang et al. , 2018 ; Arora et al. , 2017 ; Durugkar et al. , 2017 ; Ghosh et al. , 2018 ; Nguyen et al. , 2017 ) . Recently , the surge in neural architecture search ( NAS ) has triggered a wave of interest in automatically designing the network architecture of GAN ( Wang & Huan , 2019 ; Gong et al. , 2019 ; Tian et al. , 2020b ; Gao et al. , 2019 ; Tian et al. , 2020a ; Li et al. , 2020 ; Fu et al. , 2020 ; Kobayashi & Nagao , 2020 ) . Conditional GAN ( cGAN ) ( Mirza & Osindero , 2014 ) is another type of GAN that incorporates class information into the original GAN , so that achieving promising results for the class-sensitive image generation task . Most of the early methods just incorporated the class information by concatenation ( Mirza & Osindero , 2014 ; Reed et al. , 2016 ) . AC-GAN ( Odena et al. , 2017 ) incorporated the label information into the objective function of the discriminator by an auxiliary classifier . Miyato & Koyama ( 2018 ) proposed the class-projection ( cproj ) discriminator , which injected class information into the discriminator in a projection-based way . Furthermore , conditional batch normalization ( CBN ) ( de Vries et al. , 2017 ) is a very effective method to modulate convolutional feature maps by conditional information . Subsequently , cproj and CBN are widely used together , forming some powerful cGANs for class image generation ( Zhang et al. , 2018 ; Brock et al. , 2018 ) . 3 OUR APPROACH . We use NAS to design class-aware generators for cGAN . However , implementing this cGAN model is not a trivial task . First , it is not easy to define the search space and the generator of each class may suffer insufficient training data . To tackle these issues , we detail the search space and a weight sharing strategy in Section 3.1 . Second , we must design an efficient search method . In Section 3.2 , we introduce the Multi-Net NAS ( MN-NAS ) and the mixed-architecture optimization , these methods making the procedure of search and re-training of multiple networks simple . 3.1 SEARCH SPACE : SHARING DATA BY CLASS-MODULATED CONVOLUTION . The design of the search space follows the popular cell-based style . The input latent vector is passed through L up-sampling layers , each of which is followed by a cell that does not change the shape ( spatial resolution and channel number ) of the data . Each contains an input node , an output node , and N intermediate nodes and there exists an edge between each pair of nodes , propagating neural responses from the lower-indexed node to the higher-indexed node . Each node summarizes inputs from its precedents , i.e. , xj = ∑ i < j oi , j ( xi ) where oi , j ( · ) is the operator on edge ( i , j ) , chosen from the set of candidate operators , O . To guarantee that the shape of data is unchanged , at the beginning of each cell , the data is pre-processed using a 1 × 1 convolutional layer that shrinks the number of channels by a factor of N . Hence , the output of intermediate nodes , after being concatenated , recover the original data shape . An architecture with tentative operators is shown in Figure 2 . Since the operator used in each edge can be searched , the number of different architectures is |O|L× ( N+1 2 ) . Note that we allow each class to have a distinct architecture , therefore , if there are M classes in the dataset , the total number of possible combinations is |O|L× ( N+1 2 ) ×M . This is quite a large number . Even with the simplest setting used in this paper ( i.e. , |O| = 2 , L = 3 , N = 2 ) , this number is 290 ≈ 1.2× 1027 for a 10-class dataset ( e.g. , CIFAR10 ) or 2900 ≈ 8.5× 10270 for a 100-class dataset ( e.g. , CIFAR100 ) , much larger than some popular cell-based search spaces ( e.g. , the DARTS space ( Liu et al. , 2019 ; Chen et al. , 2019 ; Xu et al. , 2020 ) with 1.1×1018 architectures ) . Class-Modulated Convolution . The first challenge we encounter is that the training data need to be distributed among all classes . That being said , if the architectures of all classes are ‘ truly ’ independent ( e.g. , all network weights are individually optimized ) , the amount of training data for each architecture , compared to the scenario that all classes share the same architecture , is reduced by a factor of M . This can result in severe over-fitting and unsatisfying performance of image generation . To alleviate this risk , we share training data among different architectures by reusing model weights . In practice , most network weights are contributed by the convolutional layers , so we maintain one set of convolutional kernels and use a light-weighted module to introduce class-conditional information . Inspired by CBN ( de Vries et al. , 2017 ) and the ‘ demodulation ’ operation ( Karras et al. , 2019b ) , we propose the Class-Modulated convolution ( CMconv ) operator to incorporate classconditional information . As shown in Figure 3 , a CMconv layer consists of three parts , modulation , demodulation , and convolution . The CMconv shares convolutional weights with the corresponding regular convolution ( Rconv ) . Mathematically , let x denote the input features maps with a class label of y , and ω represent the weights of convolution . The goal of modulation is to introduce a scale factor to each of the input channels , i.e. , ω′ = ω sin where both ω and ω′ are in a shape of cin × cout × U . Here , cin and cout are the number of input and output channels , respectively , and U is the kernel size ( e.g. , 3× 3 ) ; sin is a cin-dimensional vector and ω sin multiplies each set of weights ( cout × U numbers ) by the corresponding entry in sin . We follow the conventional formulation of cGAN to define sin as an affine-transformed class vector , i.e. , sin = Aff ( ey ) , where Aff ( · ) is simply implemented as a trainable fully-connected layer and ey is a learnable embedding vector of the class label , y . The goal of demodulation is to normalize the weights and keep the variance of the input and output features same . We follow Karras et al . ( 2019b ) to use ω′′ = ω′ s−1out where sout is a cout-dimensional vector and s−1out indicates element-wise reciprocal ; sout is similar to the ` 2-norm of ω ′ , computed as sout = √∑ cin , u ( ω′cin , · , u ) 2 + , where is a small constant to avoid numerical instability . In summary , Rconv and CMconv start with the same weight , ω , and CMconv modulates ω into ω′′ . Then , regular convolution is performed , i.e. , conv ( x ; ω ) or conv ( x ; ω′′ ) . Since modulation and demodulation introduce relatively fewer parameters compared to convolution , so using weightsharing Rconv and CMconv operators in each edge is a safe option that enables the limited amount of training data to be shared among a large number of generators .","Conditional Generative Adversarial Networks (cGAN) were designed to generate images based on the provided conditions, e.g., class-level distributions. However, existing methods have used the same generating architecture for all classes. This paper presents a novel idea that adopts NAS to find a distinct architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data for each class generator. The search algorithm follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability. We evaluate our approach on CIFAR10 and CIFAR100. Besides achieving better image generation quality in terms of FID scores, we discover several insights that are helpful in designing cGAN models.","The Multi-Net Neural Architecture Search (MN-NAS) algorithm for Generative Adversarial Networks (GANs) has been developed by a team from the Chinese University of Hong Kong and Zhejiang University, China. It enables automatic design of a distinct generator network architecture by incorporating class information into the generator. The team used a carefully-designed search space for more flexibility and safety with a goal to find better search results, which was achieved by training the generator individually for each class. The team ran experiments on benchmarks including CIFAR10 and CIFAR100 datasets, achieving FID scores of 5.85 and 12.28 respectively, with possible recommendations for BigGAN design improvements through a close analysis of the algorithm.","This paper proposes a framework NAS-caGAN that adopts RL-based NAS to search the optimal class-aware generator architecture by directly optimizing the Inception Score (IS) using the  REINFORCE algorithm, and leverages the mixed-architecture optimization to mitigate the training data sparsity of each category. The authors design a Class-Modulated Convolution to allow for the weight-sharing among different searched architectures. The proposed NAS-caGAN outperforms the model that employs searched class-agnostic architecture on CIFAR 10 and achieves better results compared with cproj (Miyato & Koyama, 2018) on CIFAR 100. ","The article proposes a method for designing class-aware generators for Conditional GANs (cGANs) using a neural architecture search (NAS) algorithm. The proposed method, called Multi-Net NAS (MN-NAS), applies a Markov decision process equipped with moving average to search for the optimal architecture for each class. The search space is designed to be flexible and safe, and a new operator called Class-Modulated Convolution (CMconv) is introduced to share the training data among different architectures. The MN-NAS algorithm is evaluated on popular benchmarks, including CIFAR10 and CIFAR100, and achieves good performance compared to state-of-the-art results.","1,0,2",2,1,2,1,,"coverage, relevance (+ detail)",other,coherence,other,2,0,2,0,Summary C has good coverage without excessive details.,"Summary A is not bad but mentions the author's affiliation, which is odd in a summary.",Summary C is most coherent. ,Summary A includes too many facts that cannot be verified. 
47,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"Current knowledge distillation methods require full training data to distill knowledge from a large `` teacher '' network to a compact `` student '' network by matching certain statistics between `` teacher '' and `` student '' such as softmax outputs and feature responses . This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples . This paper proposes a novel and simple method for knowledge distillation from few samples . Taking the assumption that both `` teacher '' and `` student '' have the same feature map sizes at each corresponding block , we add a 1× 1 conv-layer at the end of each block in the student-net , and align the block-level outputs between `` teacher '' and `` student '' by estimating the parameters of the added layer with limited samples . We prove that the added layer can be absorbed/merged into the previous conv-layer to formulate a new conv-layer with the same size of parameters and computation cost as the previous one . Experiments verify that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets . 1 INTRODUCTION . Deep neural networks ( DNNs ) have demonstrated extraordinary success in a variety of fields such as computer vision ( Krizhevsky & Hinton , 2012 ; He et al. , 2016 ) , speech recognition ( Hinton et al. , 2012 ) , and natural language processing ( Mikolov et al. , 2010 ) . However , DNNs are resource-hungry which hinders their wide deployment to some resource-limited scenarios , especially low-power embedded devices in the emerging Internet-of-Things ( IoT ) domain . To address this limitation , extensive works have been done to accelerate or compress deep neural networks . Putting those works on designing ( Chollet , 2016 ) or automatically searching efficient network architecture aside ( Zoph & Le , 2016 ) , most studies try to optimize DNNs from four perspectives : network pruning ( Han et al. , 2016 ; Li et al. , 2016 ) , network decomposition ( Denton et al. , 2014 ; Jaderberg et al. , 2014 ) , network quantization ( or low-precision networks ) ( Gupta et al. , 2015 ; Courbariaux et al. , 2016 ; Rastegari et al. , 2016 ) and knowledge distillation ( Hinton et al. , 2015 ; Romero et al. , 2015 ) . Among these method categories , knowledge distillation is somewhat different due to the utilization of information from the pre-trained teacher-net . The concept was proposed by ( Bucila et al. , 2006 ; Ba & Caruana , 2014 ; Hinton et al. , 2015 ) for transferring knowledge from a large `` teacher '' model to a compact yet efficient `` student '' model by matching certain statistics between `` teacher '' and `` student '' . Further research introduced various kinds of matching mechanisms in the field of DNN optimization . The distillation procedure designs a loss function based on the matching mechanisms and enforces the loss during a full training process . Hence , all these methods usually require time-consuming training procedure along with fully annotated large-scale training dataset . Meanwhile , some network pruning ( Li et al. , 2016 ; Liu et al. , 2017 ) and decomposition ( Zhang et al. , 2016 ; Kim et al. , 2016 ) methods can produce extremely small networks , but with large accuracy drops so that time-consuming fine-tuning is required for possible accuracy recovery . Usually , it may still not be able to recover the accuracy drops with the original cross-entropy loss due to its low representation capacity . Hence , knowledge distillation may be used to alleviate the problem , since the compact student-net can sometimes be trained to match the performance of the teacher-net . For instance , Crowley et al . ( 2017 ) uses cheap group convolutions and pointwise convolutions to build a small student-net and adopts knowledge distillation to transfer knowledge from a full-sized `` teacher-net '' to the `` student-net '' . However , it still suffers from high training cost . As is known , children can learn knowledge concept from adults with few examples . This cognition phenomenon has motivated the development of the few-shot learning ( Fei-Fei et al. , 2006 ; Bart & Ullman , 2005 ) , which aims to learn information about object categories from a few training samples , and focuses more on image classification task . Nevertheless , it inspires people to consider the possibility of knowledge distillation from few samples . Some recent works on knowledge distillation address this problem by constructing `` pseudo '' training data ( Kimura et al. , 2018 ; Lopes et al. , 2017 ) with complicated heuristics and heavy engineering , which are still costly . This paper proposes a novel and simple three-step method for few-sample knowledge distillation ( FSKD ) as illustrated in Figure 1 , including student-net design , teacher-student alignment , and absorbing added conv-layer . We assume that both `` teacher '' and `` student '' nets have the same feature map sizes at each corresponding block . However , the relatively small student-net can be obtained in various ways , such as pruning/decomposing the teacher-net , and fully redesigned network with random initialization . We add a 1×1 conv-layer at the end of each block of the student-net and align the block-level outputs between `` teacher '' and `` student '' , which is done by estimating the parameters of the added layer with few samples using least square regression . Since the added 1×1 conv-layers have relatively few parameters , we can get a good approximation from a small number of samples . We further prove that the added 1×1 conv-layer can be absorbed/merged into the previous conv-layer when certain conditions fulfill , so that the new conv-layer has the same number of parameters and computation cost as the older/previous one . We argue that FSKD has many potential applications , especially when fine-tuning or full training is not feasible in practice . We just name a few such cases below . First , edge devices have limited computing resources , while FSKD offers the possibility of on-device learning to compress deep models with a limited number of samples . Second , FSKD may help software/hardware vendors optimizing the deep models from their customers when full training data is unavailable due to privacy or confidential issues . Third , FSKD enables fast model deployment optimization when there is a strict time budget . Our major contributions can be summarized as follows : ( 1 ) To the best of our knowledge , we are the first to show that knowledge distillation can be done with few samples within minutes on a desktop PC . ( 2 ) The proposed FSKD method is widely applicable not only for fully redesigned student-nets but also for compressed networks from pruning and decomposition-based methods . ( 3 ) We demonstrate significant performance improvement of the student-net by FSKD , comparing to existing distillation techniques on various datasets and network structures . 2 RELATED WORK . Knowledge Distillation ( KD ) transfers knowledge from a pre-trained large `` teacher '' network ( or even an ensemble of networks ) to a small `` student '' network , for facilitating the deployment at test time . Originally , this is done by regressing the softmax output of the teacher model ( Hinton et al. , 2015 ) . The soft continuous regression loss used here provides richer information than the label based loss , so that the distilled model can be more accurate than training on labeled data with cross-entropy loss . Later , various works have extended this approach by matching other statistics , including intermediate feature responses ( Romero et al. , 2015 ; Chen et al. , 2016 ) , gradient ( Srinivas & Fleuret , 2018 ) , distribution ( Huang & Wang , 2017 ) , Gram matrix ( Yim et al. , 2017 ) , etc . Deep mutual learning ( Zhang et al. , 2018 ) trains a cohort of student-nets and teaches each other collaboratively with model distillation throughout the training process . All these methods require a large amount of data ( known as the “ transfer set ” ) to transfer the knowledge , whereas we aim to provide a solution with a limited number of samples . We need emphasize that FSKD has a quite different philosophy on aligning intermediate responses to the closest knowledge distillation method FitNet ( Romero et al. , 2015 ) . FitNet re-trains the whole student-net with intermediate supervision using a larger amount of data , while FSKD only estimates parameters for the added 1× 1 conv-layer with few samples . We will verify in experiments that FSKD is not only more efficient but also more accurate than FitNet . Network Pruning methods obtain a small network by pruning weights from a trained larger network , which can keep the accuracy of the larger model if the prune ratio is set properly . Han et al . ( 2015 ) proposes to prune the individual weights that are near zero . Recently , channel pruning has become increasingly popular thanks to its better compatibility with off-the-shelf computing libraries , compared with weights pruning . Different criteria have been proposed to select the channel to be pruned , including norm of weights ( Li et al. , 2016 ) , scales of multiplicative coefficients ( Liu et al. , 2017 ) , statistics of next layer ( Luo et al. , 2017 ) , etc . It is usually required iterative loop between pruning and fine-tuning for achieving better pruning ratio and speedup . Similar gradually adjusting trick is also applied to train very-deep neural networks ( Smith et al. , 2016 ) . Meanwhile , Network Decomposition methods try to factorize heavy layers in DNNs into multiple lightweight ones . For instance , it may adopt low-rank decomposition to fully-connection layers ( Denton et al. , 2014 ) , and different kinds of tensor decomposition to conv-layers ( Zhang et al. , 2016 ; Kim et al. , 2016 ) . However , aggressive network pruning or network decomposition usually lead to large accuracy drops , thus fine-tuning is required to alleviate those drops ( Li et al. , 2016 ; Liu et al. , 2017 ; Zhang et al. , 2016 ) . As aforementioned , KD is more accurate than directly training on labeled data , it is of great interest to explore KD on extremely pruned or decomposed networks , especially under the few-sample setting . Learning with few samples has been extensively studied under the concept of one-shot or few-shot learning . One category of methods directly model few-shot samples with generative models ( Fei-Fei et al. , 2006 ; Lake et al. , 2011 ) , while most others study the problem under the notion of transfer learning ( Bart & Ullman , 2005 ; Ravi & Larochelle , 2017 ) . In the latter category , meta-learning methods ( Vinyals et al. , 2016 ; Finn et al. , 2017 ) solve the problem in a learning to learn fashion , which has been recently gaining momentum due to their application versatility . Most studies are devoted to the image classification task , while it is still less-explored for knowledge distillation from few samples . Recently , some works tried to address this problem . Kimura et al . ( 2018 ) constructs pseudo-examples using the inducing point method , and develops a complicated algorithm to optimize the model and pseudo-examples alternatively . Lopes et al . ( 2017 ) records per-layer meta-data for the teacher-net in order to reconstruct a training set , and then adopts a standard training procedure to obtain the student-net . Both are very costly due to the complicated and heavy training procedure . On the contrary , we aim for a simple solution for knowledge distillation from few samples .","Current knowledge distillation methods require full training data to distill knowledge from a large ""teacher"" network to a compact ""student"" network by matching certain statistics between ""teacher"" and ""student"" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both ""teacher"" and ""student"" have the same feature map sizes at each corresponding block, we add a 1× 1 conv-layer at the end of each block in the student-net, and align the block-level outputs between ""teacher"" and ""student"" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer to formulate a new conv-layer with the same size of parameters and computation cost as the previous one. Experiments verify that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.","This paper proposes a novel and simple method for knowledge distillation from few samples, called Few-Sample Knowledge Distillation (FSKD). The method is designed to transfer knowledge from a large ""teacher"" network to a smaller ""student"" network with only a few samples. The proposed method aligns the block-level outputs between the teacher and student networks, and estimates the parameters of the added 1x1 convolutional layer with few samples using least square regression. The added layer can be absorbed/merged into the previous convolutional layer when certain conditions are fulfilled, which reduces the computational cost and number of parameters. The proposed method has the potential to be widely applied, including on edge devices with limited computing resources, software/hardware vendors optimizing deep models from customers, and fast model deployment optimization in strict time budgets.","The paper proposes a new system for knowledge distillation from few samples. The system assumes both the teacher and student have the same feature map sizes at each corresponding block and adds a 1x1 conv-layer to the end of each block of the student-net. Next, the study aligns the block-level outputs between the teacher and student by estimating the parameters of the added layer with few samples using least square regression. Finally, the paper shows that the added 1x1 conv-layer can be absorbed or merged into the previous conv-layer when certain conditions are met. Experiments reveal that the proposed method is highly effective and efficient in distilling knowledge from teacher-net to student-net.","This paper proposes a framework for few-sample knowledge distillation of convolution neural networks. The basic idea is to fit the output of the student network and that of the teacher network layer-wisely. Such a regression problem is parameterized by a 1x1 point-wise convolution per layer (i.e. minimizing the fitting objective over the parameters of 1x1 convolutions). The author claims such an approach, called FSKD, is much more sample-efficient than previous works on knowledge distillation. Besides, it is also fast to finish the alignment procedure as the number of parameters is smaller than that in previous works. The sample efficiency is confirmed in the experiments on CIFAR-10, CIFAR-100 and ImageNet with various pruning techniques. In particular, FSKD outperforms the FitNet and fine-tuning by non-trivial margins if only small amount of samples are provided (e.g. 100).","2,1,0",0,1,2,1,,coverage,"concise, clarity",coverage,coverage,2,1,0,1,Summary C is comprehensive and better delimits the paper contribution (few-shot distillation for convolutional neural networks).,"Summary B is too concise, sacrificing the clarity in method description.",Summary A better represent the motivation of the paper and has least information that cannot be verified. ,Summary B has the least information that need to be included. 
48,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"1 INTRODUCTION . Encoding visual information from pixel space to a lower-dimensional vector is the core element of most modern deep learning-based solutions to computer vision . A rich set of algorithms and architectures have been developed to enable learning these encodings . A common practice in computer vision is to explicitly train the networks to map visual inputs to a curated label space . For example , a neural network is pre-trained using a large-scale annotated classification dataset ( Deng et al. , 2009 ; Krasin et al. , 2017 ) and the entire network or part of it is fine-tuned to a new target task ( Goyal et al. , 2019 ; Zamir et al. , 2018 ) . In recent years , weakly supervised and self-supervised representation learning approaches ( e.g. , Mahajan et al . ( 2018 ) ; He et al . ( 2020 ) ; Chen et al . ( 2020a ) ) have been proposed to mitigate the need for supervision . The most successful ones are contrastive learning-based approaches such as ( Chen et al. , 2020c ; b ) and they have shown remarkable results on target tasks such as image classification and object detection . Despite their success , there are two primary caveats : ( 1 ) These self-supervised methods are still trained on ImageNet or similar datasets , which are fairly cleaned up and/or include a pre-specified set of object categories . ( 2 ) This method of training is a passive approach in that it does not encode interactions . On the contrary , for humans , a vast majority of our visual understanding is shaped by our interactions and our observations of others interacting with their environments . We are not limited to learning from visual cues alone , and there are various other supervisory signals such as body movements and attention cues available to us . It is shown that by learning how to move the joints to walk and crawl , infants can significantly enhance their perception and cognition ( Adolph & Robinson , 2015 ) . Moreover , by observing another person interact with the environment humans obtain a visual and physical perception of the world ( Bandura , 1977 ) . The question we investigate in this paper is , “ can we learn a rich generalizable visual representation by encoding human interactions into our visual features ? ” . In this work , we consider the movement of human body parts and the center of attention ( gaze ) as an indicator of their interactions with the environment and propose an approach for incorporating interaction information into the musclysupervisedrepresentation learning process . To study what we can learn from interaction , we attach sensors to humans ’ limbs and see how they react to visual events in their daily lives . More specifically , we record the movements of the body parts by Inertial Movement Units ( IMUs ) and also the gaze to monitor the center of attention . We introduce a new dataset of more than 4,500 minutes of interaction by 35 participants engaging in everyday scenarios with their corresponding body part movements and center of attention . There are no constraints on the actions , and no manual annotations or labels are provided . Our experiments show that the representation we learn by predicting gaze and body movements in addition to the visual cues outperforms the visual-only baseline on a diverse set of target tasks ( Figure 1 ) : semantic ( scene classification ) , temporal ( action recognition ) , geometric ( depth estimation ) , physics ( dynamics prediction ) and affordance-based ( walkable surface estimation ) . This shows that movement and gaze information can help to learn a more informative representation compared to a visual-only model . 2 RELATED WORK . Visual representations can be learned using many different techniques from full supervision to no supervision at all . We outline the most common paradigms of representation learning , namely supervised , self-supervised , and interaction-based representation learning . Supervised Representation Learning . Supervised representation learning in computer vision is typically performed by pre-training neural networks on large-scale datasets with full supervision ( e.g. , ImageNet ( Deng et al. , 2009 ) ) or weak supervision ( e.g. , Instagram-1B ( Mahajan et al. , 2018 ) ) . These models are fine-tuned for a variety of tasks including object detection ( Girshick et al. , 2014 ; Ren et al. , 2015 ) , semantic segmentation ( Shelhamer et al. , 2015 ; Chen et al. , 2017 ) , and visual question answering ( Agrawal et al. , 2015a ; Hudson & Manning , 2019 ) . However , collecting a manually annotated large-scale dataset such as ImageNet requires extensive resources in terms of cost and time . In contrast , in this paper , we only use human interaction data , which does not require any manual annotation . Self-supervised Representation Learning . There has been a wide range of research on selfsupervised learning of visual representations in which properties of the images themselves act as supervision . The objectives for these methods cover a variety of tasks such as solving jigsaw puzzles ( Noroozi & Favaro , 2016 ) , colorizing grayscale images ( Zhang et al. , 2016 ) , learning to count ( Noroozi et al. , 2017 ) , predicting context ( Doersch et al. , 2015 ) , inpainting ( Pathak et al. , 2016 ) , adversarial training ( Donahue et al. , 2017 ) and predicting image rotations ( Gidaris et al. , 2018 ) . This type of representation learning is not limited to learning from single frames . Agrawal et al . ( 2015b ) and Jayaraman & Grauman ( 2015 ) both use egomotion , Wang & Gupta ( 2015 ) cyclically track patches in videos , Pathak et al . ( 2017 ) use low-level non-semantic motion-based cues , and Vondrick et al . ( 2016 ) predict the representation of future frames . Inspired by contrastive learning ( Hadsell et al. , 2006 ) , recent methods have used “ instance discrimination ” in which the network uniquely identifies each image . A network is trained to produce a non-linear mapping that projects multiple variations of an image closer to each other than to all other images . Using Noise Contrastive Estimation ( Gutmann & Hyvärinen , 2010 ) , networks are trained to differentiate between similar images under complex noise models ( such as non-overlapping crops and heavy color jittering ) and dissimilar images . Oord et al . ( 2018 ) and Hénaff et al . ( 2019 ) introduce and investigate the Contrastive Predictive Coding ( CPC ) method , which encodes the shared information between different crops of an image to predict the features from masked regions of the image . Wu et al . ( 2018 ) ; Misra & van der Maaten ( 2020 ) use a memory bank , which enables contrasting features of the current image against a large set of negative samples , increasing the likelihood of finding a nearby negative . The MoCo technique ( He et al. , 2020 ; Chen et al. , 2020c ) encodes the positive samples with a momentum encoder to avoid the rapid changes in the original feature extractor . They achieve comparable results with supervised learning representations . Chen et al . ( 2020a ; b ) show that by using a trainable non-linear transformation between the representation and contrastive latent space and larger batch sizes , they can omit memory banks entirely , allowing for full backpropagation through both positive and negative samples , and achieve better results . Bachman et al . ( 2019 ) ; Tian et al . ( 2019 ) maximize the mutual information between different extracted features of the same image from multiple views . Zhuang et al . ( 2019 ) enforce the extracted features of similar images to move towards the same part of the embedding space . Gordon et al . ( 2020 ) , Yao et al . ( 2020 ) , and Devon Hjelm & Bachman ( 2020 ) apply contrastive method to videos and leverage spatio-temporal cues to learn visual representations . Similar to these approaches , we do not rely on any manual annotation , but in contrast to them , we utilize human interactions along with their visual observation for representation learning . Interaction-Based Representation Learning . The third class of learning representations relies on cues obtained by interacting with a dynamic environment . Pinto et al . ( 2016 ) learn a representation from interactions of a robotic arm ( e.g. , grasping and pushing ) with different objects . Chen et al . ( 2019 ) and Weihs et al . ( 2019 ) both tackle the representation learning problem by training an agent to play a game in an interactive environment . Ehsani et al . ( 2018 ) learn a representation by modeling the non-semantic movements of a dog . Our work falls in this category since we use human interactions for learning the representation . We differ from these approaches in that we use lowlevel observations of human interaction such as body part movements and gaze to show significant improvement over a state-of-the-art baseline across multiple low-level and high-level target tasks . 3 HUMAN INTERACTION DATASET . We introduce a new dataset of human interactions for our representation learning framework . In this section , we describe the data collection . Our goal is to capture how humans react to the visual world by recording their movements and focus of attention . Previous datasets of human actions and gaze include only gaze information ( Fathi et al. , 2012 ; Xu et al. , 2018 ) , part movements from a third-person view ( Ionescu et al. , 2014 ; Hassan et al. , 2019 ) , or only action or hand labels in an egocentric setting ( Damen et al. , 2018 ; Sigurdsson et al. , 2018 ) . In contrast , our new dataset includes ego-centric observations along with the corresponding gaze and body movement information during their daily activities ranging from walking and cycling to driving and shopping . To collect the dataset , we record egocentric videos from a GoPro camera attached to the subjects ’ forehead . We simultaneously capture body movements , as well as the gaze . We use Tobii Pro2 eyetracking to track the center of the gaze in the camera frame . We record the body part movements using BNO055 Inertial Measurement Units ( IMUs ) in 10 different locations ( torso , neck , 2 triceps , 2 forearms , 2 thighs , and 2 legs ) . Figure 2 shows the data collection setup along with two clips of the captured sequences . In total , we collected 4,260 minutes of videos with their corresponding body part movement and gaze from 35 people . Unlike the common large-scale datasets used for representation learning such as ImageNet , there is no restriction on the categories observed in the images , and no manual annotation is provided . Although the supervision from movement and gaze does not come for free and still requires the participants wearing the sensors , no manual or semantic annotation is needed for acquiring this dataset . Statistical analysis of the dataset is provided in Appendix A.3 . Moreover , we provide details of aligning the video with the motion sensors and synchronization of the sensors in Appendix A.2 . The supplementary video shows a few examples of the video clips . 4 INTERACTION-BASED REPRESENTATION LEARNING . Visual representation learning is typically performed using visual cues from single images or videos ( He et al. , 2020 ; Gordon et al. , 2020 ) . Our goal in this paper is to incorporate human interactions into our representations to move beyond a purely visually-trained feature representation . Below , we describe our approach for integrating movement and gaze information in the representation learning pipeline . Intuitively , body part movements should encode the temporal changes in the image based on the underlying cause of those changes ( e.g. , moving legs results in walking which makes distant objects move closer ) . Additionally , gaze grounds the visual features with the location in the image where the person pays the most attention . This should correlate well with semantic concepts such as objects , or affordances such as walkable surfaces .","Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visualonly representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our “muscly-supervised” representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al., 2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch. Figure 1: We propose to use human’s interactions with their visual surrounding as a training signal for representation learning. We record first person observations as well as the movements and gaze of people living their daily routines and use these cues to learn a visual embedding. We use the learned representation on a variety of diverse tasks and show consistent improvements compared to state-of-the-art self-supervised vision-only techniques.","This paper proposes to improve upon unsupervised representation learning for various downstream vision tasks by leveraging human motion and attention (gaze) information. The authors collect a large spatio-temporal dataset with gaze and body motion labels for this task. They train a network to jointly predict the visual focus of attention in scenes and body motion besides visual instance recognition via an NCE loss to learn good visual representations. They show large improvements in accuracy of many different visual recognition downstream tasks with their approach versus the SOTA MOCO approach, which uses visual information only.","The article discusses the problem of learning a rich visual representation from human interactions, which can be used for various computer vision tasks, rather than relying solely on visual cues from single images or videos. The authors propose an approach that incorporates human body movements and gaze into the representation learning pipeline, which can capture the temporal changes in the image and ground the visual features with the location in the image where the person pays the most attention. The proposed approach uses a new dataset of human interactions, which includes ego-centric observations, body movements, and gaze information, to learn a representation that can improve performance across multiple low-level and high-level target tasks. The authors show that their approach outperforms a state-of-the-art baseline across various tasks, demonstrating the effectiveness of incorporating human interactions into visual representation learning.","The article presents a new approach for learning generalizable visual representations by encoding human interactions into the features. The researchers collected a new dataset of more than 4,500 minutes of interaction by 35 participants, recording body part movements and gaze during their daily activities. They show that incorporating movement and gaze information into the visual features outperforms a visual-only model on a diverse set of target tasks such as scene classification, action recognition, depth estimation, dynamics prediction, and walkable surface estimation. This approach could potentially help humans learn a more informative representation, just like infants' joint movements can enhance their perception and cognition.","0,2,1",0,1,0,1,,coverage,factuality,"coherence, concise",coverage,0,2,0,2,Summary hsa good coverage without being verbose.,Summary C has a factual error apparently? (that the approach could help *humans* to learn more informative representations).,Summary A is most coherent and concise. ,Summary C includes information that is not appropriate in summary (such as participant information).
49,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance . DeepSphere , a method based on a graph representation of the sampled sphere , strikes a controllable balance between these two desiderata . This contribution is twofold . First , we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors . Second , we evaluate DeepSphere on relevant problems . Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation . Perhaps surprisingly , comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay . Our code is available at https : //github.com/deepsphere . 1 INTRODUCTION . Spherical data is found in many applications ( figure 1 ) . Planetary data ( such as meteorological or geological measurements ) and brain activity are example of intrinsically spherical data . The observation of the universe , LIDAR scans , and the digitalization of 3D objects are examples of projections due to observation . Labels or variables are often to be inferred from them . Examples are the inference of cosmological parameters from the distribution of mass in the universe ( Perraudin et al. , 2019 ) , the segmentation of omnidirectional images ( Khasanova & Frossard , 2017 ) , and the segmentation of cyclones from Earth observation ( Mudigonda et al. , 2017 ) . As neural networks ( NNs ) have proved to be great tools for inference , variants have been developed to handle spherical data . Exploiting the locally Euclidean property of the sphere , early attempts used standard 2D convolutions on a grid sampling of the sphere ( Boomsma & Frellsen , 2017 ; Su & Grauman , 2017 ; Coors et al. , 2018 ) . While simple and efficient , those convolutions are not equivariant to rotations . On the other side of this tradeoff , Cohen et al . ( 2018 ) and Esteves et al . ( 2018 ) proposed to perform proper spherical convolutions through the spherical harmonic transform . While equivariant to rotations , those convolutions are expensive ( section 2 ) . As a lack of equivariance can penalize performance ( section 4.2 ) and expensive convolutions prohibit their application to some real-world problems , methods standing between these two extremes are desired . Cohen et al . ( 2019 ) proposed to reduce costs by limiting the size of the representation of the symmetry group by projecting the data from the sphere to the icosahedron . The distortions introduced by this projection might however hinder performance ( section 4.3 ) . Another approach is to represent the sampled sphere as a graph connecting pixels according to the distance between them ( Bruna et al. , 2013 ; Khasanova & Frossard , 2017 ; Perraudin et al. , 2019 ) . While Laplacian-based graph convolutions are more efficient than spherical convolutions , they are not exactly equivariant ( Defferrard et al. , 2019 ) . In this work , we argue that graph-based spherical CNNs strike an interesting balance , with a controllable tradeoff between cost and equivariance ( which is linked to performance ) . Experiments on multiple problems of practical interest show the competitiveness and flexibility of this approach . 2 METHOD . DeepSphere leverages graph convolutions to achieve the following properties : ( i ) computational efficiency , ( ii ) sampling flexibility , and ( iii ) rotation equivariance ( section 3 ) . The main idea is to model the sampled sphere as a graph of connected pixels : the length of the shortest path between two pixels is an approximation of the geodesic distance between them . We use the graph CNN formulation introduced in ( Defferrard et al. , 2016 ) and a pooling strategy that exploits hierarchical samplings of the sphere . Sampling . A sampling scheme V = { xi ∈ S2 } ni=1 is defined to be the discrete subset of the sphere containing the n points where the values of the signals that we want to analyse are known . For a given continuous signal f , we represent such values in a vector f ∈ Rn . As there is no analogue of uniform sampling on the sphere , many samplings have been proposed with different tradeoffs . In this work , depending on the considered application , we will use the equiangular ( Driscoll & Healy , 1994 ) , HEALPix ( Gorski et al. , 2005 ) , and icosahedral ( Baumgardner & Frederickson , 1985 ) samplings . Graph . From V , we construct a weighted undirected graph G = ( V , w ) , where the elements of V are the vertices and the weight wij = wji is a similarity measure between vertices xi and xj . The combinatorial graph Laplacian L ∈ Rn×n is defined as L = D − A , where A = ( wij ) is the weighted adjacency matrix , D = ( dii ) is the diagonal degree matrix , and dii = ∑ j wij is the weighted degree of vertex xi . Given a sampling V , usually fixed by the application or the available measurements , the freedom in constructing G is in setting w. Section 3 shows how to set w to minimize the equivariance error . Convolution . On Euclidean domains , convolutions are efficiently implemented by sliding a window in the signal domain . On the sphere however , there is no straightforward way to implement a convolution in the signal domain due to non-uniform samplings . Convolutions are most often performed in the spectral domain through a spherical harmonic transform ( SHT ) . That is the approach taken by Cohen et al . ( 2018 ) and Esteves et al . ( 2018 ) , which has a computational cost of O ( n3/2 ) on isolatitude samplings ( such as the HEALPix and equiangular samplings ) and O ( n2 ) in general . 1 https : //martinos.org/mne/stable/auto_tutorials/plot_visualize_evoked.html 2 https : //www.ncdc.noaa.gov/ghcn-daily-description On the other hand , following Defferrard et al . ( 2016 ) , graph convolutions can be defined as h ( L ) f = ( P∑ i=0 αiL i ) f , ( 1 ) where P is the polynomial order ( which corresponds to the filter ’ s size ) and αi are the coefficients to be optimized during training.3 Those convolutions are used by Khasanova & Frossard ( 2017 ) and Perraudin et al . ( 2019 ) and cost O ( n ) operations through a recursive application of L.4 Pooling . Down- and up-sampling is natural for hierarchical samplings,5 where each subdivision divides a pixel in ( an equal number of ) child sub-pixels . To pool ( down-sample ) , the data supported on the sub-pixels is summarized by a permutation invariant function such as the maximum or the average . To unpool ( up-sample ) , the data supported on a pixel is copied to all its sub-pixels . Architecture . All our NNs are fully convolutional , and employ a global average pooling ( GAP ) for rotation invariant tasks . Graph convolutional layers are always followed by batch normalization and ReLU activation , except in the last layer . Note that batch normalization and activation act on the elements of f independently , and hence don ’ t depend on the domain of f . 3 GRAPH CONVOLUTION AND EQUIVARIANCE . While the graph framework offers great flexibility , its ability to faithfully represent the underlying sphere — for graph convolutions to be rotation equivariant — highly depends on the sampling locations and the graph construction . 3.1 PROBLEM FORMULATION . A continuous function f : C ( S2 ) ⊃ FV → R is sampled as TV ( f ) = f by the sampling operator TV : C ( S2 ) ⊃ FV → Rn defined as f : fi = f ( xi ) . We require FV to be a suitable subspace of continuous functions such that TV is invertible , i.e. , the function f ∈ FV can be unambiguously reconstructed from its sampled values f . The existence of such a subspace depends on the sampling V and its characterization is a common problem in signal processing ( Driscoll & Healy , 1994 ) . For most samplings , it is not known if FV exists and hence if TV is invertible . A special case is the equiangular sampling where a sampling theorem holds , and thus a closed-form of T−1V is known . For samplings where no such sampling formula is available , we leverage the discrete SHT to reconstruct f from f = TVf , thus approximating T−1V . For all theoretical considerations , we assume that FV exists and f ∈ FV . By definition , the ( spherical ) graph convolution is rotation equivariant if and only if it commutes with the rotation operator defined as R ( g ) , g ∈ SO ( 3 ) : R ( g ) f ( x ) = f ( g−1x ) . In the context of this work , graph convolution is performed by recursive applications of the graph Laplacian ( 1 ) . Hence , if R ( g ) commutes with L , then , by recursion , it will also commute with the convolution h ( L ) . As a result , h ( L ) is rotation equivariant if and only if RV ( g ) Lf = LRV ( g ) f , ∀f ∈ FV and ∀g ∈ SO ( 3 ) , where RV ( g ) = TVR ( g ) T−1V . For an empirical evaluation of equivariance , we define the normalized equivariance error for a signal f and a rotation g as EL ( f , g ) = ( ‖RV ( g ) Lf −LRV ( g ) f‖ ‖Lf‖ ) 2 . ( 2 ) More generally for a class of signals f ∈ C ⊂ FV , the mean equivariance error defined as EL , C = Ef∈C , g∈SO ( 3 ) EL ( f , g ) ( 3 ) represents the overall equivariance error . The expected value is obtained by averaging over a finite number of random functions and random rotations . 3In practice , training with Chebyshev polynomials ( instead of monomials ) is slightly more stable . We believe it to be due to their orthogonality and uniformity . 4As long as the graph is sparsified such that the number of edges , i.e. , the number of non-zeros in A , is proportional to the number of vertices n. This can always be done as most weights are very small . 5The equiangular , HEALPix , and icosahedral samplings are of this kind . 104 105 106 107 n= 12N 2side pixels 10-6 10-5 10-4 10-3 ke rn el w id th t k-NN graph , k= 60 k-NN graph , k= 40 k-NN graph , k= 20 k-NN graph , k= 8 Perraudin et al. , k= 8 Figure 3 : Kernel widths . Figure 4 : 3D object represented as a spherical depth map . 100 101 102 103 spherical harmonic degree 10 3 10 2 10 1 100 101 po we r s pe ct ru m SHREC'17 ( depth and normal ) cosmology ( convergence map ) climate ( 16 variables ) Figure 5 : Power spectral densities . 3.2 FINDING THE OPTIMAL WEIGHTING SCHEME . Considering the equiangular sampling and graphs where each vertex is connected to 4 neighbors ( north , south , east , west ) , Khasanova & Frossard ( 2017 ) designed a weighting scheme to minimize ( 3 ) for longitudinal and latitudinal rotations6 . Their solution gives weights inversely proportional to Euclidean distances : wij = 1 ‖xi − xj‖ . ( 4 ) While the resulting convolution is not equivariant to the whole of SO ( 3 ) ( figure 2 ) , it is enough for omnidirectional imaging because , as gravity consistently orients the sphere , objects only rotate longitudinally or latitudinally . To achieve equivariance to all rotations , we take inspiration from Belkin & Niyogi ( 2008 ) . They prove that for a random uniform sampling , the graph Laplacian L built from weights wij = e − 14t‖xi−xj‖ 2 ( 5 ) converges to the Laplace-Beltrami operator ∆S2 as the number of samples grows to infinity . This result is a good starting point as ∆S2 commutes with rotation , i.e. , ∆S2R ( g ) = R ( g ) ∆S2 . While the weighting scheme is full ( i.e. , every vertex is connected to every other vertex ) , most weights are small due to the exponential . We hence make an approximation to limit the cost of the convolution ( 1 ) by only considering the k nearest neighbors ( k-NN ) of each vertex . Given k , the optimal kernel width t is found by searching for the minimizer of ( 3 ) . Figure 3 shows the optimal kernel widths found for various resolutions of the HEALPix sampling . As predicted by the theory , tn ∝ nβ , β ∈ R. Importantly however , the optimal t also depends on the number of neighbors k. Considering the HEALPix sampling , Perraudin et al . ( 2019 ) connected each vertex to their 8 adjacent vertices in the tiling of the sphere , computed the weights with ( 5 ) , and heuristically set t to half the average squared Euclidean distance between connected vertices . This heuristic however overestimates t ( figure 3 ) and leads to an increased equivariance error ( figure 2 ) . 6Equivariance to longitudinal rotation is essentially given by the equiangular sampling .","Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the sampled sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay. Our code is available at https: //github.com/deepsphere.","The article discusses the challenge of designing a convolution for a spherical neural network that balances efficiency with rotation equivariance. The method proposed, DeepSphere, uses graph convolutions to strike a balance between cost and equivariance, achieving computational efficiency, sampling flexibility, and rotation equivariance. The authors evaluate DeepSphere on a variety of problems and find it to show state-of-the-art performance. Finally, the article proposes a weighting scheme that approximates the Laplace-Beltrami operator to achieve optimal equivariance.","The paper studies the problem of designing a convolution for a spherical neural network. The authors use the existing graph CNN formulation and a pooling strategy that exploits hierarchical pixelations of the sphere to learn from the discretized sphere. The main idea is to model the discretized sphere as a graph of connected pixels: the length of the shortest path between two pixels is an approximation of the geodesic distance between them. To show the computational efficiency, sampling flexibility and rotation equivariance, extensive experiments are conducted, including 3D object recognition, cosmological mode classification, climate event segmentation and uneven sampling.","DeepSphere is a method that combines graph convolutional neural networks and spherical harmonics to analyze data on the sphere. The method is designed to balance efficiency and rotation equivariance, which is important for tasks such as cosmological parameter inference and 3D object segmentation. By leveraging graph theory, DeepSphere can achieve rotation equivariance without sacrificing computational efficiency, making it a promising approach for real-world problems. The method has been evaluated on various datasets and shown to outperform previous methods in terms of both accuracy and efficiency.","1,0,2",1,2,2,0,,coverage,coverage,coherence,other,0,2,2,1,Summary A has the best coverage of topics.,Summary C omits important terms and context (spherical neural network).,Summary C is more coherent. ,Summary B includes most facts that cannot be verified. 
50,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"1 INTRODUCTION . Computational modeling is now more than ever at the forefront of infectious disease research due to the COVID-19 pandemic . Stochastic simulations play a critical role in understanding and forecasting infectious disease dynamics , creating what-if scenarios , and informing public health policy making ( Cramer et al. , 2021 ) . More broadly , stochastic simulation ( Ripley , 2009 ; Asmussen & Glynn , 2007 ) produces forecasts about complex interactions among people , environment , space , and time given a set of parameters . They provide the numerical tools to simulate probabilistic models and stochastic processes in finance ( Lamberton & Lapeyre , 2007 ) , chemistry ( Gillespie , 2007 ) and many scientific disciplines . Unfortunately , stochastic simulations at fine-grained spatial and temporal resolution are extremely computationally expensive . For example , epidemic models whose aim is to simulate realistic diffusion dynamics via in-silico experiments require the exploration of a large parameter space ( e.g . characteristics of a virus , policy interventions , people ’ s behavior ) . Therefore , hundreds of thousands of simulations are required to explore and calibrate the epidemic model with observed epidemic surveillance data . This process significantly hinders the adaptive capability of existing stochastic simulators , especially in “ war time ” emergencies , due to the lead time needed to execute new simulations and produce actionable insights that could help guide decision makers . Learning deep surrogate models to speed up complex simulation has been explored in climate modeling and fluid dynamics for deterministic dynamics ( Sanchez-Gonzalez et al. , 2020 ; Wang et al. , 2020 ; Holl et al. , 2019 ; Rasp et al. , 2018 ) , but not for stochastic simulations . Furthermore , these surrogate models can only approximate specific system dynamics and fail to generalize under different parametrization . Especially for pandemic scenario planning , we desire models that can predict futuristic scenarios under different conditions . Furthermore , the majority of the surrogate models are trained passively using a simulation data set . This requires a large number of simulations to cover different parameter regimes of the simulator and ensure generalization . We propose Interactive Neural Process ( INP ) , a Bayesian active learning framework to speed up the stochastic simulation . Given parameters such as disease reproduction number , incubation and infectious periods , mobility dynamics , and initial state , current mechanistic simulators generate future outbreak states with time-consuming numerical integration . Our framework accelerates the simulation by learning the input-output map between parameters and future states , hence bypassing numerical integration . We develop a novel neural process ( NP ) ( Garnelo et al. , 2018 ) model to mimic the mechanistic simulator with uncertainty . We extend NP to spatiotemporal setting : we introduce a time-evolving latent process for temporal dynamics and integrate graph convolution for spatial modeling . Instead of learning passively , we design active learning algorithms to interact with the simulator and update our model in “ real-time ” . We derive a new acquisition function , Latent Information Gain ( LIG ) , based on our unique model design . Our algorithm selects the parameters with the highest LIG , queries the simulator to generate new simulation data , and continuously updates our model . Compared with Gaussian processes ( GP ) , INP is more expressive , more accurate , and scales easily to high-dimensional sequence data . We provide theoretical guarantees for the sample efficiency of this procedure over random sampling . We also demonstrate the efficacy of our method on large-scale spatiotemporal epidemic models . In summary , our contributions include : • Interactive Neural Process : a deep Bayesian active learning framework for accelerating large-scale stochastic simulation • A novel spatiotemporal neural process model ( STNP ) for high-dimensional time series data with temporal latent process and spatial graph convolution . • New acquisition function , Latent Information Gain ( LIG ) , based on the inferred temporal latent process to quantify uncertainty with theoretical guarantees • Real-world application to speed up stochastic , spatiotemporal epidemic models . 2 RELATED WORK . Bayesian Active Learning and Experimental Design . Bayesian active learning , or experimental design is well-studied in statistics and machine learning ( Chaloner & Verdinelli , 1995 ; Cohn et al. , 1996 ) . Gaussian Processes ( GPs ) are popular for posterior estimation e.g . Houlsby et al . ( 2011 ) and ( Zimmer et al. , 2018 ) , but often struggle in high dimension . Deep neural networks provide scalable solutions for active learning . Deep active learning has been applied to discrete problems such as image classification ( Gal et al. , 2017 ) and sequence labeling ( Siddhant & Lipton , 2018 ) whereas our task is continuous time series . Our problem can also be viewed as sequential experimental design where we design simulation parameters to obtain the desired outcome ( imitating the simulator ) . Foster et al . ( 2021 ) propose deep design networks for Bayesian experiment design but they require a explicit likelihood model and conditional independence in experiments . Kleinegesse & Gutmann ( 2020 ) consider implicit models where the likelihood function is intractable , but computing the Jacobian through sampling path can be expensive and their experiments are mostly limited to low ( < =10 ) dimensional design . In contrast , our design space is of much higher-dimension and we do not have access to an explicit likelihood model for the simulator . Neural Processes . Neural processes ( NP ) ( Garnelo et al. , 2018 ) model distributions over functions and imbue neural networks with the ability of GPs to estimate uncertainty . NP has many extensions such as attentive NP ( Kim et al. , 2019 ) and functional NP ( Louizos et al. , 2019 ) . However , NP implicitly assumes permutation invariance in the latent variables and can be limiting in modeling temporal dynamics . Singh et al . ( 2019 ) proposes sequential NP by incorporating a temporal transition model into NP . Still , sequential NP assumes the latent variables are independent conditioned on the hidden states . We propose spatiotemporal NP with temporal latent process and spatial graph convolution , which is well-suited for modeling the spatiotemporal dynamics of infectious disease . We apply our model to real-world large-scale Bayesian active learning . Note that even though Garnelo et al . ( 2018 ) has demonstrated NP for Bayesian optimization , it is only for toy 1-D functions . Stochastic Simulation and Dynamics Modeling . Stochastic simulations are fundamental to many scientific fields ( Ripley , 2009 ) , especially epidemic modeling . Data-driven models of infectious diseases are increasingly used to forecast the evolution of an ongoing outbreak ( Arik et al. , 2020 ; Cramer et al. , 2021 ; Lourenco et al. , 2020 ) . However , very few models can mimic the internal mechanism of a stochastic simulator and answer “ what-if questions ” . GPs are commonly used as surrogate models for expensive simulators ( Meeds & Welling , 2014 ; Gutmann et al. , 2016 ; Qian et al. , 2020 ) , but GPs do not scale well to high-dimensional data . Likelihood-free inference Lueckmann et al . ( 2019 ) ; Papamakarios et al . ( 2019 ) ; Munk et al . ( 2019 ) ; Wood et al . ( 2020 ) aim to learn the posterior of the parameters given observed data . They use neural density estimators to learn an emulator , but require a lot of simulations . We propose a novel neural process model to learn the likelihood . For active learning , instead of relying on Monte Carlo sampling , we directly compute the information gain in the latent process . For epidemic modeling , Qian et al . ( 2020 ) only consider the simple SEIR model . In contrast , we demonstrate the use of deep sequence model as a prior distribution in Bayesian active learning for real-world , large-scale , spatiotemporal dynamics . Our framework is also compatible with other deep sequence models for time series , e.g . Deep State Space ( Rangapuram et al. , 2018 ) , Neural ODE ( Chen et al. , 2018 ) and etc . 3 METHODOLOGY . Consider a stochastic process { X1 , · · · , XT } , governed by time-varying parameters θt ∈ RK , and the initial state x0 ∈ RD . In epidemic modeling , θt can represent the effective reproduction number of the virus at a given time , the effective contact rates between individuals belonging to different age groups , the people ’ s degree of short- or long-range mobility , or the effects of time varying policy interventions ( e.g . non-pharmaceutical interventions ) . The state xt ∈ RD includes both the daily prevalence and daily incidence for each compartment of the epidemic model ( e.g . number of people that are infectious and number of new infected individuals at time t ) . Stochastic simulation uses a mechanistic model F ( θ ; ξ ) to simulate the process where the random variable ξ represents the randomness in the simulator . Let θ : = ( x0 , θ1 , · · · , θT ) represent the initial state and all the parameters over time . For each θ , we obtain a different set of simulation data { ( x1 , · · · , xT ) m } Mm=1 . However , realistic large-scale stochastic simulations require the exploration of a large parameter space and are extremely computationally intensive . In the following section , we describe a pipeline called Interactive Neural Process ( INP ) to proactively query the stochastic simulator , generate simulation data , in order to learn a fast surrogate model for rapid simulation . 3.1 INTERACTIVE NEURAL PROCESS . Our high-level goal is to train a deep surrogate model to mimic the stochastic simulator . As show in Figure 1 , given parameters θ , we query the simulator , i.e. , the mechanistic model to obtain a set of simulations { ( x1 , · · · , xT ) m } Mm=1 . We train a neural process ( NP ) model to learn the probabilistic map from parameters to future states . Our NP model is spatiotemporal and it captures the unique disease dynamics of the epidemic simulator . During inference , the model needs to generate predictions ( x̂1 , · · · , x̂T ) at the target parameters θ corresponding to different scenarios . Instead of simulating at a wide range of parameter regimes , we take a Bayesian active learning approach to proactively query the simulator and update the model incrementally . Using NP , we can infer the latent temporal process ( z1 , · · · , zT ) that encodes the uncertainty of the current surrogate model . Then we propose a new reward/acquisition function , latent information gain ( LIG ) , to select the θ ? with the highest reward . We use θ ? to query the simulator , and in turn generate new simulation to further improve the model . Next , we describe each of the components in detail . 3.2 SPATIOTEMPORAL NEURAL PROCESS . Neural process ( NP ) ( Garnelo et al. , 2018 ) is a type of deep generative model that represents distributions over functions . It approximates a stochastic process by combining Gaussian process with deep learning . NP introduces a global latent variable z to capture the stochasticity and learns the conditional distribution p ( x1 : T |θ ) by optimizing the evidence lower bound ( ELBO ) : log p ( x1 : T |θ ) ≥ Eq ( z|x1 : T , θ ) [ log p ( x1 : T |z , θ ) ] − KL ( q ( z|x1 : T , θ ) ‖p ( z ) ) ( 1 ) Here p ( z ) is the prior distribution for the latent variable . We use x1 : T as a shorthand for ( x1 , · · · , xT ) . The prior distribution p ( z ) is conditioned on a set of context points θc , xc1 : T as p ( z|xc1 : T , θc ) . The set of context points is randomly sampled from the training data . Its complement is the target set . However , the global latent variable z in NP can be limiting for non-stationary , spatiotemporal dynamics in the epidemics . We propose spatiotemporal neural process ( STNP ) with two extensions . First , we introduce a temporal latent process ( z1 , · · · , zT ) to represent the unknown dynamics . The latent process provides an expressive description of the internal mechanism of the stochastic simulator . Each latent variable zt is sampled conditioning on the past history . Second , we explicitly model the spatial dependency in xt ∈ RD . Rather than treating the dimensions in xt as independent features , we capture their correlations with a graph . For instance , the travel graph between locations can be represented as an adjacency matrix A ∈ RD×D . Given parameters { θ } , simulation data { x1 : T } , and the spatial graph A as inputs , STNP models the conditional distribution p ( x1 : T |θ , A ) by optimizing the following ELBO objective : log p ( x1 : T |θ , A ) ≥ Eq ( z1 : T |x1 : T , θ , A ) log p ( x1 : T |z1 : T , θ , A ) − KL ( q ( z1 : T |x1 : T , θ , A ) ‖p ( z1 : T ) ) ( 2 ) where the distributions q ( z1 : T |x1 : T , θ , A ) and p ( x1 : T |z1 : T , θ , A ) are parameterized with neural networks . The prior distribution p ( z1 : T ) is conditioned on a set of contextual sequences p ( z1 : T |xc1 : T , θc , A ) . Figure 2 visualizes the graphical models of our STNP , the vanilla NP ( Garnelo et al. , 2018 ) , and Sequential NP ( Singh et al. , 2019 ) . We implement STNP following an encoder-decoder architecture . The encoder parametrizes the mean and standard deviation of the variational posterior q ( z1 : T |x1 : T , θ , A ) and the decoder approximates the predictive distribution p ( x1 : T |z1 : T , θ , A ) . To incorporate the spatial graph information , we use a Diffusion Convolutional Gated Recurrent Unit ( DCGRU ) layer ( Li et al. , 2017 ) which integrates graph convolution in a GRU cell . We use multi-layer GRUs to obtain hidden states from the inputs . Using re-parametrization ( Kingma & Welling , 2013 ) , we sample zt from the encoder and then decode xt conditioned on zt in an auto-regressive fashion . To ensure fair comparisons , we adapt NP and SNP to graph-based settings and use the same architecture as STNP to generate the hidden states shown in Figure 6 . See Appendix B.3 for architecture details .","Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. We propose Interactive Neural Process (INP), a Bayesian active learning framework to proactively learn a deep learning surrogate model and accelerate simulation. Our framework is based on the novel integration of neural process, deep sequence model and active learning. In particular, we develop a novel spatiotemporal neural process model to mimic the simulator dynamics. Our model automatically infers the latent process which describes the intrinsic uncertainty of the simulator. This also gives rise to a new acquisition function based on the latent information gain. We design Bayesian active learning algorithms to iteratively query the simulator, gather more data, and continuously improve the model. We perform theoretical analysis and demonstrate that our approach reduces sample complexity compared with random sampling in high dimension. Empirically, we demonstrate our framework can faithfully imitate the behavior of a complex infectious disease simulator with a small number of examples, enabling rapid simulation and scenario exploration.","The manuscript entitled, ""Accelerating Stochastic Simulation with Interactive Neural Processes"", presents a novel approach to the problem of statistical emulation for mechanistic models of epidemic disease transmission.  To this end, structured neural processes are developed to exploit and respect the temporal and spatio-temporal character of these models.  An active learning strategy is developed to train these neural processes to minimise the computational costs of generating training instances of disease simulator outputs.  The methodology developed is applied to two SEIR compartmental model examples: a minimal one with a single homogenous population and a maximal one with many age and space delimited cohorts.","The article proposes a new framework called Interactive Neural Process (INP) to accelerate large-scale stochastic simulation in epidemic modeling. INP uses a novel spatiotemporal neural process model to learn the probability distribution of future states given parameters and initial conditions. The framework is designed to be Bayesian active learning, which means it proactively queries the simulator and updates the model incrementally. The proposed method is shown to be more accurate and efficient than traditional methods, and it has important implications for real-world applications in epidemic modeling.","The article discusses the use of computational modeling and stochastic simulations in understanding and forecasting infectious disease dynamics, particularly in the context of the COVID-19 pandemic. However, these simulations are computationally expensive and hinder the adaptability of existing stochastic simulators, especially during emergencies. The article proposes an Interactive Neural Process (INP), a Bayesian active learning framework that accelerates the simulation process by learning the input-output map between parameters and future states to bypass numerical integration. INP utilizes a spatiotemporal neural process (STNP) that models the uncertain temporal dynamics and spatial graph convolution and uses a new acquisition function, the Latent Information Gain (LIG), to quantify this uncertainty and select parameters with the highest LIG for simulation, thereby allowing it to predict future scenarios under different conditions.","0,2,1",2,1,2,0,,coverage,coverage,"coverage, coherence","coverage, relevance",1,2,1,0,Summary B has good coverage despite being short.,Summary C adds a lot of background info but does not discuss results,Summary B is the best one out of the three since it has provides the motivation and the overall structure is most coherent and natural.,"Summary A does not have the motivation for the paper. It also contains information like the title of the work which is useless. Although summary C contains information that cannot be verified, but it has overall better structure and coherence than summary A."